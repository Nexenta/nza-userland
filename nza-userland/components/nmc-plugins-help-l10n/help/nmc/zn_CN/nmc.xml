<?xml version='1.0' encoding='UTF-8'?>
<templatelist name='nmc' copyright='(C) 2006-2012 Nexenta Systems, Inc.' locale='zn_CN'>
<template module='Alias' name='alias_usage'><![CDATA[
$cmdline
Usage: [name]
Usage: [name=value] [-s]

  -s       Store the new value, update NMC configuration file.

Aliases and the commands they alias can be used interchangeably.

Run 'alias' without parameters to list the current aliases.
Use -s (store) option, to store user-defined aliases persistently;
alternatively you could use 'setup appliance nmc edit-settings'
command edit the entire set of NMC options, aliases including.

With no arguments, prints out a list of the current aliases.
With only the <alias-name> argument, prints out a definition of the
alias with that name.

  * Use 'alias' and 'unalias' commands to view, set, and unset
    aliases at runtime

  * To make the settings persistent, use either -s option
    or edit the NMC configuration file via:
    ${prompt}setup appliance nmc edit-settings

  * Built-in aliases are:

   alias quit=exit
   alias options=option
   alias fastswitch='switch appliance -f'

  * The following are examples of multi-word aliases:

${prompt}alias 'setup checkpoint'='setup appliance checkpoint'
${prompt}alias "show remote appliance" = "show network appliance"


See also: 'help options'

See also: 'options'
See also: 'help commands'
See also: 'option -h'

See also: 'unalias'

See also: 'setup appliance nmc'
See also: 'setup appliance nms'

]]></template>
<template module='Cd' name='cd_usage'><![CDATA[
Usage: [directory]

Change the current directory to the specified <directory>.

The special directory "-" is interpreted as "return to the previous
directory".

Omitting directory name is equivalent to 'cd /'

Example:
	nmc:/\$ cd vol1/a
	nmc:/vol1/a\$ cd
	nmc:/\$ dirs
	\%2     /
	\%1     /vol1/a
	\%0   > /

'cd \%<num>' will jump to a certain directory in the stack.

For instance, 'cd \%3' changes directory to the one stored at
the position number 3 in the stack.

'cd +<num> and 'cd -<num> will go forward/backward in the directory
stack.  For instance, 'cd -3' changes directory to the one stored
3 positions prior relative to the current position.

Example:
	nmc:/\$ cd vol1/a
	nmc:/vol1/a\$ cd b
	nmc:/vol1/a/b\$ dirs
	\%2     /
	\%1     /vol1/a
	\%0   > /vol1/a/b

	nmc:/vol1/a/b\$ cd %1
	nmc:/vol1/a\$ dirs
	\%2     /
	\%1   > /vol1/a
	\%0     /vol1/a/b

As always, use TAB-TAB to complete directory names and make selections.

See also: 'dirs'

]]></template>
<template module='Dtrace' name='dtrace_usage'><![CDATA[
*****************************************************************
*								*
*               System DTrace based toolkit                     *
*                                                               *
*****************************************************************

DTrace can be used to generate performance profiles and analyze
performance bottlenecks.

DTrace can help to troubleshoot problems, by providing detailed views
of the system internals.

Use TAB-TAB to navigate, or simply press Enter and make a selection.

See help (-h) for usage. In most cases examples are provided; to see
an example, select 'example' option. For instance:

   ${prompt}dtrace cpu cpuwalk example

To override the default behavior of any given dtrace utility,
specify extra options in the command line, for instance:

   ${prompt}dtrace cpu cpuwalk 5

   This will run for 5 seconds (as opposed to default:
   to run until Ctrl-C is pressed).

For details on particular command line options use help (-h),
for instance:

   ${prompt}dtrace cpu cpuwalk -h


See also: 'show volume iostat'
See also: 'show volume <name> iostat'

See also: 'show performance'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Dtrace' name='dtrace_script_run_usage'><![CDATA[
	
See also: 'run diagnostics'
See also: 'show volume iostat'
See also: 'show volume <name> iostat'

See also: 'show performance'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

See also: 'help iostat'
See also: 'show usage'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Foreach' name='foreach_usage'><![CDATA[
$cmdline
Usage: <iteration variable> (<list of values>)

  <iteration variable>  iteration variable

  <list of values> 	list of values to substitute the 
                        <iteration variable> within the scope
			of 'foreach'. The values are separated
			with space or comma. Brackets may 
			be optionally used, for instance:
			(vol1, vol2, vol3) 
			or (the same):
			vol1, vol2, vol3
			The first word after 'foreach' is always 
			treated as iteration variable name, 
		  	and the rest of the list - as the list 
			of values.
				
NMC LOOP facility
=================

NMC 'foreach' primitive allows to execute any number of NMC commands
in a loop.

The outline of NMC 'foreach' loop follows below:

  ${prompt}foreach <var> (value1, value, ...)
  (f)${prompt}<command1>
  (f)${prompt}<command2>
  ...
  ...
  (f)${prompt}endfor


Foreach can be used inside recording mode that makes possible to create 
small and powerful scripts

Foreach indicates the beginning of a script cycle. Active 'foreach' 
mode is denoted by (f) in the NMC prompt. While in this mode all
commands are stored without execution. 

Enter 'endfor' to complete the foreach cycle and execute the stored
list of commands. 

NOTES:
-----
     An active foreach mode is denoted by (f) in NMC promt.

     Command 'record' is not allowed and skipped without storing.

     Foreach commands can not be nested.

     It is allowed to exit NMC promt using command 'exit' while 
     foreach mode is on. After entering 'exit' NMC inquires for
     confirmation of the it`s next action: "Execute?". Positive
     answer works as if endfor was enetered and negative answer
     exits NMC promt. 

Examples:

1) Create several folders and auto-snap for each folder
   ${prompt}foreach fld (vol1/documents vol1/users)
   (f)${prompt}create folder fld
   (f)${prompt}create auto-snap fld -r -T 3am -k 2 -p 1 -i daily -I ''
   (f)${prompt}endfor

2) Disable several auto-snaps
   ${prompt}foreach snap (vol1/documents vol1/users)
   (f)${prompt}setup auto-snap snap disable
   (f)${prompt}endfor

3) Clean up created folders and auto-snaps
   ${prompt}foreach fld vol1/documents vol1/users
   (f)${prompt}destroy auto-snap fld -y
   (f)${prompt}destroy folder fld -y
   (f)${prompt}endfor


See also: 'create $NZA::RUNNER_SCRIPT'

See also: 'record'
See also: 'run recording'
See also: 'show recording'
See also: 'destroy recording'

See also: 'show $NZA::RUNNER_SCRIPT'
See also: 'setup $NZA::RUNNER_SCRIPT'
See also: 'help runners'
   
]]></template>
<template module='Lunsync' name='lunsync_usage'><![CDATA[
Usage: [-C] [-y] [-r]

  -C	Cleanup obsolete (dangling) device links
  -y	Skip confirmation dialog by automatically answering Yes.
  -r    Re-configure and re-scan all HBAs
  
Re-enumerate devices in the appliance.

The command is particularly useful in rare cases when (and if) the system 
does not discover newly attached LUNs (physical or logical disk drives)
automatically. To troubleshoot, run:

   ${prompt}show lun

   This will show all physical and logical disks, commonly called LUNs.
   If a disk is missing in this output, run:

   ${prompt}lunsync
  
The process of re-enumerating devices may take some time, especially
if you deploy remote attachments via iSCSI and/or FC SAN. 

Example: Re-discover and re-enumerate devices, cleanup old and obsolete
device links:

   ${prompt}lunsync -C


See also: 'show lun'
See also: 'help'

]]></template>
<template module='NMC' name='re_register'><![CDATA[
                          * * *
                      LICENSE ERROR

    Error: $error
    The following options are available at this point:

       (a) purchase $NZA::PRODUCT license on-line or via third party
           and re-register the appliance
       (b) stop using $NZA::PRODUCT software

    Note that Your data stored on the appliance remains intact and
    accessible via network shares, including NFS and CIFS shares.

]]></template>
<template module='NMC' name='process'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Warning! Your $lic_info->{license_capacity} space is at a critical limit.
     Remaining storage space is less then $lic_info->{license_size_left_percent}% of maximum
     allowed by the license.
     For more information, run 'show appliance license'.

]]></template>
<template module='NMC' name='process1'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     $s3
     For more information, run 'show appliance license'.

]]></template>
<template module='NMC' name='initialize_interactive_mode'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Appliance's initial configuration is incomplete. Please
     point your Internet browser to the URL below and follow
     online instructions:

              $nmvurl

]]></template>
<template module='NMC' name='initialize_interactive_mode1'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     New upgrades available!
     Run 'setup appliance upgrade' to upgrade your system.
     To skip this check: 'option check_for_upgrades=0 -s'

]]></template>
<template module='NMC' name='initialize_interactive_mode2'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Some appliance's volumes are currently being scrubbed.
     Please run 'show volume' for details.

]]></template>
<template module='NMC' name='initialize_interactive_mode3'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Appliance Reboot Required.

     In order to complete the configuration change, your system
     needs to be restarted. Until you do so, new functionality may
     be unavailable.

]]></template>
<template module='NMC' name='initialize_interactive_mode4'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Appliance Reboot Required.

     In order to complete the software upgrade, your system needs to be
     restarted. Until you do so, security updates may not be fully
     applied, newly inserted or supported hardware may not function
     and new services may not be available.

]]></template>
<template module='NMC' name='nms_is_gone_handler'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Failed to initialize NMC:
     $err
     Suggested possible recovery actions:
	- Reboot into a known working system checkpoint
	- Run 'svcadm clear nms'; then try to re-login
	- Run 'svcadm enable -rs nms' to enable nms daemon and then try to re-login
     Suggested troubleshooting actions:
	- Run 'svcs -vx' and collect output for further analysis
	- Run 'dmesg' and look for error messages
	- View "/var/log/nms.log" for error messages
	- View "/var/svc/log/application-nms:default.log" for error messages

]]></template>
<template module='NMC' name='check_reg_plugins'><![CDATA[
                          * * *
                      SYSTEM NOTICE

     Detected not registered plugins.

     Some of the plugins installed on this appliance appear to be not
     registered with the central $NZA::PRODUCT software repository.

     Following is the complete list:
$str

     It is possible that the plugins were installed during free trial
     evaluation of the $NZA::PRODUCT software.

     You may remove these plugins right now, or contact technical
     support for further assistance.

]]></template>
<template module='Ndmpcopy' name='ndmpcopy_usage'><![CDATA[
Usage:  ndmpcopy src_appliance:/src/dir dest_appliance:/dest/dir
                 [-sa user:password] [-da user:password]
                 [-sport ndmp_src_port] [-dport ndmp_dest_port]
                 [-v] [-h]

   src_appliance, dest_appliance
                   Source and destination hostnames or IP addresses.

   /src/dir, /dest/dir
                   Source and destination pathnames.

   -v              Verbose output. Add more v's to increase the
   		   verbosity.

   -sa user:password
                   NDMP user name and password for the source host.
		   Use ndmpadm to setup user and password.

   -da user:password
                   NDMP user name and password for the destination host.
		   Use ndmpadm to setup user and password.

   -sport srcport  Use different TCP port for source host. Default port
                   is 10000.

   -dport dstport  Use different TCP port for destination host. Default
                   port is 10000
	
Easy-to-use NDMP management utility that can be used to perform
NDMP transfers between two NDMP capable (and NDMP compliant) servers. 

At runtime 'ndmpcopy' establishes connections to NDMP server processes 
on the source and destination machines, performs authentication to the
servers, and initiates NDMP backup on the source machine and NDMP 
restore on the destination machine.

For more information, please see User Guide, Section "NDMP".
			
Example:

${prompt}ndmpcopy -v -v hostone:/opt/a hosttwo:/opt/b -sa tmpuser:tMppassword777 -da tmpuser:tMppassword777


See also: 'setup network service ndmp-server'

See also: 'show network service ndmp-server devices'
See also: 'show network service ndmp-server log'
See also: 'show network service ndmp-server performance'
See also: 'show network service ndmp-server sessions'

]]></template>
<template module='Option' name='option_usage'><![CDATA[
$cmdline
Usage: <option>[=<new_value>] [-s]

  -s       Store the new value, update NMC configuration file.

Display NMC configurable options and/or modify their values. Note
that unless -s option is specified, the new setting will not be
persistent - it will affect only the current NMC session.  To make
persistent changes in the NMC configuration, use the -s option,
or run 'setup appliance nmc'.
$rc_lines
Examples:

1) Show all NMC configurable options and their values:
   ${prompt}option

2) Show a given option named 'bg' (background color):
   ${prompt}option bg

3) Change the 'bg' option value to 'dark'
   ${prompt}option bg=dark

4) Same as above, with the only difference that the new 'bg'
   value will be stored in NMC configuration file:
   ${prompt}option bg=dark -s

See also: 'help options'

See also: 'setup appliance nmc'
See also: 'setup appliance nms'

]]></template>
<template module='Query' name='query_usage'><![CDATA[
$cmdline
Usage: [object-type] [-e expression] [-v] [q] [-w]
or:
       <object-type> [-L]

 object-type	 One of the top level appliance objects:
		 *	@nmc_groups[0..3]
		 *	@nmc_groups[4..$#nmc_groups]
		 If omitted, perform global search on all appliance objects.

 -e <expression> Logical expression based on objects properties. The names
    	         of the properties serve as expressions variables, e.g.:

		 "compression eq off" - to query the corresponding folders
		         (for which compression is switched off),
		 "quota < 1M"        - to query data sets limited to 1 MB.
		 "type eq hourly"    - to see all hourly auto-services
		         (including auto-sync, auto-scrub, auto-tier, etc.),
		 "from-fs"           - to query objects with a property
		                       called "from-fs".

	   Note that the latter is, strictly speaking, an extension
	   of the syntax that allows to disregard property values and
	   query based on property names, or more exactly, based on
	   the presence of properties with matching names. For instance,

	   ${prompt}query -e size and type

	   will locate all objects having both 'size' and 'type'
	   properties.

	   Many more examples are given below.

	   Use regular expression syntax (canonical symbols)
	   such as:
		eq, ne		   for string comparison,
		<, >, ==, <=, >=   for number comparison,
		=~                 for regular expressions,
		and, or, not 	   to combine all those elements
				   into one compound search
				   criteria.
	   Parts of the names of the objects' properties can be
	   used, for instance:

0            "compression eq off" is equivalent to "compr eq off"

 	   As always, expression can be entered interactively -
	   simply omit the '-e' command line option and type
	   the expression in, when prompted.

	   NOTE:
	   -----
	         When using -e command line option, it is always
		 safer to embed the expression into "", for instance:

 		 ${prompt}query lun -e "size>20420 and type eq ide"

		 Otherwise NMC may have a trouble to distinguish
		 between '>' and '<' comparison operations on one hand,
	         and '>','<' output redirection, on another.

		 This quideline does not apply when the expression
		 is entered via a dialog.

	   Note also that all appliance's objects, independently
	   of their type, always have a property called 'name' -
	   the object's name.  The 'name' can be used in all
	   expressions and for all object types.

 -v   	         Verbose (default). For all selected objects,
  	 	 display the corresponding (queried) property values.
		 This is the default query mode; to disable it use the
		 -q (quiet) option.

 -q   	         Quiet. Print only the names of the objects that
                 satisfy the query. Do not display the corresponding
		 property values.

 -w              Enable warnings. In particular, produce a warning
  	         if more then one object's property matches the
		 specified pattern.

 -L              Display object's properties. The -L is a helper option
		 that'll display all available property names.
		 The -L requires the <object-type> to be specified, and
		 it is incompatible with all the rest options listed above.
		 The result of 'query <object-type> -L' can later be used
		 to build a meaningful query.

-----------------------------------------------------------------------

Examples:

1)  Check volumes' health:

    ${prompt}query -e health -v
    or, more specifically:
    ${prompt}query volume -e health -v

    The result may look like:

NAME   PROPERTY
vol1   health=ONLINE
vol2   health=ONLINE
vol3   health=ONLINE
vol4   health=DEGRADED
vol5   health=ONLINE
...

2)  Same as above, performed on a group of appliances:

    ${prompt}switch group some-group
    ${g_prompt}query volume -e health

    As always, in a group context NMC operations are run on each
    member appliance.

3)  Query folders based on capacity, availability, quota - and display
    only the names of the selected objects, do not print values of
    the corresponding properties:

    ${prompt}query folder -e "used > 500K" -q
    ${prompt}query folder -e "avail < 2M and quota eq none" -q

4)  Query all auto-services
    (that is, $auto_services)
    for a given interval type:

    ${prompt}query auto-service -e "type eq hourly"
    ${prompt}query auto-service -e "type eq daily and state eq online"

5)  Query those objects that have attributes 'used', 'available', and
    'quota', and display the corresponding names and values.

    ${prompt}query -e "used and available and quota" -v

    The output may look like:

NAME          PROPERTIES
vol2/x/y      quota=none, used=24.5K, available=1.95G
vol2/a-dest   quota=none, used=29.5K, available=1.95G
vol1/a/b      quota=none, used=65.5K, available=975M
...

6)  Query objects with matching names:

    ${prompt}query -e "name =~ /auto/"
    or, the same:
    ${prompt}query -e "name =~ auto"

7)  Query objects with matching names:

    ${prompt}query snapshot -e "name =~ /vol1/ and creation =~ 2006"
    or, the same:
    ${prompt}query snapshot -e "name=~vol1 and creation=~2006"

8)  Query objects that have a property 'fs-name', and print this
    property value:

    ${prompt}query -e "fs-name"

9) Query objects with a property that matches 'state',
    and print this property value; enable warnings to make sure that
    there is no ambiguity (well, in the case there is):

    ${prompt}query -e "state"  -w

10) Find out which runners are currently in maintenance;
    display the status and run-time state:

    ${prompt}query runner -e "status eq maintenance and state"

    Here's a sample output:

NAME                    PROPERTIES
cpu-utilization-check   status=maintenance, state=ready

11) List all volume's and LUN's properties.
    (This can later be used to search volumes and LUNs..)

    ${prompt}query volume -L
    ${prompt}query lun -L

-----------------------------------------------------------------------

See also: 'show all'
See also: 'show usage'
See also: 'help commands'

]]></template>
<template module='Record' name='record_usage'><![CDATA[
$cmdline
Usage:  <name> | stop | view-current

  <name>        Name of a new or existing NMC recording session.
  		The session can be later "re-played" via
		'run recording <name>'
  
  stop		Stop recording.
  view-current  View session that is being currently recorded. Note that
                an active recording is denoted by (r) in the NMC prompt.

Starting, stopping and viewing NMC recording sessions. The recorded 
NMC session is a plain text file (a script) stored under 
$NMC::Util::RECORDINGS_DIR directory. 

Once recorded, the NMC session can be replayed any number of times,
manually (via '$NMC::run_now') and automatically - via $NZA::RUNNER_SCRIPT.

For more information on running NMC recordings, please see:

   ${prompt}run recording
   and
   ${prompt}create $NZA::RUNNER_SCRIPT 


   Recording vs. Batch Mode Operation
   ==================================

   The facility enhances capability of the $NZA::COMPANY Management Console
   to execute batch (non-interactive) jobs. While recording is active,
   all commands are stored in their "final" form, even though you may
   be entering some (or all) of the options interactively.
   
   For instance, run:
   1) record aaa
   2) create $NZA::AUTO_TIER

   At this point NMC will prompt you to start filling out all required
   parameters. Once completed, run:
   
   3) record view-current

   You will see a lengthy command populated with all the right options.
   This command, and the entire recorded session, can now run without
   user interaction. 

   ==================================

To start a new recording session or continue existing one, simply state 
the session name in the command line. 

To view already existing (stored) recordings, run 'record show',
or use 'show recording' command to view content of any specific 
existing recorded session.

While the recording is active all commands are stored with the session.

NOTE:
----
    An active recording is denoted by (r) in the NMC prompt.

A recording session may be closed (stopped) and re-opened multiple
times. Each next time new records are appended to the session file.
NOTE: 'record view-current' will show only the commands entered
      after the current recording has started. New commands are 
      appended to an existing recording file after recording is stopped. 

To replay a session, run it by name. See the corresponding 
'setup recording' and 'show recording' commands, and examples below. 

Examples:

1) Start recording session:
   ${prompt}record my-nmc-session
2) View my-nmc-session:
   (r)${prompt}record view-current
3) Stop and close my-nmc-session:
   (r)${prompt}record stop
4) Run my-nmc-session
   ${prompt}run recording my-nmc-session

5) The following example sets up a temporary group of 5 appliances,
   upgrades them all, and removes the group. The entire sequence is
   recorded, so that it may be repeated (re-played) later..

   ${p1}record upgrade-all-5 
   ${p1}create group -g temp_group host1 host2 host3 host4 host5

   Alternatively, run:
   ${p1}create group

   and enter group and appliances interactively. The result will be the
   same - fully populated command that can be executed non-interactively.
   See "Recording vs. Batch Mode Operation" above.

   ${p1}switch group temp_group
   ${p2}setup appliance upgrade -y
   ${p2}exit
   ${p1}destroy group temp_group -y
   ${p1}record stop 

   Note the '-y' usage above. This avoids confirmation dialogs
   (by automatically answering Yes), and thus allows to execute
   'upgrades-all-5' in a batch mode.

   For more examples see 'help tips'

See also: 'run recording'
See also: 'show recording'
See also: 'destroy recording'

See also: 'create $NZA::RUNNER_SCRIPT'
See also: 'setup $NZA::RUNNER_SCRIPT'
See also: 'show $NZA::RUNNER_SCRIPT'

See also: 'help tips'

]]></template>
<template module='Setup' name='create_appl_group_usage'><![CDATA[
$cmdline
Usage: [group-name] [-d description]
       [-p] [-y]
       [hostname1 [hostname2 ...]]

   <group-name>              Name of the new group (of appliances)
   -d <description>          Optional description of the group

   -y			     Skip confirmation dialog by automatically
   		 	     answering Yes.
   -p			     Ping first. When defining a group
    			     non-interactively, it often makes sense
			     to ensure connectivity with the specified
			     appliances. The -p option is provided
			     to verify that each appliance supplied
			     via command line is "pingable" and is
			     properly ssh-bound.

   hostname1 [hostname2...]  Space-delimited appliances - members of the
                             new group, identified by their respective
			     hostnames.

Type once, run anywhere! Group appliances to be able to automatically
run all top-level commands:

]]></template>
<template module='Setup' name='create_appl_group_usage1'><![CDATA[

on each appliance in the group. More than one group of appliances can
be defined simultaneously.

Once a group is defined, use the 'switch' command to change to this group
(or any other group defined at this point). See 'switch $NMC::APPL_GROUP'.

To view the existing groups of appliances, run 'show $NMC::APPL_GROUP'.

Examples:
1) Define a new group interactively, via NMC multiple-choice dialog:
   ${prompt}setup $NMC::APPL_GROUP create

   or, same:
   ${prompt}create $NMC::APPL_GROUP

2) Create a group 'mygroup' of two appliances:
   ${prompt}create $NMC::APPL_GROUP -g mygroup localhost myhost.abcdef.com

3) Same as above, plus verify the availability of the remote appliance,
   plus add description:
   ${prompt}setup $NMC::APPL_GROUP -g mygroup myhost.abcdef.com localhost -p
   -d "my new group of appliances"

4) Show existing groups
   ${prompt}setup $NMC::APPL_GROUP show
   or, same:
   ${prompt}show $NMC::APPL_GROUP


See also: 'switch $NMC::APPL_GROUP'
See also: 'destroy $NMC::APPL_GROUP'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='destroy_appl_group_usage'><![CDATA[
$cmdline
Usage: [-y] [group name]

  -y	Skip confirmation dialog by automatically responding Yes

  -f    Force destroy operation, even though some of the group members
        may be offline or not reachable.
	***
	Warning: if the group configuration is distributed between
	appliances in the group (case in point: HA cluster, seacrh
	F.A.Q. pages on the website for "cluster") -
	forceful destroy may leave the group configuration
	on the remote appliance, which may be offline or not reachable
	at the time of the operation. You can cleanup this configuration
	later, by simply executing 'destroy' locally on this appliance.
	***


Destroy a given logical group of appliances. Note that the
current (active) group cannot be destroyed - to perform the operation
you first need to exit from the group back into single-appliance mode.

See also: 'create group'
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='_service_usage'><![CDATA[
$cmdline
Usage: [$optstring]

  Method '$method' for service '$service'
  $longdesc
  $body
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='_share_usage'><![CDATA[
$cmdline
Usage:
${prompt}setup folder <folder-name> share $srv [$optstring]

  Share local folder(s) via ${srv_u}.
  ${body_u}${body}
See also: 'setup <folder> unshare $srv'
See also: 'show <folder> share $srv'

See also: 'setup folder <name> property inherit'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='_unshare_usage'><![CDATA[
$cmdline
Usage: [-r]

Unshare volume or folder: make $srv_u-shared volume or folder unavailable.

  -r	Recursive; unshare nested folders

	Note that recursive behaviour is implied for all inherited share
	definitions. For instance, let's say you defined a share
	for vol1/a. Because of generic attribute inheritance
	(and 'sharenfs' is just one of the inherit-able attributes),
	all nested folders automatically inherit the same definition.
 	Unsharing vol1/a automatically unshares all nested folders if
	property inheritance is enabled.

See also: 'setup <folder> share $srv'
See also: 'show <folder> share $srv'

See also: 'setup folder <name> property inherit'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='volume_replace_lun_usage'><![CDATA[
$cmdline
Usage: [-o old_disk] [-n new_disk] [-r] [-y] [-f]

  -o old_disk   Old disk to replace

  -n new_disk   New disk to use as a replacement for old one

  -r            Indicates that old disk was already replaced by operator

  -y            Skip confirmation dialog by automatically answering Yes.

  -f            Force operation. Forces usage of (virtual of physical)
                disks that belong to an exported volume. Forces use of
                disks even if this results in a conflicting replication
		level (for instance, adding a pool-ed disk to an existing
		mirror based volume)

Replace old_device with new_device. Can be used to replace a failing or
failed device.

To see the devices (a.k.a. LUNs) of a given volume, say vol1, run:
'show volume vol1 status' or 'show vol1/ status.
This will also display the device states and statuses -
the information to be taken into account when performing the replace-lun
operation.

The size of new_device must be greater than or equal to the minimum size of
all the devices in a mirror or raidz configuration.

If new_device is not specified, it defaults to old_device. This form of
replacement is useful after an existing disk has failed and has been
physically replaced. In this case, the new disk may have the same name as
the old device, even though it is actually a different disk.

Examples:

1)  ${prompt}setup volume vol1 replace-lun -o c0d1 -n c1d0

    This will replace IDE drive c0d1 with a new IDE drive c1d0
    in the volume 'vol1'.

2)  ${prompt}setup volume vol1 replace-lun -o c2t0d1 -n c2t0d1

    This will replace a faulted drive c2d0d1 with a new drive.
    The scenario here is that, after c2t0d1 had faulted, it was
    removed and another drive was attached to the same controller
    (c2, in the case) and placed into the same slot.

3)  ${prompt}setup volume syspool replace-lun -o c0t0d1 -n c0t0d1

    Same as above, performed on a system volume. As any other volume,
    syspool may fail. It is generally advised to always provision
    system volumes with mirrored or RAIDZ configurations. Once
    it is done, you could always use 'replace-lun' operation
    to handle disk faults, with the appliance remaining online
    and operational.

4)  ${prompt}setup volume syspool replace-lun

    As always, you could use NMC dialog mode to enter the required
    parameters. NMC will let you select the drives, with the
    selection process simplified by NMC-displayed tips and
    prompts.

5)  ${prompt}setup volume vol1 replace-lun -o c2t0d1 -r

    The -r option indicates that the old device ('c2t0d1' in this case)
    was already physically replaced and the new device happens to
    have the same name.
    That is, after c2t0d1 had faulted, it was removed and another
    drive was attached to the same controller
    (c2, in the case) and placed into the same slot.

6)  ${prompt}setup volume vol1 replace-lun -r

    Same as above except that the device-to-be-replaced is entered
    interactively.


See also: 'show lun'
See also: 'show volume <volume name> status'

See also: 'lunsync'

See also: 'setup volume <volume-name> attach-lun'
See also: 'setup volume <volume-name> detach-lun'
See also: 'setup volume <volume-name> $NMC::CLEAR_ERRORS'

See also: 'setup volume <volume-name> recover-faulted'

See also: 'setup usage'

]]></template>
<template module='Setup' name='volume_attach_lun_usage'><![CDATA[
$cmdline
Usage: [-o old_disk] [-n new_disk] [-y] [-f]

  -o old_disk   Existing disk

  -n new_disk   New disk to attach to the old_disk

  -y            Skip confirmation dialog by automatically answering Yes.

  -f            Force operation. Forces usage of (virtual of physical)
                disks that belong to an exported volume. Forces use of
                disks even if this results in a conflicting replication
		level (for instance, adding a pool-ed disk to an existing
		mirror based volume)

Attach new_device to the existing old_device. Can be used to convert
non-mirrored pool to the mirror, or for replacing of a failed device
(for the latter, use 'detach-lun' first).

To see the devices (a.k.a. LUNs) of a given volume, say vol1, run:
'show volume vol1 status' or 'show vol1/ status.
This will also display the device states and statuses -
the information to be taken into account when performing the attach-lun
operation.

Examples:

1)  ${prompt}setup volume vol1 attach-lun -o c0d1 -n c1d0

    This will attach IDE drive c0d1 to a new IDE drive c1d0
    in the volume 'vol1'. If the existing drive c0d1 was
    used as a standalone (non-mirrored) drive, this command will
    automatically create a mirrored configuration
    out of (c0d1, c1d0).
    Alternatively, if the old drive c0d1 was already a part
    of the mirror, the command will simply add another drive
    to the existing mirror, thus converting, say, two-way
    mirror into three-way mirror.

3)  ${prompt}setup volume syspool attach-lun -o c0t0d1 -n c0t0d1

    Similar to the above, performed on a system volume.
    It is generally advised to always provision
    system volumes with mirrored or RAIDZ configurations.
    If the system pool is based on a single drive, you could
    use the 'attach-lun' to convert it to a mirror. If the
    system volume is already a mirror, the operation
    will convert two-way mirror into three-way, three-way
    into four-way, etc.

4)  ${prompt}setup volume syspool attach-lun

    As always, you could use NMC dialog mode to enter the required
    parameters. NMC will let you select the drives, with the
    selection process simplified by NMC-displayed tips and
    prompts.


See also: 'show lun'
See also: 'lunsync'

See also: 'show volume <volume name> status'

See also: 'setup volume <volume-name> detach-lun'
See also: 'setup volume <volume-name> replace-lun'
See also: 'setup volume <volume-name> $NMC::CLEAR_ERRORS'

See also: 'setup volume <volume-name> recover-faulted'

See also: 'setup usage'

]]></template>
<template module='Setup' name='volume_offline_lun_usage'><![CDATA[
$cmdline
Usage:

Take the specified physical device offline.

Quoting from zpool(1m) man page:

   While the device is offline, no attempt is made to read or write
   to the device. This command is not applicable to spares.

To see the devices (a.k.a. LUNs) of a given volume, say vol1, run:
'show volume vol1 status' or 'show vol1/ status.

Example:
    ${prompt}setup volume vdemo offline-lun c2t0d0

    This will take offline 'c2t0d0' in the volume 'vdemo'


See also: 'setup volume <vol-name> online-lun
See also: 'setup volume <vol-name> replace-lun
See also: 'setup volume <vol-name> $NMC::CLEAR_ERRORS'

See also: 'setup volume <volume-name> recover-faulted'

See also: 'lunsync'
See also: 'setup usage'

]]></template>
<template module='Setup' name='volume_online_lun_usage'><![CDATA[
$cmdline
Usage:

Bring the specified physical device online.

To see the devices (a.k.a. LUNs) of a given volume, say vol1, run:
'show volume vol1 status' or 'show vol1/ status.

Example:
    ${prompt}setup volume vdemo online-lun c2t0d0

    This will bring online 'c2t0d0' in the volume 'vdemo'

See also: 'setup volume <vol-name> offline-lun
See also: 'lunsync'
See also: 'setup volume <vol-name> replace-lun
See also: 'setup volume <vol-name> $NMC::CLEAR_ERRORS'
See also: 'setup usage'

]]></template>
<template module='Setup' name='volume_version_upgrade_usage'><![CDATA[
$cmdline
Usage: [-y] [-r]

  -y	   Skip confirmation dialog by automatically answering Yes.

  -r	   Recursive upgrade of all volume's folders

Upgrades or downgrades a given volume to the current system ZFS version.
All folders on the volume can be upgraded automatically after the
volume upgrade finishes.


]]></template>
<template module='Setup' name='volume_version_upgrade_usage1'><![CDATA[


Examples:

1) ${prompt}setup volume vol1 version-upgrade

   This will upgrade volume vol1 only

2) ${prompt}setup volume vol1 version-upgrade -r

   This will upgrade volume vol1 and all it's subfolders.


See also: 'show volume <vol-name> version'
See also: 'show volume version'

See also: 'show volume -h'
See also: 'show folder <folder-name> version -h'

See also: 'show appliance checkpoint'

]]></template>
<template module='Setup' name='volume_free_reserve_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	   Skip confirmation dialog by automatically answering Yes.

Frees reserved space for emergency cases. Use this command if get into
troubles with volume without any free space.

See also: 'setup appliance nms property autoreserve_space'

]]></template>
<template module='Setup' name='folder_version_upgrade_usage'><![CDATA[
$cmdline
Usage: [folder-name] version-upgrade [-y] [-r]

  -y	   Skip confirmation dialog by automatically answering Yes.

  -r	  Upgrade folder and its subfolders recursively.

Upgrade a given folder to the current system ZFS version. The folder
name may be omitted. In this case all existing folders are upgraded.

Parameter '-r' is used only if folder's name is set in command.


]]></template>
<template module='Setup' name='folder_version_upgrade_usage1'><![CDATA[

Examples:

1) ${prompt}setup folder vol1/folder1 version-upgrade

   This will upgrade folder vol1/folder1 only

2) ${prompt}setup folder vol1/folder1 version-upgrade -r

   This will upgrade folder vol1/folder1 and its subfolders.

2) ${prompt}setup folder version-upgrade

   This will upgrade all appliance's folders.


See also: 'show volume <vol-name> version'
See also: 'show folder <folder-name> version'

See also: 'show volume -h'

See also: 'setup volume <vol-name> version-upgrade'
See also: 'show appliance checkpoint'

]]></template>
<template module='Setup' name='volume_recover_faulted_usage'><![CDATA[
Recover faulted devices in a volume.

This operation is currently supported for iSCSI attached devices
only; it will have no effect on a faulted or degraded volume that
is based on non-iSCSI LUNs.  Functionality to support other types
of device attachments will be added in the future.

Example:

The following example shows volume in a DEGRADED state:

${prompt}show volume vol1 status
  state: DEGRADED
  status: One or more devices could not be opened.
          Sufficient replicas exist for
          the pool to continue functioning in a degraded state.
  config:
  NAME                                       STATE     READ WRITE CKSUM
  vol1                                       DEGRADED     0     0     0
    raidz2                                   DEGRADED     0     0     0
      c2t600144F04941790200000C2976700700d0  ONLINE       0     0     0
      c2t600144F04941790300000C2976700700d0  ONLINE       0     0     0
      c2t600144F049417A2600000C29D2025E00d0  UNAVAIL      6 1.32K     0
      c2t600144F049417A5100000C29D2025E00d0  ONLINE       0     0     0
    spares
      c2t600144F04941B32200000C29D2025E00d0  AVAIL

Assuming that iSCSI connectivity to remote Target backing up faulted
device 'c2t600144F049417A2600000C29D2025E00d0' is restored,
the following operation will do the rest, that is - recover the volume:


${prompt}show volume vol1 recover-faulted

Once done, the volume status is restored back to ONLINE, as shown below:

  NAME                                       STATE     READ WRITE CKSUM
  vol1                                       ONLINE       0     0     0
    raidz2                                   ONLINE       0     0     0
      c2t600144F04941790200000C2976700700d0  ONLINE       0     0     0
      c2t600144F04941790300000C2976700700d0  ONLINE       0     0     0
      c2t600144F049417A2600000C29D2025E00d0  ONLINE       6 1.32K     0
      c2t600144F049417A5100000C29D2025E00d0  ONLINE       0     0     0
    spares
      c2t600144F04941B32200000C29D2025E00d0  AVAIL


See also: 'setup volume <name> $NMC::CLEAR_ERRORS'

See also: 'setup volume <volume-name> attach-lun'
See also: 'setup volume <volume-name> detach-lun'
See also: 'setup volume'

]]></template>
<template module='Setup' name='volume_remove_usage'><![CDATA[
$cmdline
Usage: <disk> [-y]

  <disk>        Disk to remove/detach from volume
  -y            Skip confirmation dialog by automatically responding Yes

Remove/Detach disk from volume.

Current limitations:
====================

 1. 'detach-lun' can be done only on a mirrored configuration.

 2. 'remove-lun' currently only supports removing hot spares
     and cache devices.


]]></template>
<template module='Setup' name='volume_create_grow_usage'><![CDATA[
$cmdline
Usage: [-y] [-n] [-s] [-f] [-c compression] [disk1 disk2 ...]
       @plugin_params

Create a new volume. The <option_list> consists of a new volume name
and devices, for instance:

  ======================= Example #1 ==================================

  ${prompt}setup volume create vol1 c2t0d0 c2t1d0 -y
  or, the same:
  ${prompt}create volume vol1 c2t0d0 c2t1d0 -y

  This will non-interactively (notice the '-y' option)
  create volume 'vol1' based on two SCSI drives. There is
  no redundancy in this configuration, which makes it relatively unsafe.

  ======================= Example #2 ==================================

  ${prompt}create volume vol1 mirror c2t0d0 c2t1d0 -y

  This will create a two-way mirror 'vol1' based on two SCSI drives.

  ======================= Example #3 ==================================

  ${prompt}create volume vol1 mirror c2t0d0 c2t1d0 log c2t5d0 -y

  Same as above, with a separate ZFS intent log device, to get better
  write performance.

  ======================= Example #4 ==================================

  ${prompt}create volume vol1 c1t1d0 cache c1t2d0 c1t3d0

  This creates a single-disk volume with two cache devices, to optimize
  read performance. Note again that it is advisable to use redundant
  mirror or raidz based configurations.


  Of course, all these commands can be entered interactively, with
  NMC providing helpful hints and prompts.


  You could run 'create volume' to create a new volume interactively.
  Along the way NMC will be providing helpful hints and guidance.

  Use the '-n' (dry run) option to try out any list of options and
  see the result without actually creating a volume.


  -s               Use default snapshot policy (that is, default
                   $NZA::AUTO_SNAP services) for the new volume.
                   The defaults are:
                     - keep $snapshot_volume_keep_dailies daily snapshots
                     - keep $snapshot_volume_keep_monthlies monthly snapshots
                     - take snapshots at $snapshot_volume_scheduled_time.
                   For more information, see 'create $NZA::AUTO_SNAP -h'
                   To view or modify the defaults, use 'option' command.

  -c               Controls the compression algorithm used for this dataset.
                   Default is "on". Setting compression to "on" uses the
                   lzjb compression algorithm. The lzjb compression
                   algorithm is optimized for performance while providing
                   decent data compression. Currently, "gzip" is equivalent
                   to "gzip-6".

]]></template>
<template module='Setup' name='volume_create_grow_usage1'><![CDATA[
$cmdline
Usage: [-y] [-n] [-f] [disk1 disk2 ...]

Grow an existing volume. The <option_list> consists of devices to be
added to the volume, for instance:

  ======================= Example #1 ==================================

  ${prompt}setup volume vol1 grow c2t0d0 -y

  This will non-interactively (notice the '-y' option) add a new LUN
  to volume 'vol1'.
  If the volume is actively block-mirrored using auto-cdp,
  the added device(s) are automatically added to the service, and will
  get mirrored as well.

  ======================= Example #2 ==================================

  ${prompt}setup volume vol1 grow spare c1t0d0 -d c2t2d0 -y

  This non-interactively adds spare disk c1t0d0 to an existing volume
  'vol1'. If the volume is actively block-mirrored using auto-cdp,
  the added device is automatically added to the service, and will
  get mirrored as well.

  ======================= Example #3 ==================================

  ${prompt}setup volume vol1 grow cache c1t2d0 c1t3d0

  This adds to an existing volume a group of cache devices, to optimize
  read performance.


  To grow an existing volume interactively, run 'volume <vol-name> grow'
  and follow the instructions.

]]></template>
<template module='Setup' name='volume_create_grow_usage2'><![CDATA[
  -y               Skip all confirmation dialogs (including those
                   that warn about using disks that belong to exported
                   volumes) by automatically answering Yes

  -n               Dry-run. Display the resulting configuration without
                   actually performing the operation.

  -f               Force operation. Forces usage of (virtual of physical)
                   disks that belong to an exported volume. Forces use of
                   disks even if this results in a conflicting
                   replication level
                   (for instance, adding a pool-ed disk to an existing
                   mirror based volume - which is the case when you
                   may want to consider 'attach-lun' operation
                   instead of 'grow').

@plugin_lines
  disk1 disk2 ...  Space separated list of disks (LUNs).


        Please consider your data redundancy and IO performance
        requirements carefully. For instance, a simple mirror
        configuration provides both the benefits of data redundancy
        and read performance. If you need even better data protection,
        3-way has significantly greater MTTDL (Mean Time To Data Loss)
        than a 2-way mirror, while going to a 4-way mirror offers only
        marginal improvement. More guidelines can be found at
        www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

        * A raidz1 configuration maximizes disk space and generally
        performs well when data is written and read in large chunks
        (128K or more).

        * A raidz2 configuration offers excellent data availability,
        and performs similarly to raidz.

        * A raidz2 has significantly better mean time to data loss
        (MTTDL) than either raidz1 or 2-way mirrors.

        * A mirrored configuration consumes more disk space but
        generally performs better with small random reads.

        * If your I/Os are large, sequential, or write-mostly,
        then ZFS's I/O scheduler aggregates them in such a way that
        you'll get very efficient use of the disks regardless of the
        data replication model.

        * For better performance, a mirrored configuration is strongly
        favored over a raidz configuration particularly for large,
        uncacheable, random read loads.

        Each configuration option has its pros and cons. If unsure,
        dry-run the command with -n option.


See also: 'show lun'
See also: 'show volume'
See also: 'create auto-snap'
See also: 'setup volume import'
See also: 'setup volume'

See also: 'setup volume <name> grow'
See also: 'setup volume <name> remove-lun'
See also: 'setup volume <name> attach-lun'
See also: 'setup volume <name> detach-lun'
See also: 'setup volume <name> offline-lun'

See also: 'lunsync'

See also: 'record'
See also: 'option'
See also: 'query'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='volume_import_usage'><![CDATA[
$cmdline
Usage: [-D] [-f] [-s] [vol-name] [new-name] @plugin_params

  <vol-name>    Name of the exported or destroyed volume.
                The name may optionally contain unique volume
		identifier, as explained and illustrated below.

  <new-name>    If specified, the volume is imported under this
                new name. This can be used to resolve a conflict
		in the case when a volume named <vol-name> already
		exists in the system.

  -D    Import destroyed volumes. The option allows to reimport
        volumes destroyed via 'setup volume <vol> destroy' command.
	By default, only exported volumes can be re-imported.

  -f    Forces import, even if the volume appears to be active.

  -y    Skip confirmation dialog by automatically answering Yes.

  -s    Use default snapshot policy (that is, default $NZA::AUTO_SNAP
	services) for the imported volume.
        The defaults are:
          - keep $snapshot_volume_keep_dailies daily snapshots
          - keep $snapshot_volume_keep_monthlies monthly snapshots
	  - take snapshots at $snapshot_volume_scheduled_time.
        For more information, see 'create $NZA::AUTO_SNAP -h'
        To view or modify the defaults, use 'option' command.

@plugin_lines
Imports a volume that was either previously destroyed or exported
(the latter, via 'setup volume <vol> export' command).

NOTE:
=====
   Run 'show volume import' to list all volumes that were exported
   and can be re-imported.

   Run 'show volume import -D' to list all volumes that were destroyed
   and can be re-imported.

Notice that both 'show' commands (above) will list a volume name followed
by a volume ID, for instance:

   ${prompt}show volume import
   VOLUME:ID                      STATE    D LUNS
   vol1:380397436658166737        AVAIL    N c1t1d0

This unique identifier ('380397436658166737', in the example above)
can be used to resolve the ambiguity, in terms of specifying the exact
instance that needs to be imported.

For the purposes of illustrating this, let's say a volume named 'vol1'
was at one point in time T1 destroyed, while at some other point in
time T2 (either before or after T1) volume also named 'vol1' was
destroyed.

In this case, the 'show volume import' output may look as follows:

   ${prompt}show volume import
   VOLUME:ID                      STATE    D LUNS
   vol1:380397436658166737        AVAIL    N c1t1d0
   ${prompt}show volume import -D
   VOLUME:ID                      STATE    D LUNS
   vol1:14138375834007198484      DEGRADED Y c1t1d0,c1t3d0

Notice that both volumes in this example are named 'vol1'. The two
importable instances, however, have different (and unique) IDs.

To import the first (exported, in this example) instance, run

   ${prompt}setup volume import vol1:380397436658166737

To import the second (destroyed, in this example) instance, run
respectively:

   ${prompt}setup volume import vol1:14138375834007198484 -D

Note that the -D option must be used to import destroyed volumes.

Finally, if <new-name> is specified in the command line,
the volume is imported under this new name. More examples follow below.


1) List exported volumes, and press TAB-TAB to possibly select one of
   the exported volumes for importing:
   ${prompt}setup volume import   <TAB><TAB>

   Note: to simply list exported volumes, run:
   ${prompt}show volume import

2) List destroyed volumes:
   ${prompt}show volume import -D

3) Assuming 'vol2' is one of the exported volumes, the following
   command will re-import it:
   ${prompt}setup volume import vol2

4) Same as above, with the only difference that we are re-importing
   'vol2' under a new name: as 'new_vol2':
   ${prompt}setup volume import vol2 new_vol2


See also: 'show volume'
See also: 'create volume'
See also: 'destroy volume'
See also: 'setup volume'

See also: 'option'
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='volume_export_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	Skip confirmation dialog by automatically responding Yes

Export the specified volume from the appliance. Exported volumes can
be re-imported later via 'setup volume import' command.

See also: 'setup volume import'
See also: 'destroy volume'
See also: 'create volume'
See also: 'show volume'

See also: 'help zpool'
See also: 'setup usage'

See also: 'help'

]]></template>
<template module='Setup' name='volume_destroy_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	Skip confirmation dialog by automatically responding Yes

Destroy the specified volume, free up devices it uses. You could try
to re-import the destroyed volume later by running
'setup volume import -D' command.

]]></template>
<template module='Setup' name='volume_clear_usage'><![CDATA[
Clear all device errors in a volume.

Examples:

1) ${prompt}setup vol1/ $NMC::CLEAR_ERRORS

   This will clear all device errors in a volume vol1. To see device errors,
   run:
   ${prompt}show vol1/ status

2) ${prompt}show volume vol2 status
   Here's a sample output:

volume: vol2
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
        using 'zpool clear' or replace the device with 'zpool replace'.
   see: http://www.sun.com/msg/ZFS-8000-9P
 scrub: none requested
config:

     NAME           STATE     READ WRITE CKSUM
     vol2           ONLINE       0     0     0
          raidz1    ONLINE       0     0     0
            c1t4d0  ONLINE       0     0     0
            c1t5d0  ONLINE       0     0     0
            c1t6d0  ONLINE       0     0    16
            c1t7d0  ONLINE       0     0     0

errors: No known data errors

Notice 16 correctable checksum errors on c1t6d0. To clear the errors, run:

   ${prompt}setup vol2/ $NMC::CLEAR_ERRORS


See also: 'setup volume <name> recover-faulted

See also: 'setup volume <volume-name> attach-lun'
See also: 'setup volume <volume-name> detach-lun'

See also: 'help zpool'
See also: 'setup usage'

See also: 'help'

]]></template>
<template module='Setup' name='zfs_set_property_usage'><![CDATA[
$cmdline
Usage: [-s value]

  -s <value>	New${s1}'s ${s2}value.
  		If not specified via command line -s option, the value
		can be entered interactively.

Update${s1} property.

Quoting from zfs(1m) man page:

  creation         The time this dataset was created.

  used             The amount of space consumed by this dataset and all its
  		   descendants.

  copies=1|2|3     Controls the number of copies of  data stored for this
		   dataset. These copies are in addition to any redundancy
		   provided by the pool (for example, mirroring or raid-z).
		   The copies are stored on  different  disks  if  possible.

  readonly=on|off  Controls whether this dataset can be modified.
  		   The default value is "off".

  checksum=on | off | fletcher2, | fletcher4 | sha256
                   Controls the checksum used to verify data integrity.
		   The default value is "on", which automatically selects
		   an appropriate algorithm. Disabling checksums is NOT
		   a recommended practice.

  compression=on | off | lzjb | gzip | ...
                  Controls the compression algorithm used for this dataset.
		  There are several algorithms available, more to be added
                  future releases. The default value is "off".

  ...
  ...
For these and other (not listed here) properties please refer to zfs manual
pages (run 'help zfs').

Note on Inheriting ZFS Properties
=================================
   All settable properties, with the exception of quotas and reservations,
   inherit their value from their parent folder.
   To list inheritable properties, type:

   ${prompt}setup $NMC::FOLDER vol1/a $NMC::PROPERTY inherit

   and press TAB-TAB

   Sub-folders always inherit properties of their parent folder unless
   these properties were explicitly modified.

   Here's an extended example:

   (a) Let's say, for the purposes of illustration, that there exists
   folder 'vol1/a' and its immediate sub-folder 'vol1/a/b'

   (b) Let's say, you have modified some ZFS property <prop_name>
   of 'vol1/a/b'.  This (local modification) in effect breaks
   inheritance of this particular property from the parent folder.

   (c) Finally, let's say you now want 'vol1/a/b' to start inheriting
   again this (locally modified) <prop_name> from its parent 'vol1/a'

   For this to happen, run:

   ${prompt}setup $NMC::FOLDER vol1/a $NMC::PROPERTY inherit <prop_name>

   This command effectively restores the system-default way for
   sub-folders to inherit their properties.


Note on Finding by Property
===========================
   To search appliance's objects based on (any combination of)
   property values, use NMC 'query' command. See 'query -h' for details.


Examples:
=========
1) To list modifiable properties for a folder '$zname', type:

${prompt}setup $NMC::FOLDER $zname $NMC::PROPERTY  <TAB><TAB>

The output will look like:

   checksum     devices      readonly     reservation  sharenfs
   atime        compression  quota        recordsize   setuid

2) To see all properties and their values, run:

${prompt}show $zname -v


See also: 'setup $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'setup $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'setup $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'show $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'show $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'show $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'create $NMC::ZFS_LUN'
See also: 'create $NMC::FOLDER'

See also: 'help zfs'
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='zpool_set_property_usage'><![CDATA[
$cmdline
Usage: [-s value]

  -s <value>	New volume's ${prop_name}property value.
  		If not specified via command line -s option, the value
		can be entered interactively.

Update volume's property.

This interface can be used to update both the properties of the
specified volume, as well as properties of all its folders - the latter
via inheritance. For instance, 'autoreplace' (see below) is the
property of the volume per se, while 'compression' is the property
of all volume's datasets, including all contained folders and zvols.
To make sure that all folders and zvols on a given volume are
compressed, simply update its 'compression' property. You can later
override the 'compression' value for any specific contained dataset.
The same is certainly true for all the rest ZFS properties of the
contained datasets.

The following are specific volume's read-only properties
(quoting from zpool(1m) manual page):

  available    Amount of storage available within the pool.

  capacity     Percentage of pool space used.

  health       The current health of the pool.
               Health can be "ONLINE", "DEGRADED", "FAULTED",
	       "OFFLINE", "REMOVED", or "UNAVAIL".

  guid         A unique identifier for the pool.

  size         Total size of the storage pool.

  used         Amount of storage space used within the pool.

The following are volume's modifiable properties:

  autoreplace=on | off

  This property controls automatic device replacement.
  The default value is "on".
  *********   Please Note! *******************************
  *     The default autoreplace value is "on"            *
  ********************************************************
  If set to "off", device replacement must be done by using
  'setup $NMC::VOLUME <name> replace-lun' command.


Finally, this same command can be used to update ZFS properties
of all contained datasets. For the corresponding help, run:

${prompt}setup $NMC::FOLDER <name> $NMC::PROPERTY -h


  To search appliance's objects based on (any combination of)
  property values, use NMC 'query' command.
  See 'query -h' for details.

Examples:

1) To list modifiable properties for a volume '$zname', type:

${prompt}setup $NMC::VOLUME $zname property  <TAB><TAB>

2) To disable autoreplace property:

${prompt}setup $NMC::VOLUME vol1 property autoreplace -s off

Or, you can do the same interactively, by running:

${prompt}setup $NMC::VOLUME vol1 property autoreplace

3) To see all volume's properties and their values:

${prompt}show $NMC::VOLUME $zname -v

or, the same:

${prompt}show $NMC::VOLUME $zname $NMC::PROPERTY


See also: 'setup $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'setup $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'setup $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'show $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'show $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'show $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'setup folder <name> property inherit'

See also: 'create zvol'
See also: 'create folder'

See also: 'help zpool'
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='zfs_inherit_property_usage'><![CDATA[
$cmdline
Usage:  [-N]

  -N    Non-recursively inherit a given property for a
        given folder.


Restore system-default inheritance of a given ZFS property.
The command clears the specified property, causing it to be inherited
from the parent folder. If the parent folder does not have this
property set, the default value is used.

Note:
=====
   All settable properties, with the exception of quota and
   reservation, inherit their value from their parent folder.

To list inheritable properties, type:

${prompt}setup folder <name> property <prop_name> inherit

and press TAB-TAB

Sub-folders always inherit properties of their parent folder unless
these properties were explicitly modified.
Note: default behavior is 'recursive'. You have to use -N option to
force inherit operation to be non-recursive.

Examples:

1) ${prompt}setup $NMC::FOLDER vol1/a/b $NMC::PROPERTY inherit sharenfs
   Inherit recursively property 'sharenfs' for the vol1/a/b and all descendent
   subfolders (like vol1/a/b/c) from parent folder vol1/a.

2) ${prompt}setup $NMC::FOLDER vol1/a/b $NMC::PROPERTY inherit sharenfs -N
   Inherit non-recursively property 'sharenfs' for the vol1/a/b from parent
   folder vol1/a. This operation doesn`t touch local modified 'sharenfs'
   propery off descendent subfolders


Here's an extended example:

  (a) Let's say, for the purposes of illustration, that there exists
  folder 'vol1/a' and its immediate sub-folder 'vol1/a/b'

  (b) Let's say, you have modified some ZFS property <prop_name>
  of 'vol1/a/b'. This (local modification) in effect breaks inheritance
  of this particular property from the parent folder.

  (c) Finally, let's say you now want 'vol1/a/b' to start inheriting
  this (locally modified) <prop_name> from its parent 'vol1/a'.

  For this to happen, run:

  ${prompt}setup $NMC::FOLDER vol1/a $NMC::PROPERTY inherit <prop_name>


The command (above) effectively restores the system-default way for
sub-folders to inherit their properties.

Note on Finding by Property
===========================
   To search appliance's objects based on (any combination of)
   property values, use NMC 'query' command. See 'query -h' for details.


See also: 'setup $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'setup $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'setup $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'show $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'show $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'show $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'create $NMC::ZFS_LUN'
See also: 'create $NMC::FOLDER'

See also: 'help zfs'
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_folder_ownership_usage'><![CDATA[
$cmdline
Usage: [-f filter]

  -f <filter>	Use filter. Note that filtering of user names is
  		especially useful when interacting with large
		LDAP/AD databases, especially since the LDAP or AD
		server may impose administrative limit on the size
		of the associated result set
		The following notation finds all users with the
		username starting with 'jo': -f jo
	   	See more examples below.

Modify folder ownership. This interfaces allows to change user and/or
group onwers without changing the permissions granted to the
owners.

The operation is RECURSIVE - the same ownership settings
are applied to all nested folders and directories (if any).
The operation is interactive - NMC will prompt you to specify
all the relevant parameters, and will help along the way.

To change permissions granted to folder's owners, use:

   ${prompt}setup folder $zname' acl

]]></template>
<template module='Setup' name='setup_folder_ownership_usage1'><![CDATA[

See also: 'show folder <name> acl'
See also: 'setup folder <name> acl'
See also: 'setup folder <name> acl reset'

See also: 'show appliance user'
See also: 'setup appliance user'
See also: 'show appliance $NMC::USERGROUP'
See also: 'setup appliance $NMC::USERGROUP'

]]></template>
<template module='Setup' name='setup_folder_acl_usage'><![CDATA[
$cmdline
Usage: [-f filter]

  -f <filter>	Use filter. Note that filtering of user names is
  		especially useful when interacting with large
		LDAP/AD databases, especially since the LDAP or AD
		server may impose administrative limit on the size
		of the associated result set
		The following notation finds all users with the
		username starting with 'jo': -f jo
	   	See more examples below.

Modify or delete existing, or add new entries to the folder's
Access Control List (ACL). Use this interface to grant or revoke access
permissions for existing local as well as LDAP/AD managed users and
groups.

The operation is RECURSIVE - the same ACL settings are applied to
all nested folders and directories (if any).

The operation is interactive - NMC will prompt you to specify
all the relevant parameters, and will help along the way.
You can grant special permissions to a number of users and groups.
You can also revoke permissions. At any point during the
interactive operation press 'q' to apply the changes.

To reset the folder's ACL to system defaults, use:

   ${prompt}setup folder $zname acl reset

   or, skipping confirmation:

   ${prompt}setup folder $zname acl reset -y


To change folder ownership without changing the owners' permissions,
use:

   ${prompt}setup folder $zname' ownership

]]></template>
<template module='Setup' name='setup_folder_acl_usage1'><![CDATA[

NMC will guide you along the way to perform a number of access control
related actions. Run the command 'setup folder <name> acl' once
and interactively change access controls for any number of specific
users and groups. You could also change system default (POSIX)
access controls for the folder - the ACLs that are associated with
'owner\@', 'group\@', and 'everyone\@'.

To delete any given ACL entry, select DELETE and press Enter.
See examples below.

Press 'q' (quit) at any time to finish modifications and apply the
changes.

Examples:

1) Interactively modify Access Control List (ACL):
   ${prompt}setup folder vol1/a acl

   A sample NMC interactive session and output follows below:

${prompt}setup folder vol1/a acl
Entity type                     : user
Change permissions for user     : abc
Permissions                     : read_xattr, read_attributes, ...
============== vol1/a  (user owner: abc, group owner: other) ==============
ENTITY       ALLOW                          DENY
owner@       add_file, add_subdirectory,
             append_data, execute,
             list_directory, read_data,
             write_acl, write_attributes,
             write_data, write_owner,
             write_xattr

group@       execute, list_directory,       add_file, add_subdirectory,
             read_data                      append_data, write_data

everyone@    execute, list_directory,       add_file, add_subdirectory,
             read_acl, read_attributes,     append_data, write_acl,
             read_data, read_xattr,         write_attributes, write_data,
             synchronize                    write_owner, write_xattr

group:staff  read_attributes, read_xattr

user:abc     list_directory, read_acl,
             read_attributes, read_data,
             read_xattr

group:other  add_file, add_subdirectory,
             append_data, execute,
             write_data


2) Same as above but in addition query LDAP or AD server database
   for users and/or groups starting with 'new':

   ${prompt}setup folder vol1/a acl -f new

   Note:
   =====
        Filtering of user names (the -f option) may speed
	up interaction with LDAP/AD databases and also may
	help to circumvent the administrative limitation
        on the number of records returned by the
	LDAP or AD server.


3) Reset Access Control List to system defaults:

   ${prompt}setup folder vol1/a acl reset


4) Delete ACL entry for user 'abc' (see example 1 above):

   ${prompt}setup folder vol1/a acl

   When prompted by NMC, first select 'user',
   then select 'abc', and finally select DELETE.


See also: 'show folder <name> acl'
See also: 'setup folder <name> acl reset'

See also: 'setup folder <name> ownership'
See also: 'show folder <name> ownership'

See also: 'show appliance user'
See also: 'setup appliance user'
See also: 'show appliance $NMC::USERGROUP'
See also: 'setup appliance $NMC::USERGROUP'

]]></template>
<template module='Setup' name='rename_jbod_model_usage'><![CDATA[
$cmdline
Usage: [-v new-vendor] [-p new-product]

  -v <vendor>         new vendor
  -p <product>        new product

This command enables you to change JBOD model, in case it was detected
incorrectly.

Model name consist of two parts: <vendor>-<product>. To see current model and
the model that is returned via SES protocol run 'show jbod <name> model'
or 'show jbod <name> -v'

You may want to specify -v or -p option to execute command in non-interactive
mode. If you execute command without these options, you are promted to choose
vendor and product in interactive mode from the list of supported models. If
you can't find the required JBOD in the list, then select 'other' and enter
a value manually.

See also: 'show jbod <jbod-name> model'

See also: 'show jbod <jbod-name> -v'
See also: 'show jbod -v'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_lun_slotmap_usage'><![CDATA[
$cmdline
Usage: [-f new-slotmap]

  -f <new-slotmap>    import and merge factory mapping from the file
                      <new-slotmap>
  -y                  skip confirmation dialog by automatically responding Yes

Setup a new physical slot to disk mapping. By default, the
operation will auto-detect not yet mapped disks and enable blinking,
so that physical slot can be identified and programmed in.

Use -f option to load/import preset factory mapping.

Slot map has the following outline:

  { disk GUID => slot number }

This map, as well as JPEG image of the box (with drive slots shown
and enumerated) is used then by the appliance's UI to perform
related monitoring and management (including fault management)
operations.

Note that appliance's UI does not need to be re-built to work
with a new slot mapping. Additional mappings can be added over time.

The slotmap facility supports SCSI only devices. The latter includes:
an absolute majority of existing storage devices, and in particular:

 * Disk drives
 * Tape drives
 * Tape library changer devices
 * iSCSI devices
 * FireWire devices

SCSI device can be identified by on-disk label and serial number.
Most SCSI devices have a globally unique manufacturer supplied
idenfifier called GUID.

The following FOUR basic types of GUID are detected by $NZA::PRODUCT:

  a. IEEE Extended (eg 20030003BA27D517)
     direct-attach FC luns: T3/T4, A5x00, SE3510

  b. IEEE Registered (eg 500000E011460730), and
     SAS, SATA, FC luns attached via SAN

  c. IEEE Registered Extended
     (eg 600A0B8000254D3E00000A0845E4F906)
     MPxIO-enabled SAS, SATA and FC luns

  d. iSCSI format
     (eg 010000144F3A645200002A004674C1EB)
     MPxIO-enabled by default


See also: 'show lun slotmap'

See also: 'show lun'
See also: 'help'

]]></template>
<template module='Setup' name='setup_lun_cache_usage'><![CDATA[
$cmdline

View, enable and disable write-cache and/or read-cache on the device.
The command is restricted only to those drives that provide on-device
caches and support SCSI compliant way to enable and disable them.

As a first step, run 'setup lun cache show' to see which devices do have this
capability. The following sample output shows two write-cache capable
devices:

  LUN           WRITE-CACHE  READ-CACHE
  c1t0d0        unsupported  unsupported
  c1t1d0        unsupported  unsupported
  c1t2d0        disabled     enabled
  c1t3d0        disabled     enabled


Important Note:
==============
    Prior to the version 1.0.3 $NZA::PRODUCT used to automatically
    disable all on-disk write caches. Starting with the 1.0.3 this
    default behavior (controlled via 'disable_write_cache' NMS
    property) has changed. The 'disable_write_cache' is now
    by default set to zero (that is, false).

    The appliance itself provides sophisticated caching
    mechanisms and ensures data integrity under all possible workloads.

    Note however that running the appliance with on-disk write-caches
    enabled (which is the new default) and with another server property
    called 'sys_zfs_nocacheflush' set to non-zero
    (which is NOT the default value and is generally not recommended)
    is risky, in particular when using older SATA-I drives.


See also: 'show lun'
See also: 'show zvol'
See also: 'setup zvol'

See also: 'setup usage'
See also: 'help'


]]></template>
<template module='Setup' name='zvol_promote_usage'><![CDATA[
$cmdline

Promotes a cloned $NMC::ZFS_LUN to no longer be dependent on its "origin" snapshot.
This makes it possible to destroy the $NMC::ZFS_LUN that the clone was created from.
The clone parent-child dependency relationship is reversed, so that the
"origin" $NMC::ZFS_LUN becomes a clone of the specified $NMC::ZFS_LUN.

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='zvol_create_usage'><![CDATA[
$cmdline
Usage: [zvol-pathname] [-s devsize] [-b blocksize] [-S] [-T]
       [-c compression]

Creates $NMC::ZFS_LUN - a virtual block device of the given size,
based on the given volume. The primary $NMC::ZFS_LUN usage is to provide
an easy iSCSI integration. To create a new block device and immediately
share it via built-in iSCSI target, simply use the -T option.

Note that ${NMC::ZFS_LUN}s can be shared via iSCSI and secondly,
can be used as additional swap devices (swap areas),
that can be utilized by the appliance's virtual memory subsystem to
handle out-of-memory scenarios.

To add or delete swap areas, use 'setup appliance swap' command.
Note however that if the appliance lacks physical memory, it is always
preferable to add more RAM rather than extend swap;
the latter is a last resort option.

The size of the device (-s <devsize> option) is automatically rounded
up to the nearest 128 Kbytes to ensure that the new device has an
integral number of blocks regardless of blocksize.

More examples are given below.

As always, you can either specify some or all of the parameters via
command line, or enter them interactively.

   zvol-pathname    $NMC::ZFS_LUN fully qualified pathname is no
                    different from folders' pathnames, and has the
                    following outline:
                    <volume_name>[/folder_name/]<zvol_name>,
                    for instance: 'vol1/zvol1', 'vol1/a/zvol1'
                    See usage examples below.

  -b <blocksize>    Block size of the device

  -s <devsize>      Maximum size of the device, e.g.: 100GB, 500M, 100K
                    Note: You can grow the maximum size of the zvol at
                    any time, be executing:

                    ${prompt}setup zvol <zvol-name> property volsize

  -S                Creates a "sparse" (that is, thinly provisioned)
                    device with no initial reservation.
                    The effective used size is limited by the specified
                    <devsize> - that is, the size specified via -s option.

                    Note 1:
                    =======
                    By default, if the -S option is omitted, the entire
                    <devsize> is allocated at device creation time.

                    Note 2:
                    =======
                    Irrespectively of whether or not the -S option is used,
                    the actual used space on the device is reflected
                    by its 'used' property. This is consistent with
                    appliance's folders and volumes.

  -c                Controls the compression algorithm used for this dataset.
                    Default is "on". Setting compression to "on" uses the
                    lzjb compression algorithm. The lzjb compression
                    algorithm is optimized for performance while providing
                    decent data compression. Currently, "gzip" is equivalent
                    to "gzip-6".

Examples:

1) ${prompt}create $NMC::ZFS_LUN vol1/new-block-device -s 1TB -S
   Create virtual block device based on volume 'vol1' with maximum
   capacity of 1TB; do not reserve space for it.

2) ${prompt}create volume vol2 $NMC::ZFS_LUN new-block-device -s 500M
   Create virtual LUN based on volume 'vol2', allocate 500MB of 'vol2'
   space for it right away.

3) ${prompt}create $NMC::ZFS_LUN vol1/zvol1 -s 3TB -S
   Create a new thinly provisioned device which will grow up to
   3TB.

4) ${prompt}create $NMC::ZFS_LUN vol1/folder1/zvol1 -s 3TB -S
   Same as above, but creates $NMC::ZFS_LUN under folder named
   'vol1/folder1'.

5) ${prompt}setup volume vol1 $NMC::ZFS_LUN create folder1/zvol1 -s 3TB -S
   Same as above


See also: 'show zvol'
See also: 'share zvol'
See also: 'unshare zvol'

See also: 'setup iscsi'
See also: 'setup zvol <zvol-name> property'
See also: 'setup network service iscsi-target'

See also: 'setup appliance swap'

See also: 'destroy zvol'
See also: 'setup zvol'
See also: 'setup volume'

See also: 'setup iscsi target tpgt create'

See also: 'setup usage'

]]></template>
<template module='Setup' name='zvol_destroy_usage'><![CDATA[
$cmdline
Usage: [-y] [-R] [-f] [-r]

  -r    Recursively destroy all snapshots of the specified $NMC::ZFS_LUN

  -R    Recursively destroy all dependents, including snapshots and
        cloned $NMC::ZFS_LUN devices, if any.

  -f    Force unmount even if there are open files.
        Caution! A forced unmount can result in a loss of data.

  -y	Skip confirmation dialog by automatically responding Yes


Destroy a specified "$NMC::ZFS_LUN" device.

Caution! Extreme care should be taken when applying either
the -r or the -R or the -f options.

Examples:

1) ${prompt}setup $NMC::ZFS_LUN vol1/iscsi_dev1 destroy -y

2) ${prompt}destroy volume vol1 $NMC::ZFS_LUN iscsi_dev1 -y
   Same as above

3) ${prompt}destroy $NMC::ZFS_LUN vol1/iscsi_dev1
   Same as above, with confirmation

4) ${prompt}destroy $NMC::ZFS_LUN vol1/iscsi_dev1 -R
   Destroy 'vol1/iscsi_dev1' and all its snapshots, if any


See also: 'show zvol'
See also: 'create zvol'
See also: 'setup zvol'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='zfs_create_usage'><![CDATA[
$cmdline
Usage: [new-folder-pathname] [-o prop1=value1,prop2=value2,...]

Create a new folder. If not specified in the command line,
the new pathname can be entered interactively.
Folder pathname must start with a name of the volume, and can only contain
alphanumeric characters as well as underscore ('_'), dash ('-'), period ('.')
and forward slash ('/'). Use the '/' pathname separator to create folders
and sub-folders, in one shot.


  -o <creation_time_props>  Comma-separated list of properties and
     their values specified at folder's creation time.

   One important creation time property is:
   casesensitivity = sensitive | insensitive | mixed
   (see example below).

   From zfs(1m):
       casesensitivity indicates whether the file name matching
       algorithm used by the file system should  be case-sensitive (default),
       case-insensitive, or allow a combination of both styles of matching.

   For complete list of properties, see zfs(1m) man page or run
   'help zfs'.

   See also: 'show folder <pathname> -v'
   See also: 'setup folder <pathname> property -h'

Examples:

1) ${prompt}create folder vol1/a/b/c
   Create folder a/b/c of the volume vol1. Depending on the availability
   of vol1/a and vol1/a/b, this will create either all 3 sub-folders,
   or two, or just one.

2) ${prompt}setup folder create vol1/a/b/c
   Same as above.

3) ${prompt}setup volume vol1 folder create a/b/c
   Same as above.

3) ${prompt}create folder vol1/a/b/c -o compression=on,quota=1G
   Same as above, but in addition set two 'vol1/a/b/c' properties
   at creation time:
      * use compression (compression=on)
      * limit at 1GB the total storage space for
        the folder and its sub-folders (quota=1G)

4) ${prompt}create folder vol1/a/b/c -o compression=on,quota=1G,recordsize=4k
   Same as above, but in addition specifies a 4KB block size for files
   in the new folder.

5) ${prompt}create folder vol1/qqq -o casesensitivity=insensitive
   Create a folder that can be used in a mixed NFS and CIFS
   environment, whereby both CIFS and NFS clients can simultaneosly
   access the folder's files and directories and the the file name
   matching algorithm used by the file system is case-insensitive
   (as CIFS requires).

6) ${prompt}show vol1/a -r
   Show 'vol1/a' and its sub-folders, recursively.

7) ${prompt}show vol1/a -r -v
   Same as above, but generates a detailed output
   including all sub-folder properties and their values.


See also: 'show folder'
See also: 'destroy folder'
See also: 'setup folder <pathname> property'

See also: 'create volume'
See also: 'create zvol'

See also: 'query folder'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='zfs_destroy_usage'><![CDATA[
$cmdline
Usage: [-y] [-r] [-R] [-f]

  -y	Skip confirmation dialog by automatically responding Yes

  -r    Recursively destroy all dependent datasets and snapshots
        ("children" of the given folder, if any).

  -R    Recursively destroy:
        - all dependent (nested) datasets,
        - all snapshots of the specified folder
        - all cloned folders.

  -f    Force unmount even if there are open files.


Destroy a specified folder. Examples:

Caution! Extreme care should be taken when applying either
the -r or the -R or the -f options.

1) ${prompt}setup folder vol1/a destroy -y
   Destroy folder vol1/a and all its sub-folders. Also, stop and destroy
   all storage services that use and/or depend on vol1/a

2) ${prompt}destroy folder vol1/a
   Same as above, with the only difference that the confirmation will be
   requested.

See also: 'create folder'
See also: 'setup usage'

See also: 'help'

]]></template>
<template module='Setup' name='zfs_clone_usage'><![CDATA[
$cmdline
Usage:

Clone a folder or a $NMC::ZFS_LUN from a specified snapshot.

A clone is a writable folder or $NMC::ZFS_LUN that at creation (cloning) time is
identical to the snapshot it is cloned from. As with snapshots, creating a
clone is nearly instantaneous, and initially consumes no additional space.

Clones can only be created from a snapshot.

    Note on a dependency between a snapshot and its clone(zfs(1m):
    =============================================================
    When a snapshot is cloned, it creates an implicit dependency
    between the parent (the snapshot) and the child (cloned folder
    or cloned $NMC::ZFS_LUN).
    Even though the clone is created somewhere else in the dataset
    hierarchy, the original snapshot cannot be destroyed as long as
    a clone exists. The "origin" property exposes this dependency,
    and the destroy command lists any such dependencies, if they
    exist.


]]></template>
<template module='Setup' name='zfs_clone_usage1'><![CDATA[
Examples:

1) Clone a given folder 'vol1/a' from its snapshot 'vol1/a\@today';
   name the clone 'vol1/a-clone2'

   ${prompt}setup folder vol1/a snapshot \@today clone
   New clone pathname : vol1/a-clone2
   NAME           USED    AVAIL   REFER  MOUNTED QUOTA  COMPRESS
   vol1/a-clone2  0       926M    50.0M  yes     none   off

2) Same as above, based on a full snapshot name:

   ${prompt}setup snapshot vol1/a\@today clone
   New clone pathname : vol1/a-clone2
   NAME           USED    AVAIL   REFER  MOUNTED QUOTA  COMPRESS
   vol1/a-clone2  0       926M    50.0M  yes     none   off

3) Same as above, via TAB-TAB from volume to folder to snapshot:

   ${prompt}setup volume vol1 folder a snapshot \@today clone vol1/a-clone2

4) Clone a $NMC::ZFS_LUN from its fully qualified snapshot:

   ${prompt}setup snapshot vol1/a/zvol1\@snap1
   Option ?  clone
   New clone pathname : vol1/a/b/zvol-clone
   LUN ID        TPGT  Size   Reservation Volume      Share Cnx# Swap
   vol1/*clone   -     1M     none        vol1        none     0 no

5) Let's now try to destroy the snapshot used to clone a dataset
   in the examples above. The operation will fail with:

   ${prompt}destroy snapshot vol1/a\@today
   Destroy snapshot  'vol1/a\@today'?  Yes
   com.nexenta.nms.SystemCallError: cannot destroy 'vol1/a\@today':
   snapshot has dependent clones

See also: 'create $NZA::AUTO_SYNC_PLUGIN' (*)
See also: 'run $NZA::AUTO_SYNC_PLUGIN' (*)
See also: 'destroy $NZA::AUTO_SYNC_PLUGIN' (*)

See also: 'create folder'
See also: 'destroy folder'
See also: 'show folder'

See also: 'show snapshot'
See also: 'create snapshot'
See also: 'setup snapshot <name> rollback'

See also: 'help keyword clone'
See also: 'help keyword snapshot'

See also: 'help data-replication'

See also: 'setup appliance checkpoint <name> snapshot'
See also: 'show appliance checkpoint <name> snapshot'

See also: 'setup usage'
See also: 'help'

   Note:
        (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Setup' name='snapshot_create_usage'><![CDATA[
$cmdline
Usage: [snapshot name] [-r]

Create snapshot either at the current volume/dataset location or
by specifying a fully-qualified name.

  -r    Recursively create snapshots of all descendent datasets.
        The operation is atomic: all recursive snapshots correspond
	to the same moment in time.

Examples:

1) ${prompt}create snapshot vol1/a${str}today
   Take snapshot of the vol1/a dataset (which could possibly be a
   folder or a $NMC::ZFS_LUN block device) and name it 'vol1/a${str}today'

2) ${prompt}setup volume vol1 folder a create snapshot today
   Same as above.

3) ${prompt}setup folder vol1/a create snapshot today
   Same as above.

4) ${prompt}create snapshot
   Specify dataset and snapshot name interactively.


See also: 'create $NZA::AUTO_SNAP'
See also: 'setup snapshot <name> clone'

See also: 'help keyword clone'
See also: 'help keyword snapshot'

See also: 'setup appliance checkpoint <name> snapshot'
See also: 'show appliance checkpoint <name> snapshot'

See also: 'show snapshot'
See also: 'destroy snapshot <snapshot-name>'
See also: 'setup usage'

]]></template>
<template module='Setup' name='snapshot_destroy_usage'><![CDATA[
$cmdline
Usage: [-y] [-r] [snapshot name]

  -y	Skip confirmation dialog by automatically responding Yes

  -r    Destroy recursively. Destroy all snapshots with a given
        <snapshot name> in descendent folders.

Destroy the specified snapshot(s).

Examples:

1) ${prompt}destroy snapshot vol1/a${str}today
   Destroy snapshot 'vol1/a${str}today'

2) ${prompt}destroy snapshot vol1/a${str}today -y
   Same as above, with the only difference that confirmation is skipped

3) ${prompt}destroy vol1/a snapshot today -y
   Same as above

4) ${prompt}setup vol1/a snapshot today destroy -y
   Same as above

5) ${prompt}destroy snapshot vol1/a${str}today -r
   Recursively destroy snapshots with a given name'.
   Assuming, snapshot vol1/a/b/c${str}today exists, it will be
   destroyed as well since folder vol1/a/b/c is a descendant of vol1/a.

6) ${prompt}destroy vol1/a snapshot today show
   Instead of destroying, show the specified snapshot first (and then
   possibly destroy..). This is a time-saving tip; the alternative
   would be to type 'show vol1/a snapshot..' command separately.

See also: 'show snapshot'
See also: 'create snapshot'

See also: 'create folder'
See also: 'destroy folder'
See also: 'show folder'

See also: 'setup appliance checkpoint <name> snapshot'
See also: 'show appliance checkpoint <name> snapshot'

See also: 'create $NZA::AUTO_SNAP'
See also: 'destroy $NZA::AUTO_SNAP'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='snapshot_rollback_usage'><![CDATA[
$cmdline
Usage: [-y] [-r] [-R] [-f]
    -y	  Skip confirmation dialog by automatically responding Yes
    -r    Recursively destroy snapshots more recent than the one specified
    -R    Recursively destroy more recent snapshots, as well as clones of
          those snapshots
    -f    Force unmounts

Rollback a folder to its specified snapshot. All data that has changed
since  the  snapshot  is  discarded, and the folder reverts to the
state at the time of the snapshot.

By default, the command refuses to roll back to a snapshot
other than the most recent one. In order to do so, all intermediate
snapshots must be destroyed by specifying the  -r  option. The folder
is unmounted and remounted, if necessary.

Examples:

1) ${prompt}create snapshot vol1/a${str}today
   Take snapshot of the vol1/a.

   <make some changes inside the folder 'vol1/a' ... >

   ${prompt}setup folder vol1/a snapshot ${str}today rollback
   In combination with the actions above, this will rollback the vol1/a
   to the point in time when vol1/a${str}today was taken.


2) ${prompt}create snapshot vol1/a${str}yesterday

   <make changes "A" inside the folder 'vol1/a' ... >

   ${prompt}create snapshot vol1/a${str}today

   <make changes "B" inside the folder 'vol1/a' ... >

   ${prompt}setup folder vol1/a snapshot ${str}yesterday rollback -r
   This will rollback all the changes denoted as "A" and "B" above,
   and generally roll the folder back to the point in time when
   vol1/a${str}yesterday was taken.


See also: 'create folder'
See also: 'destroy folder'
See also: 'show folder'
See also: 'show snapshot'
See also: 'destroy snapshot'

See also: 'setup appliance checkpoint <name> snapshot'
See also: 'show appliance checkpoint <name> snapshot'

See also: 'create $NZA::AUTO_SNAP'

]]></template>
<template module='Setup' name='snapshot_rename_usage'><![CDATA[
$cmdline
Usage: [-r] new_name
    -r   Rename all the subfolder's snapshots recursively. Snapshot is the only dataset that can be renamed recursively.

Examples:

1) ${prompt}setup snapshot vol1\@nameA rename nameB
   Rename snapshot vol1\@nameA to vol1\@nameB.

2) ${prompt}setup snapshot vol1\@nameA rename
   Rename snapshot vol1\@nameA to the name, that you input in the dialog.

See also: 'show snapshot'
See also: 'destroy snapshot'
See also: 'create $NZA::AUTO_SNAP'

]]></template>
<template module='Setup' name='snapshot_schedule_usage'><![CDATA[
$cmdline
Usage: 	[-y] [-r] [-R]
	[-x latest-suffix] [-X]
	[-p period] [-k keep_value]
	[-e exclude]
	[-D day] [-T time] [-i interval]
	[-u custom_name]
	[-I comment]

  -y		Skip confirmation by automatically answering Yes
  -r		Recursive - snapshot all nested folders
  -R		Non-recursive - do NOT snapshot nested folders
  -p <period>	Interval of time between automatic local snapshots
  -k <keep_value>	Snapshot retention policy. Parameter defines the number
			of snapshots on the appliance. Specifies snapshot retention
			policy. The following format may also be used: YDHMS, for
			example 1y10d12h30m10s means store snapshots 1 year 10 days
			12 hours 30 minutes and 10 seconds. 10d - means store
			snapshots for 10 days.
  -e <pattern1|patt2|...>	Exclude folders matching specified
			pattern, or list of patterns.
			Pattern syntax is the same used in Unix shells
			for file/directory name filtering. For instance,
			use -e *.old  to skip generating snapshots for
			folders with extension .old.
			Use '|' delimiter to specify more than one
			matching pattern.
			The -e (exclude) option is only useful in
			combination with recursive (-r) snapshots.
			Use single or double quotation marks to specify
			multiple exclude patterns, for instance:
			     -e 'data/home/joe|data/home/mike'
			     -e "data/home/joe|data/home/mike"
			     * NOTE *
			By default, all folders are included, and
			the -e option is assumed to be omitted.
			This option is not inquired via
			NMC 'create $service' dialogs. You can either
			specify folders to be excluded via command line
			(and -e option) at service creation time,
			or via NMC 'setup $service <name> property'
			at any later time.


  -x <latest suffix>	It is often convenient to have the latest
			(the most recent) snapshot have a fixed name.
			The appliance produces those fixed-named
			snapshots with a certain (default) suffix:
			"$NZA::SNAP_LATEST_SUFFIX"
			You can override this default with -x option.
			The -x option allows to use custom suffix for
			the latest snapshot.
			    * NOTE *
			Similar to the previous one, the -x option
			is not inquired via
			NMC 'create $service' dialogs. You can either
			specify an alternative naming for the most
			recent snapshot via command line
			(and the -x option) at service creation time,
			or via NMC 'setup $service <name> property'
			at any later time.

  -X		Don't create latest snapshot. See -x <latest suffex>
			option for more information.

  -T <time of day>	Time of the day (e.g.: 3am, 6:45pm)
			Not applicable to hourly snapshots
  -D <day of month/wk>	Day of the month (1..31) or week (Mon, Tue, etc) as
			appropriate to the interval
  -i <interval>	Auto-generate snapshots periodically at the
			specified time interval, one of:
			$interval

  -u <custom_name>	Give service custom name. Name may exist of lower-
			case letters, numbers and '-', '_' signs.

  -I <comment>	Any text. Optional comment that helps identify the
  			purpose of this service, its criticality, business
			related aspects, etc.
			Informational comment can be modified or added
			after the service is created.
			Hint: to see all modifiable properties, run:
			'setup $NZA::AUTO_SNAP <name> property' and press
			TAB-TAB
			Use this option to add any relevant context.

$summary

Multiple snapshot policies can be defined per volume or folder as long as
they are of different intervals (daily, hourly, etc). Any inherited snapshot
policy can be overwritten on any sub-folder level, so its best to define
a common policy at the top level and only define differences in children
where necessary.

Once created, $NZA::AUTO_SNAP instance can be:

 * modified             see 'setup $NZA::AUTO_SNAP <name> property'
 * saved:		see 'setup appliance configuration'
 * run on-demand:	see 'run $NZA::AUTO_SNAP'
 * destroyed:		see 'destroy $NZA::AUTO_SNAP'
 * restored:		see 'setup appliance configuration'
 * disabled, enabled, etc.

Also, it is possible to view the service's current status (online,
maintenance, etc.), run-state (idle, running), and log files via
'show $NZA::AUTO_SNAP' set of commands.

The following are "non-interactive" usage examples. When creating
$NZA::AUTO_SNAP (or any other) storage service instance,
you can  always "fallback" into dialog mode by running and fill out
the requested fields interactively. Moreover, a combination of NMC
recording facility (see 'record -h') and 'create $NZA::AUTO_SNAP'
dialogs gives the best of both:

 * interactive (guided) filling-out of the $NZA::AUTO_SNAP parameters
and
 * ability to execute the "recorded" command many times,
   on many different $NZA::PRODUCT appliances.


Examples:

${prompt}setup $NZA::AUTO_SNAP create -r -p 1 -T 12am -k 30 -i daily
or, the same:
${prompt}create $NZA::AUTO_SNAP -r -p 1 -T 12am -k 30 -i daily

  This would establish a daily snapshot run at midnight for all
  volumes and child volumes on the system. 30 daily snapshots are kept.

${prompt}setup $NZA::AUTO_SNAP vol1/ create -r -p 4 -k 2d -i hourly

  This would establish snapshots to run every 4 hours, keeping 2 days
  worth, on volume 'vol1' with any children volumes of 'vol1' also
  inheriting the snapshot policy.

${prompt}create $NZA::AUTO_SNAP vol1/users/ -p 1 -k 1d -i hourly -R

  If hourly snapshots were defined for vol1/, this would setup and
  override the policy for vol1/users/ to keep 24 hourly snapshots,
  for 1d day.

${prompt}create $NZA::AUTO_SNAP vol1/ -e "scratch" -r -p 4 -k 1 -i hourly

  This is an alternative volume definition used to completely avoid
  a snapshot for the folder named "scratch" when defining snapshots
  for 'vol1'.

See also: 'create $NZA::AUTO_SYNC_PLUGIN' (*)
See also: 'create $NZA::AUTO_TIER'
See also: 'setup appliance configuration'
See also: 'record'
See also: 'setup usage'

   Note:
        (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Setup' name='setup_appliance_auth_keys_usage'><![CDATA[
Add or delete authentication key.
The following shows 'add' and 'delete' usage and examples:


]]></template>
<template module='Setup' name='add_appliance_auth_keys_usage'><![CDATA[
$cmdline
Usage: [-k key] [-v value]

  -k key         Specifies key identifier
  -v value       Specifies RSA key value

Add new key to the client key authorization table

For a background on $NZA::PRODUCT authentication, please run:

   ${prompt}show appliance $NMC::client_auth -h


See also: 'setup appliance $NMC::client_auth dbus-iptable'
See also: 'show appliance $NMC::client_auth'

]]></template>
<template module='Setup' name='remove_appliance_auth_keys_usage'><![CDATA[
$cmdline

Delete existing authentication key

For a background on $NZA::PRODUCT authentication, please run:

   ${prompt}show appliance $NMC::client_auth -h


See also: 'setup appliance $NMC::client_auth dbus-iptable'
See also: 'show appliance $NMC::client_auth'

]]></template>
<template module='Setup' name='setup_appliance_auth_iptable_usage'><![CDATA[
Add a new 'allow' or 'deny' record to the appliance's dbus iptable.
The following shows 'allow' and 'deny' usage and examples:


]]></template>
<template module='Setup' name='allow_appliance_auth_iptable_usage'><![CDATA[
$cmdline
Usage: mask | ip

Allow access from the specified IP address or subnet.

  mask | ip    - IPv4 mask or IPv4 address

For a background on $NZA::PRODUCT authentication, please run:

   ${prompt}show appliance $NMC::client_auth -h

Examples:

1) Allow access from IP 192.168.1.112
   ${prompt}setup appliance $NMC::client_auth dbus-iptable add allow 192.168.1.112

2) Allow access from subnet 192.168.1.*
   ${prompt}setup appliance $NMC::client_auth dbus-iptable add allow 192.168.1.*


See also: 'show appliance $NMC::client_auth'
See also: 'setup appliance $NMC::client_auth keys'

See also: 'setup appliance'

]]></template>
<template module='Setup' name='deny_appliance_auth_iptable_usage'><![CDATA[
$cmdline
Usage: mask | ip

Deny access from the specified IP address or subnet.

  mask | ip    - IPv4 mask or IPv4 address

For a background on $NZA::PRODUCT authentication, please run:

   ${prompt}show appliance $NMC::client_auth -h

Examples:

1) Deny access from IP 192.168.1.112
   ${prompt}setup appliance $NMC::client_auth dbus-iptable add deny 192.168.1.112

2) Deny access from subnet 192.168.1.*
   ${prompt}setup appliance $NMC::client_auth dbus-iptable add deny 192.168.1.*


See also: 'show appliance $NMC::client_auth'
See also: 'setup appliance $NMC::client_auth keys'

See also: 'setup appliance'

]]></template>
<template module='Setup' name='remove_appliance_auth_iptable_usage'><![CDATA[
$cmdline
Usage: mask | ip

Remove all client authentication records associated with the
specified IP address or subnet.

  mask | ip    - IPv4 mask or IPv4 address

For a background on $NZA::PRODUCT authentication, please run:

   ${prompt}show appliance $NMC::client_auth -h

Examples:

1) Remove all allow/deny records for IP 192.168.1.112
   ${prompt}setup appliance $NMC::client_auth dbus-iptable delete 192.168.1.112

2) Remove all allow/deny records for MASK 192.168.1.*
   ${prompt}setup appliance $NMC::client_auth dbus-iptable delete 192.168.1.*


See also: 'show appliance $NMC::client_auth'
See also: 'setup appliance $NMC::client_auth keys'

See also: 'setup appliance'

]]></template>
<template module='Setup' name='install_appliance_plugin_usage'><![CDATA[
$cmdline
Usage: [-v] [-y] [-C] [plugin1 plugin2 ...]

  [plugin1 [plugin2 ...]]     Space separated list of plugins to install -
                              one or more plugin package names

  -v           Enable verbose mode
  -y           Skip confirmation dialog by automatically responding Yes
  -C           Cleanup upgrade caches (note: cleanup is generally not
               required and can be skipped in most cases)

Install one or more $NZA::PRODUCT pluggable modules (plugins) from the
central software repository.

Installation of a pluggable module (plugin) is a transactional operation,
which includes:

   1) searching for the specified plugin in the remote repository;
   2) downloading it;
   3) installing the plugin and all its software dependencies;
   4) registering it with the appliance software.

The plugins can be specified in the command line, or can be entered
interactively, via NMC multiple-choice dialog.

A new system checkpoint is created and, at the end of the operation,
$NZA::COMPANY Management Server is automatically restarted.

Plugin software dependencies - that is, the plugin runtime dependencies
on other software packages and other plugins - are handled automatically.


See also: 'show plugin remotely-available'
See also: 'show plugin installed'

See also: 'setup appliance repository'
See also: 'setup appliance upgrade'
See also: 'show plugin'

]]></template>
<template module='Setup' name='uninstall_appliance_plugin_usage'><![CDATA[
Usage: [-y] [-c]

  -y	Skip confirmation dialog by automatically responding Yes

  -c	leanup the stored information and snapshots.

Unregister and uninstall the specified plugin. Note that plugin
dependencies will not be removed. The operation will restart
$NZA::COMPANY Management Server.


See also: 'show plugin remotely-available'
See also: 'show plugin installed'

See also: 'show plugin'
See also: 'setup plugin install'

See also: 'setup appliance repository'
See also: 'setup appliance upgrade'

]]></template>
<template module='Setup' name='install_appliance_plugin'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Appliance Reboot Required.

     In order to complete the configuration change, your system
     needs to be restarted. Until you do so, new functionality may
     be unavailable.

]]></template>
<template module='Setup' name='setup_appliance_netmasks_usage'><![CDATA[

See also: 'show appliance netmasks'

See also: 'setup appliance hosts'
See also: 'setup appliance hostname'
See also: 'setup appliance domainname'
See also: 'setup appliance init'

]]></template>
<template module='Setup' name='setup_appliance_hosts_usage'><![CDATA[

See also: 'show appliance hosts'

See also: 'setup appliance hostname'
See also: 'setup appliance domainname'
See also: 'setup appliance init'

]]></template>
<template module='Setup' name='setup_hostname_usage'><![CDATA[
$cmdline
Usage: [hostname] [-y]

 hostname	- New hostname of this appliance
 -y		- Do not ask question, apply the hostname

If not specified in the command line, the new hostname is inquired
interactively. You can than type it over the old (current) hostname.

See also: 'setup appliance hosts'
See also: 'setup appliance domainname'

]]></template>
<template module='Setup' name='setup_swap_usage'><![CDATA[
$cmdline
$s1

Examples:

1) Extend system swap area with a new block device
   ${prompt}create $NMC::ZFS_LUN
   	... follow prompts to create a new block device;
	    see 'create $NMC::ZFS_LUN -h' for details...
   ${prompt}setup appliance swap add     <TAB><TAB>
   	... select device from the list and press Enter
   ${prompt}show appliance swap

	Note:
	=====
	If the appliance lacks physical memory, it is always
	preferable to add more RAM rather than extend swap;
	the latter is a last resort option.

2) Delete swap area and free (from being used as a swap) the corresponding
   block device:
   ${prompt}setup appliance swap delete     <TAB><TAB>
   	... select device from the list and press Enter

See also: 'show appliance swap'
See also: 'create $NMC::ZFS_LUN'
See also: 'show $NMC::ZFS_LUN'

]]></template>
<template module='Setup' name='setup_repository_usage'><![CDATA[
Usage: [-c]
$cmdline

   -c      cleanup local APT cache information

Administer appliance APT repository

See also: 'show appliance checkpoint'
See also: 'setup appliance upgrade'
See also: 'show appliance version'
See also: 'show appliance license'
See also: 'show plugin'

]]></template>
<template module='Setup' name='setup_register_usage'><![CDATA[
$cmdline
Usage: [-U key] [-u <license module>]

Register $NZA::PRODUCT product. The license key can be obtained
at http://www.nexenta.com. Use -u or -U options to upgrade evaluation
license to commercial.

  -U <key>              Upgrade $NZA::PRODUCT licensing module automatically
                        using provided key
  -u <module file>	Upgrade $NZA::PRODUCT licensing module by explicitly
                        specifying the module's file name

For more information on registering the software or upgrading your existing
license, see $NZA::PRODUCT Quick Start Guide or User Guide documentation.

See also: 'show appliance license'

See also: 'setup appliance repository'

]]></template>
<template module='Setup' name='setup_nmc_usage'><![CDATA[
$cmdline

View and edit NMC configuration file.

Note #1:
========
   A preferable way to change NMC configuration is via
   'option' command (see 'option -h' for details).

Note #2:
========
   All changes made in the NMC configuration file are
   appliance-wide, persistent, and immediate.
   With 'option' you can make both persistent changes, or
   simply change settings for the current NMC session.

See also: 'option'
See also: 'help options'

See also: 'setup appliance nms'

]]></template>
<template module='Setup' name='setup_nms_restart_usage'><![CDATA[
$cmdline
Usage: [-f] [-y]

  -f	Force server restart operation, even if failed to gain exclusive
        access
  -y	Skip confirmation dialog by automatically responding Yes

  -q    Quick restart: restart only the Nexenta Management Server itself,
        without necessarily terminating all currently active
	dependent services, such as $NZA::AUTO_SYNC_PLUGIN(*) and $NZA::AUTO_TIER.

Safely restart $NZA::COMPANY Management Server (NMS). Advanced usage - expert
mode is required to carry out this operation. See 'option' or 'help'
for the current settings and details on all NMC options, including
'expert_mode'.

NMS restart completely removes the process from memory and then
loads and starts it all over again. During the corresponding interval
of time all NMS clients (including but not limited to storage and
fault management services) will have their requests delayed.


See also: 'setup appliance nms'
See also: 'setup appliance nmc'

   Note:
        (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Setup' name='edit_nms_config_usage'><![CDATA[
$cmdline

Edit $NZA::COMPANY Management Server configuration file.

Beware: advanced usage only.

It is recommended to use 'setup appliance nms property'
instead.

See also: 'setup appliance nms property'
See also: 'show appliance nms property'

]]></template>
<template module='Setup' name='__pool_nms_usage'><![CDATA[
Background
==========
The 3.x generation of the appliance software includes a "server pool"
facility, with multiple management servers simultaneously processing
concurrent requests.

The Management Server (NMS) is just another resource that can be added
to a pool (of servers) using this NMC command.

This provides for better response time, concurrent execution of
simultaneous client requests from multiple users, and,
last but not the least - server redundancy and
overall reliability of management adnimistrative access to the appliance.

Additional servers may be added ("pooled") at NMS startup, based on
the following NMS properties:

 * srvpool_cnt_initial
 * srvpool_cnt_max

To view the pool, along with properties and usage statistics
of individual management servers, run:

 ${prompt}show appliance nms pool

or see 'show appliance nms pool -h' for the man page and examples.

]]></template>
<template module='Setup' name='start_nms_usage'><![CDATA[
$cmdline

Add $NZA::COMPANY Management Server to the pool of management servers.

]]></template>
<template module='Setup' name='start_nms_usage1'><![CDATA[
To remove a server from the pool:

 ${prompt}setup appliance nms pool remove


See also: 'show appliance nms pool'
See also: 'setup appliance nms property'
See also: 'show appliance nms property'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Setup' name='kill_nms_usage'><![CDATA[
$cmdline
Usage: [-i ID]

  -i <ID>    pool ID, the ID of the server in the pool of management servers

Remove $NZA::COMPANY Management Server from the pool of management servers.

]]></template>
<template module='Setup' name='kill_nms_usage1'><![CDATA[
To add a new server to the pool:

 ${prompt}setup appliance nms pool add


See also: 'setup appliance nms pool disable'

See also: 'show appliance nms pool'
See also: 'setup appliance nms property'
See also: 'show appliance nms property'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Setup' name='disable_srvpool_usage'><![CDATA[

Disable server pool and stop using additional management servers.

]]></template>
<template module='Setup' name='disable_srvpool_usage1'><![CDATA[
To re-enable server pool:

  ${prompt}setup appliance nms pool enable


See also: 'show appliance nms pool'
See also: 'setup appliance nms pool'
See also: 'setup appliance nms property'
See also: 'show appliance nms property'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Setup' name='enable_srvpool_usage'><![CDATA[

Enable server pool and start using additional management servers.
This will run 'srvpool_cnt_initial' management servers, that is,
the number of servers specified as NMS property 'srvpool_cnt_initial':

  ${prompt}setup appliance nms property srvpool_cnt_initial

or:

  ${prompt}show appliance nms property srvpool_cnt_initial

]]></template>
<template module='Setup' name='enable_srvpool_usage1'><![CDATA[
To re-enable server pool:

  ${prompt}setup appliance nms pool enable


See also: 'show appliance nms pool'
See also: 'setup appliance nms pool'
See also: 'setup appliance nms property'
See also: 'show appliance nms property'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Setup' name='restart_srvpool_usage'><![CDATA[

Restart server pool. The operation restarts management servers in the pool;
as such it is effectively a combination of:

  ${prompt}setup appliance nms pool disable

followed by:

  ${prompt}setup appliance nms pool enable

]]></template>
<template module='Setup' name='restart_srvpool_usage1'><![CDATA[
Pool Management
===============
You can manage the pool of management servers via this
'setup appliance nms pool' interface, and "srvpool_..." properties;
the latter includes:

 * srvpool_cnt_initial
 * srvpool_cnt_max
 * srvpool_port_range_min
 * srvpool_affinity_timeout
 * srvpool_interface_serialization_timeout


See also: 'show appliance nms pool'
See also: 'setup appliance nms pool'
See also: 'setup appliance nms property'
See also: 'show appliance nms property'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_mailer_usage'><![CDATA[
$cmdline
Usage: [-p value]

  -p <value>	New $prop value

Specify $prop. The value can be entered either interactively, or via
-p option in the command line.

]]></template>
<template module='Setup' name='setup_mailer_usage1'><![CDATA[
Configure appliance's mailing facility. Note that a properly configured
mailer is critical for the appliance's fault management and reporting.

]]></template>
<template module='Setup' name='setup_mailer_usage2'><![CDATA[
Setting up the mailer consists of the following steps:

1) ${prompt}setup appliance mailer smtp_server
   Sets up outgoing SMTP server (may optionally contain SMTP port separated
   by colon). Example: smtp.mail.yahoo.com or smtp.mail.yahoo.com:465

2) ${prompt}setup appliance mailer smtp_user
   Enter SMTP server username.

3) ${prompt}setup appliance mailer smtp_password
   Enter SMTP server password.

4) ${prompt}setup appliance mailer smtp_addresses
   Enter e-mail address(es), which will be used to send reports and
   notifications. Example: ann${str}yahoo.com

Steps 2 and 3 are optional. Once the mailer is configured, you will
start receiving fault notifications, volume and service reports
according to configured schedule.

You can specify separate addresses for sending fault and statistic reports.

5) ${prompt}setup appliance mailer smtp_addresses_faults
   Enter separate e-mail address(es) for fault notifications.
   Example: ann-faults${str}yahoo.com

6) ${prompt}setup appliance mailer smtp_addresses_stats
   Enter separate e-mail address(es) for statistics.
   Example: ann-stats${str}yahoo.com

Steps 5 and 6 are optional. In that case, e-mail address(es)
specified in step 4 will be used for fault notifications, reports and statistics.

To review the mailer setting, run:

7) ${prompt}show appliance mailer

Sample output follows:

smtp_timeout            : 30
smtp_password           : xxxxxxx
smtp_server             : mail.xyz-corp.com
smtp_addresses          : ann${str}xyz-corp.com,jim${str}xyz-corp.com
smtp_addresses_faults   : ann-faults${str}xyz-corp.com,jim-faults${str}xyz-corp.com
smtp_addresses_stats    : ann-stats${str}xyz-corp.com,jim-stats${str}xyz-corp.com
smtp_user               : sysadmin
smtp_auth               : Plain
smtp_cc                 :
smtp_to                 : support\@nexenta.com

See also: 'setup trigger'
See also: 'setup reporter'
See also: 'setup collector'

See also: 'show appliance mailer'
See also: 'show faults'
See also: 'show faults all-appliances'

See also: 'setup usage'

See also: 'help'

]]></template>
<template module='Setup' name='setup_appliance_driver_usage'><![CDATA[
$cmdline
Usage: [-f filename]

  -f filename	Specify driver file name. File needs to be downloaded
                using URL suggested by 'show appliance driver' command

Install third party driver.

See also: 'query'
See also: 'setup usage'

]]></template>
<template module='Setup' name='setup_appliance_remote_access_usage'><![CDATA[
$cmdline
Usage: [-m] [-k secret-key]

  -m             Monitor the support session activity
  -k secret-key  Enable encrypted and password protected session via
                 provided secret key (usually provided by support
		 engineer)
  -p port        Use specific support port (usually provided by support
		 engineer)

Enable remote access to the appliance for a support engineer.
The appliance will be connecting to the support organization allowing
NMC-level console access for advanced troubleshooting.

See also: 'support'

]]></template>
<template module='Setup' name='setup_reboot_usage'><![CDATA[
$cmdline
Usage: [-f] [-y] [-r]

  -f	Force server $action operation, even if failed to gain exclusive
        access
  -r    Re-enumerate devices on reboot
  -y	Skip confirmation dialog by automatically responding Yes

Safely $action $NZA::PRODUCT appliance.

See also: 'setup appliance nms restart'

]]></template>
<template module='Setup' name='setup_appliance_checkpoint_usage'><![CDATA[

NMC provides a set of interfaces to conveniently list, create, and
destroy system checkpoints which utilize ZFS snapshots (and the fact
that the operating system itself resides on ZFS). Typically, a new
checkpoint is created at appliance's software upgrade time. NMC
interfaces (listed below) allow to create a new checkpoint at any
time. For information on $NZA::PRODUCT transactional checkpoints,
system rollback, safe and live upgrades - please see User Guide
documentation.

  TERMS
  =====
  Throughout the product, the terms "system folder",
  "root filesystem", and "checkpoint" are used interchangeably.

  The appliance's root filesystem (rootfs) contains OpenSolaris
  kernel and system configuration.


See also: 'setup appliance checkpoint'
See also: 'setup appliance checkpoint <checkpoint-name> destroy'
See also: 'setup appliance checkpoint <checkpoint-name> activate'

See also: 'setup appliance checkpoint restore'

See also: 'setup appliance checkpoint <checkpoint-name> property'

See also: 'help data-replication'

See also: 'setup appliance upgrade'
See also: 'setup appliance reboot'

See also: 'show appliance checkpoint'

]]></template>
<template module='Setup' name='setup_checkpoint_rootfs_usage'><![CDATA[
$cmdline
Usage:

Create a new rollback checkpoint.

]]></template>
<template module='Setup' name='setup_destroy_cloned_rootfs_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	Skip confirmation dialog by automatically responding Yes

See also: 'show appliance checkpoint'

]]></template>
<template module='Setup' name='setup_activate_cloned_rootfs_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	Skip confirmation dialog by automatically responding Yes

]]></template>
<template module='Setup' name='runner_action_all_usage'><![CDATA[
$cmdline
Usage:

]]></template>
<template module='Setup' name='runner_action_all_usage1'><![CDATA[

See also: 'show faults'
See also: 'show network all-faults'

See also: 'show appliance runners'
See also: 'show appliance auto-services'
See also: 'show appliance nms locks'

See also: 'help runners'
See also: 'help fault-management'

]]></template>
<template module='Setup' name='runner_set_property_usage'><![CDATA[
$cmdline
Usage: [-y] [-p value]

For a given $s1 - view or modify its property value. To display all
${type}'s properties, run 'show $type $runner -v'

  -y		Skip confirmation dialog by automatically responding Yes.
  -p <value>	New property value


See also: 'show $type $runner'

See also: 'setup $type reset'
See also: 'setup $type $runner reset'

See also: 'help runners'
See also: 'help terms'

]]></template>
<template module='Setup' name='runner_reset_usage'><![CDATA[
$cmdline
Usage:

Reset $type '$runner' properties to system defaults.

Note: this operation requires NMS restart.

See also: 'setup $type reset'
]]></template>
<template module='Setup' name='runner_reset_usage1'><![CDATA[
$cmdline
Usage:

Reset properties of all ${type}s to system defaults. To reset a specific
$type, type 'setup $type', press TAB-TAB and make a selection.

Note: this operation requires NMS restart.

See also: 'setup $type <name> reset'
]]></template>
<template module='Setup' name='runner_reset_usage2'><![CDATA[

See also: 'show $type'
See also: 'setup $type'

See also: 'show faults'
See also: 'show network all-faults'

See also: 'help runners'
See also: 'help fault-management'
See also: 'help terms'

]]></template>
<template module='Setup' name='setup_trigger_clear_usage'><![CDATA[
$cmdline
Usage: [-e fault_id]

  -e fault_id	Clear only specified fault

If no fault_id is specified, the command clears all outstanding faults
(if any) generated by '$trigger' fault trigger.

Otherwise, clear only the specified fault.

Run 'show faults' to see all faults in the current appliance
- some/all of them may require administrative action.

See also: 'setup appliance mailer'
See also: 'setup usage'
See also: 'show faults all-appliances'

See also: 'help fault-management'
See also: 'help runners'

]]></template>
<template module='Setup' name='setup_indexer_create_usage'><![CDATA[
$cmdline
Usage: [folder pathname]

Create indexer for the specified folder. The indexer will run
at scheduled intervals, and index all folder's snapshots created
in the period of time since the indexer's previous run.


See also: 'show indexer'
See also: 'destroy indexer'

See also: 'help index'
See also: 'help runners'

]]></template>
<template module='Setup' name='setup_indexer_destroy_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	Skip confirmation dialog by automatically responding Yes

Destroy indexer for the specified folder. The operation will effectively
stop the corresponding instance of the "indexing runner" and remove
(cleanup) all the accumulated indexing data that corresponds to
the specified folder.

See also: 'show indexer'
See also: 'create indexer'

See also: 'help index'
See also: 'help runners'

]]></template>
<template module='Setup' name='setup_indexer_search_usage'><![CDATA[
$cmdline
Usage: [search terms]

Retrieve probabilistic search results for given [search terms].

Examples of queries:

1) search for documents which possibly contains mentioning of
   words 'mathematics' and 'geometry':

   ${prompt}setup $NZA::RUNNER_INDEXER school/books search mathematics AND geometry

2) search for documents which possibly contains mentioning of
   words 'physics' or 'theory':

   ${prompt}setup $NZA::RUNNER_INDEXER school/books search physics OR theory

See also: 'help index'
See also: 'help runners'

]]></template>
<template module='Setup' name='script_runner_create_usage'><![CDATA[
$cmdline
Usage:	[-n pathname]
	[-i interval] [-p period] [-D day] [-T time]
	[-C condition_command]
	[-E condition_expression]
	[-I info]
	[-o outfile]
	[-a]
	[-e]
	[-c cmdline_opts]
	[-y]

  -n <filename>		A fully qualified filename of the script to run,
			periodically and on condition.
			Examples of filenames: 'vol1/scripts/a.nmc', 'vol2/b'
			Note that the actual script must exist prior to
			creating $NZA::RUNNER_SCRIPT object, to run
			this script (periodically and on condition).
			****
			To create actual custom script, use NMC 'record'
			operation or appliance's built-in editor.
			****

  -i <interval>		Time interval, one of the following enumerated values:
  			$interval

  -p <period>		One or more "time intervals" (above). For instance,
  			an hourly service with a period equal 4 will run
			every 4 hours.

  -T <time of day>	Time of the day (e.g.: 3am, 6:45pm)

  -D <day of the month>	Day of the month (1..31)

  -I <info>		Any text. Optional comment that helps identify the
  			purpose of this custom script.
			Use this option to add any relevant context.
			Informational comment can be modified or added
			after the service is created.
			Hint: to see all modifiable properties, run:
			'setup $NZA::RUNNER_SCRIPT <name> property' and press
			TAB-TAB

  -C <condition_cmd>
  -E <condition_expr>	Optional pre-condition that is evaluated each time
  			prior to running the script <filename>.

  			Condition consists of two distinct elements:
			   (1) <condition_cmd> - any valid NMC command that
			       does not require interation with user
			       (for instance, any valid 'show' command)

			   (2) <condition_expr> - an arbitrary logical
			       expression based on the values produced by
			       the <condition_cmd>

			Please see "Condition" below.

			Use empty value to execute the script unconditionally,
			strictly based on schedule.

			If non-empty, the condition is evaluated each time
			prior to execution of the script; a false or undefined
			value of the condition causes the script to skip
			this run. For details and examples, please refer to
			"Condition" (below).

  -o <outfile>		Output file. The option allows to redirect output to a
  			the specified file.
			Note that by default <outfile> will be created anew
			every time the script runs, and the results of the
			previous run will be lost. Use the option -a (next)
			to append the output to the <outfile>

			Please see also two related options (-a and -e)
			below.

  -a			Append output to <outfile> (see previous).
  			The option is valid only in combination with -o, and
			allows to append the script's output to a given file,
  			rather than overriding an existing file.
			Default value: true.

  -e			Send email to the appliance's administrator, with
  			<outfile> (see above) as an attachment.
  			The option is valid only in combination with -o
			and causes the $NZA::RUNNER_SCRIPT to automatically
			generate email with the attached <outfile>.
			For details on appliance's mailer, please see:
			'show appliance mailer'
			Default value: false (email not generated).

  -m			Start each run of the script by logging the current
  			date and time. Default value: true.

  -c <cmdline_opts>	An arbitrary string of options that is passed as is
  			(without any parsing or validation) to the script.
			Examples:
				  -p 123 -rt pool1
				  folder vol1/a -H --skip-empty pool1
				  somevalue anyvalue -q -t
			etc.
			Note that there is no need to embed the string
			into double or single quotes.

  -y			Skip confirmation by automatically answering Yes.


Schedule custom script for periodic and/or conditional execution.

This NMC interface creates a new instance of $NZA::RUNNER_SCRIPT - which
is a persistent wrapper object "on top" of an arbitrary custom script.
You can create any number of $NZA::RUNNER_SCRIPT instances, to run your
custome script periodically, on demand, and on condition.

Please see section "Condition" below for details on how to specify an
arbitrary custom condition to execute your custom script.

   * Question: where this "custom" script comes from in the first place?

   * Answer: you could use NMC top-level 'record' command, to start,
     stop and view NMC recording sessions.

     Recorded NMC sessions are plain text files (scripts) stored under
     $NMC::Util::RECORDINGS_DIR directory. Please see 'record -h'
     for details.

     Once recorded, the NMC session can be re-played any number of times,
     manually (via '$NMC::run_now') and automatically - via $NZA::RUNNER_SCRIPT.

     You could also simply copy paste commands from the console and store
     them in a text file.  Or copy a prepared script onto the appliance.
     In short, NMC does not place any restrictions on how the script
     that the $NZA::RUNNER_SCRIPT will be running is created


The $NZA::RUNNER_SCRIPT provides a power and flexibility to:

   * support any schedule

     You can schedule your script to run every minute, every hour at a
     given minute of the hour, every few hours, every day at a certain time,
     bi-weekly at a given day/time, and so on.

   * modify any parameter at runtime

     You can modify the schedule, condition to run the script, command line
     options that are being passed to the script, and in general, each
     and every of the tunable parameters - at any time

     To do this, simply run:

     ${prompt}setup $NZA::RUNNER_SCRIPT <name> property

   * temporarily disable and re-enable the $NZA::RUNNER_SCRIPT, run
     it on demand ("run-now"), etc.

     In general, since $NZA::RUNNER_SCRIPT IS A "runner", it supports
     all the corresponding generic interfaces and funcionality.
     For general information on the appliance's "runners", please see:

     ${prompt}help runners


     ===============
     $cond_sect
     ===============

The $NZA::RUNNER_SCRIPT interfaces support powerful and flexible mechanism
to specify custom conditions (to execute your custom script). If specified,
condition is evaluated at run-time, prior to executing your custom scipt.

The condition consists of two distinct elements:

   (1) <condition_cmd> - any valid NMC command that does not require
       interation with user (the first choice would probably be
       any valid 'show' command)

   (2) <condition_expr> - an arbitrary logical expression based
       on the values produced by the <condition_cmd>


NOTE
====
   Throughout the rest of this article we'll use "condition_cmd" and
   "condition_expr" to indicate the "command" and the "logical expression"
   parts of the condition, respectively.


Let's start with an example that uses NMC command 'show appliance memory'
(<condition_cmd> = 'show appliance memory')

NMC 'show appliance memory' produces a fixed output, similar to one
shown below:

	PROPERTY                  VALUE
	total                   : 4096MB
	unusable                : 8MB
	kernel                  : 3992MB
	locked                  : 0
	free                    : 96MB
	paging                  : 0
	used                    : 3992MB

Examples of valid logical expressions based on this particular command
follow:
	* [free,2] > 100MB
	* [ free, 2 ] > 100MB
	* [ ^free, 2 ] > 100MB
	* [ free, : ] > 100MB
	* [5,2] > 100MB
	* [5 , 2 ] > 100MB
	* [5 , : ] > 100MB

All the 7 expressions above are equivalent, and all 7 express a simple
condition: the amount of free memory is greater than 100MB.

The <condition_expr> syntax supports the following ubiquitous canonical
symbols:

	eq, ne		   for string comparison,

	<, >, ==, <=, >=   for number comparison,

	and, or, not 	   to express compound conditions

   Here's how it works
   -------------------

To evaluate <condition_expr>, the output of the NMC command
<condition_cmd> is interpreted in terms of rows and columns containing
non-empty values.

Both rows and non-empty elements within the rows are indexed starting
from 0 (zero).

In the example above, there are rows inxeded 0 to 7. Therefore,
the element [5,2] refers to the 2nd non-empty value in the 5th row.
This would be: '96MB'

As demonstrated by the example above, $NZA::RUNNER_SCRIPT condition
syntax supports variations that allow avoiding using exact indexes.
For instance, [ free, 2 ] refers to the row that contains substring
"free" and to an element with index = 2 in this row. That, again,
would be: '96MB'


To summarize, as far as the sample output of 'show appliance memory',
the following is true:

	Element		Refers To
	=========================
	[free,2] 	96MB
	[ free, 2 ] 	96MB
	[ ^free, 2 ] 	96MB
	[ free, : ] 	96MB
	[5,2] 		96MB
	[5 , 2 ] 	96MB
	[5 , : ] 	96MB


Please keep in mind that, as far as indexing an output of a given
NMC command <condition_cmd>, all non-empty elements without exception
are counted.

For instance, [5,1] in our example would point to a non-empty
element ":", while [0, 1] would refer to "VALUE" in the title
of the 'show appliance memory'  produced output.

In both cases, it would probably be a mistake to use these constants
in the logical expression <condition_expr>.

The "selected" elements of the NMC command <condition_cmd> can be
used to form an arbitrary logical expression, using any valid
combination of these elements and the following canonical symbols:

	eq, ne		   for string comparison,

	<, >, ==, <=, >=   for number comparison,

	and, or, not 	   to combine two or more conditions

Empty condition evaluates to true, and simply means that $NZA::RUNNER_SCRIPT
executes unconditionally, strictly based on assigned schedule.

Use 'evaluate' sub-command to evaluate and test the condition.

${prompt}setup $NZA::RUNNER_SCRIPT <name> evaluate

You can also modify the ${NZA::RUNNER_SCRIPT}'s condition at any time
via:

${prompt}setup $NZA::RUNNER_SCRIPT <name> property


See also: 'foreach'

See also: 'record'
See also: 'run recording'
See also: 'show recording'
See also: 'destroy recording'

See also: 'show $NZA::RUNNER_SCRIPT'
See also: 'setup $NZA::RUNNER_SCRIPT'
See also: 'help runners'

]]></template>
<template module='Setup' name='script_runner_destroy_usage'><![CDATA[
$cmdline
Usage: [-y]

  -y	Skip confirmation dialog by automatically responding Yes

Destroy the specified $NZA::RUNNER_SCRIPT.


See also: 'show $NZA::RUNNER_SCRIPT'
See also: 'setup $NZA::RUNNER_SCRIPT'

See also: 'setup $NZA::RUNNER_SCRIPT $name evaluate'
See also: 'setup $NZA::RUNNER_SCRIPT $name property'

See also: 'help runners'

]]></template>
<template module='Setup' name='script_runner_evaluate_usage'><![CDATA[
$cmdline
Usage:	[-v]

  -v          Verbose output

Evaluate $NZA::RUNNER_SCRIPT condition.

As stated elsewhere, the condition that defines whether to run the script
at a given scheduled time consists of two distinct elements:

   (1) <condition_cmd> - any valid NMC command that does not require
       interation with user (the first choice would probably be
       any valid 'show' command)

   (2) <condition_expr> - an arbitrary logical expression based
       on the values produced by the <condition_cmd>

Normally, the $NZA::RUNNER_SCRIPT condition is evaluated at run-time,
prior to executing the corresponding custom script. Please see:

${prompt}create $NZA::RUNNER_SCRIPT -h

for definitions of a "custom script", "condition", and examples.

You can use 'evaluate' sub-command to evaluate and test the condition.

You can also edit the ${NZA::RUNNER_SCRIPT}'s condition at any time
via:

${prompt}setup $NZA::RUNNER_SCRIPT $name property'

and then evaluate the resulting condition immediately (and check
whether the result is expected), without waiting for the script to
run at its scheduled time.


Example 1:

The following example assumes that a given instance of $NZA::RUNNER_SCRIPT
runs a script '/tmp/test', and is configured with the following two
elements of a condition (to run the /tmp/test):

  (*) condition_cmd  = show appliance memory
  (*) condition_expr = [ free, : ] > 220.5MB

NMC command 'show appliance memory' produces a fixed output, similar to the
one shown below:

	PROPERTY                  VALUE
	total                   : 4096MB
	unusable                : 8MB
	kernel                  : 3992MB
	locked                  : 0
	free                    : 204MB
	paging                  : 0
	used                    : 3992MB

In this example the [ free, : ] references 204MB - the value that is
used to substitute the former and evaluate the condition.

The following shows a sample output when running:

     ${prompt}setup script-runner /tmp/test evaluate
     SUBST: [5, 2] > 220.5MB => 204MB > 220.5MB
     EVAL: '213909504 > 231211008' = ''
     FALSE

Note that the condition (above) has evaluated to FALSE, which would
mean that if '/tmp/test' would be scheduled to run right at this moment,
it would skip this run.


Example 2:

Let's now use 'setup script-runner /tmp/test property' to change
the condition as follows:

  (*) condition_expr = [ free, : ] > 190MB

The resulting output may look as follows:

    ${prompt} setup script-runner /tmp/qq evaluate
    SUBST: [free, :] > 190MB => 200MB > 190MB
    EVAL: '209715200 > 199229440' = '1'
    TRUE

In this second example the condition = TRUE.


Example 3:

The following condition evaluates to true if the remaining capacity
of the volume 'vol1' is less than 30%

  (*) condition_cmd = show volume
  (*) condition_expr = [ vol1, 4 ] > 70%

This certainly relies on the fact that the output of 'show volume'
command looks as follows:

    ${prompt} show volume
    NAME                   SIZE    USED    AVAIL   CAP  HEALTH
    syspool                3.97G   2.12G   1.61G   53%  ONLINE
    vol2                   1008M   163K    976M    0%   ONLINE
    vol1                   1008M   100M    876M    9%   ONLINE

The same condition can be specified differently (see next).

Example 4:

  (*) condition_cmd = show volume vol1 -v
  (*) condition_expr = [ capacity, 1 ] > 70%


See also: 'show $NZA::RUNNER_SCRIPT'
See also: 'setup $NZA::RUNNER_SCRIPT'
See also: 'help runners'

]]></template>
<template module='Setup' name='service_action_all_usage'><![CDATA[
$cmdline
Usage: [-y] [-i interval] $add_option

  -y		  Skip confirmation dialog by automatically responding Yes

  -i <interval>	  Apply${action_str} selectively - filter services
		  $type_str.

]]></template>
<template module='Setup' name='service_action_all_usage1'><![CDATA[
  -f              Force $action - $action the service instance and
	          in addition, if the service is currently running,
		  terminate it immediately.
		  Without the -f (force) option, the operation
		  simply removes the service definition from the system
		  database. It will fail though to acquire the
		  corresponding lock if the service happens to be
		  running at the time.
		  The -f option is provided to "force" the '$action'
		  operation to proceed anyway.
	          Caution! Forcing the service to stop running may cause
		  partial and incomplete results.
		  The exact nature of these "results" depends on the
		  concrete service instance and timing.
		  Use the -f option sparingly!

]]></template>
<template module='Setup' name='service_action_all_usage2'><![CDATA[
  -f              Force $action - $action the service instance and
	          in addition, if the service is currently running,
		  terminate it immediately.
		  Without the -f (force) option, the operation
		  simply unschedules the service, as far as the future
		  scheduled execution is concerned, and marks the
		  service 'disabled'.
	          Caution! Forcing the service to stop running may cause
		  partial and incomplete results.
		  The exact nature of these "results" depends on the
		  concrete service instance and timing.
		  Use the -f option sparingly!

]]></template>
<template module='Setup' name='service_action_all_usage3'><![CDATA[
  -w              Wait for the service to complete. In certain cases
		  it maybe convenient to "dedicate" a given NMC session
		  to a given service run. The -w option blocks NMC
		  execution until the service completes its data
		  transfer, or fails. In the latter case, the error
		  will be displayed immediately.
		  Note that you can always open multiple NMC sessions,
		  and therefore, continue working in parallel.

]]></template>
<template module='Setup' name='service_action_all_usage4'><![CDATA[

For background, general information, features and differences between
appliance's 2nd tier data services, please see:

${prompt}help data-replication


See also: 'show $service'
See also: 'setup $service'
See also: 'create $service'

See also: 'setup usage'

See also: 'show appliance auto-services'

]]></template>
<template module='Setup' name='scrub_schedule_usage'><![CDATA[
$cmdline
Usage: [-p period] [-D day] [-T time] [-i interval] [-y]

  -i <interval>		Schedule $service with the specified time interval,
			one of: $interval.

  -p <period>		One or more "intervals" (above). For instance,
  			an hourly service with a period equal 4 will run
			every 4 hours.

  -T <time of day>	Time of the day (e.g.: 3am, 6:45pm)
  -D <day of the month>	Day of the month (1..31)

  -y			Skip confirmation by automatically answering Yes

Create new $NZA::AUTO_SCRUB service.

Scrubbing effectively exercises the entire volume and checks for any
errors of data in stable storage. Scrubbing can be reset to the beginning
if snapshots occur during its service period.

The auto-scrubbing service can be run daily, weekly, or monthly.
It applies to the entire volume. You should run this command from
within a volume tree to set scrubbing services for that volume.

Use -D and -T options to specify day of the month and time when to
run $service service. Note that hourly intervals are also supported -
if specified, the -D and -T will be ignored.

Once created, $NZA::AUTO_SCRUB instance can be:

   * modified           see 'setup $NZA::AUTO_SCRUB <name> property'
   * run on-demand:	see 'run $NZA::AUTO_SCRUB'
   * destroyed:		see 'destroy $NZA::AUTO_SCRUB'
   * saved:		see 'setup appliance configuration'
   * restored:		see 'setup appliance configuration'
   * disabled, enabled, etc.

Also, it is possible to view the service's current status (online,
maintenance, etc.), run-state (idle, running), and log files via
'show $NZA::AUTO_SCRUB' commands.

Examples:

1) Scrub appliance's system volume:
   ${prompt}create $service volume $NZA::SYSPOOL

   Or, type 'create $service volume', press TAB-TAB,
   and select a volume. The system volume will be
   listed at this point as one of the possible choices.

   Or, simply type 'create $service', press TAB-TAB,
   and make a selection..

2) Scrub all volumes on a weekly basis, on Saturdays at 4am:
   ${prompt}setup $service create -T 4am -D Sat -i weekly

3) Scrub volume vol1 on the first of each month, at 10:30pm:
   ${prompt}setup $service vol1/ create -T 10:30pm -D 1 -i monthly
   or, same:
   ${prompt}setup $service volume vol1 create -T 10:30pm -D 1 -i monthly

4) Scrub all volumes each hour, skip confirmation dialog:
   ${prompt}create $service -i hourly -y


See also: 'show $NZA::AUTO_SCRUB'
See also: 'setup $NZA::AUTO_SCRUB'

See also: 'setup $NZA::AUTO_SCRUB <name> property'

See also: 'destroy $NZA::AUTO_SCRUB'
See also: 'run $NZA::AUTO_SCRUB'

See also: 'create $NZA::AUTO_SCRUB'
See also: 'create $NZA::AUTO_SYNC_PLUGIN'(*)
See also: 'create $NZA::AUTO_TIER'

See also: 'setup appliance configuration'
See also: 'record'

See also: 'setup usage'

   Note:
        (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Setup' name='replication_bind_ssh_usage'><![CDATA[
$cmdline
Usage: [[user@]host[:port]] [-b] [-f]

  [user@]<host>[:port]   - Remote appliance hostname or IPv4 address
             with superuser name and port if any. If no superuser name
             assigned, default name is 'root'. Root name is assumed
             when two NexentaStor appliances need to be bound.
  -b                     - Bind the two appliances bidirectionally
  -f                     - Do not verify host by pinging

Create SSH "binding" between the local appliance and remote host,
unless already bound.

The command prompts for remote password matching ssh authorized keys
between the remote host and the local appliance.

Clustering, tiering over SSH, syncing (that is, running $NZA::PRODUCT
auto-sync service - see User Guide, Section "Terminology") over SSH,
and managing remote appliance over SSH - these are some of the
capabilities of the $NZA::PRODUCT appliance that rely on pre-existing trust.

SSH binding allows for remote management as well as strong cryptographic
verification of multi-host trusts, forming a network of ssh-bound hosts.

SSH binding is an operation that establishes a secure connection
to a remote host using the SSH protocol. Creating ssh binding between
two appliances is required when using appliance groups, clusters,
replication services, VMDC and other features of the appliance.


Examples:

 1) ssh-binding to another appliance inside local network:

    nmc\$ setup network ssh-bind 192.168.1.63
    Super-User password   : xxxx

 2) ssh-binding to another machine on remote location with non-standard
    SSH port:

    nmc\$ setup network ssh-bind jack\@nexentakr.dyndns.org:2022


See also: 'setup network ssh-unbind'
See also: 'show network ssh-bindings'

See also: 'setup network ssl-bind'

]]></template>
<template module='Setup' name='replication_unbind_ssh_usage'><![CDATA[
$cmdline
Usage: [[user@]host[:port]] [-b] [-f]

  [user@]<host>[:port] - Remote appliance hostname or IPv4 address
			 with user name and port
  -b                   - Unbind the two appliances bidirectionally
  -f                   - Force unbind even if remote is not reachable
			 at the time of operation

Undo SSH "binding" between local appliance and remote host.

See also: 'show network ssh-bindings'
See also: 'setup network ssh-bind'

See also: 'setup network ssl-bind'

]]></template>
<template module='Setup' name='replication_unbind_ssh'><![CDATA[

                          * * *
                          NOTICE

     Remote appliance '$hostname' is still present in the following
     group$s of appliances:
     $str

     You can continue using the appliance '$hostname' as part of
     the listed group$s, and for the 'fastswitch' operation.
     Use 'show group' and 'setup group' for more information."

]]></template>
<template module='Setup' name='_list_connections_and_warn'><![CDATA[

                          * * *
                     SECURITY NOTICE

     A number of still open connections between the current (local)
     appliance and the '$hostname' is: $n
     For security reasons, it is recommended to restart all local
     management clients, or reboot the appliance.

]]></template>
<template module='Setup' name='bind_ssl_usage'><![CDATA[
$cmdline
Usage: [-s hostname:port] [-a alias] [-c certfile]

Create SSL-LDAP "binding" between the appliance and remote LDAP server.

This interface allows to authenticate the appliance with LDAP server
using X.509 certificates.

There are two principal ways to create SSL-LDAP "binding":

 1) retrieve SSL certificate from a known LDAP server (Usage #1 below)
 2) use a local certificate file (see Usage #2)


Usage #1: [-s hostname:port] [-a alias]

  -s <hostname:port>	LDAP server hostname and port number.
                        Default port: 636
  -a <alias>		Certificate alias. If not specified, hostname
                        will be used

   Example #1:

   ${prompt}setup network ssl-bind -s ee-od1.mycorp.com:636


Usage #2: [-c certfile] [-a alias]

  -c <certfile>		Trusted remote host's certificate file
  -a <alias>		Certificate alias. Optional - if omitted,
	                the alias will be retrieved from the
			certificate file

   Example #2:

   ${prompt}setup network ssl-bind -c 0.cert -a abc

   This example assumes that '0.cert' certificate was previously
   copied (for instance, using NMC 'scp' command).


See also: 'setup network ssl-unbind'
See also: 'show network ssl-bindings'

See also: 'setup network ssh-bind'
See also: 'setup network ssh-unbind'

]]></template>
<template module='Setup' name='unbind_ssl_usage'><![CDATA[
$cmdline
Usage: <hostname> | -a <alias>

  <hostname>   	- Remove all certificates for the specified <hostname>
  <alias>	- Remove certificate identified by the specified alias

Undo SSL-LDAP "binding" between local and remote host.

This interface will remove X.509 certificates retrieved via
'setup network ssl-bind'.

There are two certificate removal methods: by Alias and by Hostname.
In the first case, you will be prompted to specify certificate's alias
If "Hostname" method is selected, all certificates for the specified
host will be removed'.


See also: 'setup network ssl-bind'
See also: 'show network ssl-bindings'

See also: 'setup network ssh-bind'
See also: 'setup network ssh-unbind'

]]></template>
<template module='Setup' name='replication_schedule_usage_tier'><![CDATA[
$cmdline
Usage: [-y] [-S snapshot] [-Y snapname] [-O] [-p period] [-s source]
       [-d destination] [-e patt1|patt2|...] [-D day] [-T time]
       [-i interval] [-r rsyncdepth] [-o options] [-l limit_rate]
       [-R]
       [-x latest-suffix] [-X]
       [-k keep_value]
       [-u custom_name]
       [-I comment]

  -y			skip confirmation by automatically answering Yes

  -s <source>		Replication source. If specified in the command
			line, the source of $NZA::AUTO_TIER replication
			contains the following 3 elements:
			   (a) protocol (a.k.a. data transport)
			   (b) hostname
			   (c) fully qualified source directory name
			and has the following layout:
			[<proto>://]<hostname>/<folder-name>

  -d <destination>	Replication destination. If specified in the
			command line, the destination of $NZA::AUTO_TIER
			replication contains the following 3 elements:
			   (a) protocol (a.k.a. data transport)
			   (b) hostname
			   (c) fully qualified destination directory name
			and has the following layout:
			[<proto>://]<hostname>/<folder-name>
                * TIP *
		        First time users and beginners: do not specify
			source and destination in the command line.
			Use NMC dialogs. Use NMC recording facility
			('record').

  -r <rsyncdepth>	RSYNC fanout depth

  -S <snapshot>		Replicate from (or same, tier from) a
			specified (tiering) snapshot.
			Snapshot must exist at moment of replication.
			The source host may be any ZFS-based system,
			e.g., Solaris, OpenSolaris, NexentaOS,
			$NZA::PRODUCT, etc.
                * NOTE *
			If -S is not specified in the command
			line, in presense of -y option (batch operation)
			the value of -S is assumed to be empty, which
		        effectively means not using specific snapshot
			as a tiering source.

  -Y <snapname>		Replicate from a temporary created snapshot.
			The source host may be any ZFS-based system,
			e.g., Solaris, OpenSolaris, NexentaOS,
			$NZA::PRODUCT, etc.
			This paramer  is optional, but recomended.
			It is used to guarantee that data is not modified
			between start and end time of tiering process.
			$NZA::AUTO_TIER creates a temporary snapshot at start
			and deletes it after replication is done.

  -e <pattern1|patt2|...>   exclude directories matching specified
			pattern, or list of patterns.
			The pattern syntax is the same used in Unix
			shells for file/directory name filtering.
			For instance, use:
			-e *.old      to skip replicating directories
			with extension .old.
			Use '|' delimiter to specify more than one
			matching pattern.
			The -e option is useful only in
			combination with recursive replication tasks.
			Use single or double quotation marks to specify
			multiple exclude patterns, for instance:
		         -e 'data/home/joe|data/home/mike'
		         -e "data/home/joe|data/home/mike"
                * NOTE *
			By default, all directories are included, and
			the -e option is assumed to be omitted.
			This option is not inquired via
			NMC 'create $NZA::AUTO_TIER dialogs. You can either
			specify directories to be excluded via command
			line (and -e option) at service creation time,
			or via NMC 'setup $NZA::AUTO_TIER <name> property'
			at any later time.

  -x <latest suffix>	It is often convenient to have the latest
			(the most recent) snapshot have a fixed name.
			The appliance produces those fixed-named
			snapshots with a certain (default) suffix:
			"$NZA::SNAP_LATEST_SUFFIX"
			You can override this default with -x option.
			The -x option allows to use custom suffix for
			the latest (the most recently taken) snapshot
			at the destination.
			This option requires -k option to be used, and
			will have no effect if the number of days
			at the destination is not defined or zero.
                * NOTE *
			Similar to the previous options, the -x option
			is not inquired via
			NMC 'create $NZA::AUTO_TIER' dialogs. You can either
			specify an alternative naming for the most
			recent snapshot via command line
			(and the -x option) at service creation time,
			or via NMC 'setup $NZA::AUTO_TIER <name> property'
			at any later time.

  -X			Don't create latest snapshot. See -x <latest suffex>
			option for more information.

  -1			Run only once, according to the schedule.
			This option allows to schedule $NZA::AUTO_TIER service
			to run just one time, at a scheduled time.
			After executing, the service automatically
			transitions to a disabled state.
			You can re-enable the service at any later
			time, to, again, execute it just once in
			accordance to its schedule.
                * NOTE *
			Similar to the previous option, the -1 option
			is not inquired via
			NMC 'create $NZA::AUTO_TIER' dialogs.
			If not specified in the command line,the service
			will execute periodically at scheduled intervals.

  -o <options>		specify extra protocol-specific tiering options

  -O		request for modification of protocol specific
                        extra options during interactive tiering creation

  -k <keep_value>		Parameter defines the number of snapshots on the
			appliance. The following format may also be used: YDHMS, for
			example 1y10d12h30m10s means store snapshots 1 year 10 days
			12 hours 30 minutes and 10 seconds. 10d - means store
			snapshots for 10 days. If not specified, the value is
			assumed to be zero, that is, by default snapshots at
			destination are not generated.
			Setting this parameter to a non-zero value
			is equivalent to running a separate $NZA::AUTO_SNAP
			service at the destination, synchronized to run
			immediately after tiering has finished.
                * NOTE *
			Omitting this option (default) or setting
			the value to zero means that the tiering service
			will *not* be generating snapshots at the
			destination.
			Otherwise, the service will generate one snapshot
			each time after it has run, and also remove older
			snapshot(s) once the total number of generated
			snapshots exceeds the specified <number>.


  -T <time of day>	Time of the day (e.g.: 3am, 6:45pm)
  -D <day of the month>	Day of the month (1..31)

  -i <interval>		Time interval.
			Units to measure the period of time between two
			consecutive runs of the service. For additional
			information, please see 'help data-replication'
			The value is one of:
			  * minute
			  * hourly
			  * daily
			  * weekly
			  * monthly

  -p <period>		One or more "intervals" (above). For instance,
			an hourly service with a period equal 4 will run
			every 4 hours.

  -R			Retry the service if it fails to execute because
			of the loss of connectivity with remote host.
			This option provides capability to automatically
			run the service prior to its next scheduled
			execution time, assuming the service has failed
			because of the loss of connectivity and when
			the remote host becomes reachable.

  -l <number>		In kbytes/s. Limits maximum speed of replication.
			Used to manually allocate resources during
			copying.

  -u <custom_name>	Give service custom name. Name may exist of lower-
			case letters, numbers and '-', '_' signs.

  -I <comment> 		Any text. Optional comment that helps identify the
			purpose of this service, its criticality, business
			related aspects, name of the destination, etc.
			Use this option to add any relevant context.
			Informational comment can be modified or added
			after the service is created.
			Hint: to see all modifiable properties, run:
			'setup $NZA::AUTO_TIER <name> property' and press
			TAB-TAB
  -a 		Copy ACLs if possible. The default value is 'no'.


Create $NZA::AUTO_TIER service and schedule it to run at a given
time/interval.

$NZA::PRODUCT $NZA::AUTO_TIER (tiering) service regularly and
incrementally replicates a given source directory (and all nested sub-
directories except those that are excluded - see exclude option)
to another storage pool, whether local or remote. By using snapshots on
the destination, this tiered copy may have arbitrarily different
retention and expiration policies and can be administered separately
from the source file system.

For background, general information, features and capabilities of the
appliance's 2nd tier data services, please see:

${prompt}help data-replication

For differences between $NZA::PRODUCT $NZA::AUTO_TIER and $NZA::AUTO_TIER
services, and related discussion, please also see F.A.Q. pages on the
website.


Both -s and -d use a protocol definition, usually one of $NZA::PROTO_RSYNC_SSH://,
$NZA::PROTO_RSYNC_NFS://, $NZA::PROTO_RSYNC:// and $NZA::PROTO_RSYNC_LOC://,
but when a protocol definition is omitted, it refers to a local path.
Paths support the use of '/*' to identify items (sub-folders and
sub-directories)  within a directory.

For $NZA::PROTO_RSYNC_SSH://, $NZA::PROTO_RSYNC_NFS://, $NZA::PROTO_RSYNC://
and $NZA::PROTO_RSYNC_LOC:// protocols, one can override the default rsync
options of '--delete --inplace --ignore-errors -vHlptgoD' with the -o flag.
The -O allows for defining this interactively.

All ssh protocol tiering requires previously defined ssh-binding.
Run 'show network ssh-bindings' to list all hosts
already ssh-bound to the local appliance.  Run 'setup network ssh-bind'
to setup a new ssh-binding.

Once created, $NZA::AUTO_TIER instance can be:

   * modified           see 'setup $NZA::AUTO_TIER <name> property'
   * saved:		see 'setup appliance configuration'
   * run on-demand:	see 'run $NZA::AUTO_TIER'
   * destroyed:		see 'destroy $NZA::AUTO_TIER'
   * restored:		see 'setup appliance configuration'
   * disabled, enabled, etc.

Also, it is possible to view the service's current status (online,
maintenance, etc.), run-state (idle, running), and log files via
'show $NZA::AUTO_TIER' commands.

===================================================================
           Note on the '/*' (dircontent) option
===================================================================
   Replication source URL may have two forms:
   1) <source URL>/.
   and
   2) <source URL>/*

   The difference between the two is easier to demonstrate on example.
   Let's say, the directory that is being replicated is 'vol1/a' and
   it contains sub-directory 'b':

   `-- vol1
       `-- a
            `-- b

    Let's now say that the destination is vol2/x, possibly
    on a different host. If the source is 'vol1/a',
    the resulting destination, after service has run, will look like:

   `-- vol2
       `-- x
            `-- a
            	`-- b

    However, if the source is 'vol1/a/*' (notice the '*'),
    result of the replication will look like:

   `-- vol2
       `-- x
           `-- b

    Easy to remember: '/*' means - only the content.
===================================================================

The following are "non-interactive" usage examples. When creating
$NZA::AUTO_TIER (or any other) storage service instance,
you can  always "fallback" into dialog mode by running and fill out
the requested fields interactively. Moreover, a combination of NMC
recording facility (see 'record -h') and 'create $NZA::AUTO_TIER'
dialogs gives the best of both:

 * interactive (guided) filling-out of the $NZA::AUTO_TIER parameters
and
 * ability to execute the "recorded" complete command many times,
   on many different $NZA::PRODUCT appliances.

Examples:

${prompt}setup $NZA::AUTO_TIER create -y -s $NZA::PROTO_RSYNC_NFS://nas-server/path/to/users/*  -d /data/users -r 1 -T 3am -i daily -p 1

  This would setup a daily tiering schedule, at a period of once per day,
  from the NFS-based file server nas-server to a local directory with a
  folder named "users" within /volumes/data. One must have created the
  "users" folder before hand. It recurses one level deep on the source
  volume to breaking up the periodic comparisons into manageable pieces.

${prompt}setup $NZA::AUTO_TIER create -y -s $NZA::PROTO_RSYNC_SSH://unix-server/path/to/users/* -d data/users -r 1 -i hourly -p 4

  This example runs every four hours, and runs rsync over the SSH protocol
  to pull the data from the remote unix-server and write to the local
  data/users directory.

${prompt}create $NZA::AUTO_TIER -y -s vol1/users/* -d $NZA::PROTO_RSYNC_SSH://ahost.college.edu/volumes/data/tmp1 -r 1 -i hourly -p 4

  Push data with rsync-over-ssh from folder "users" on local volume
  vol1 to the remote destination specified via -d option. Notice the
  alternative way to create management objects and services - by using
  'create' top-level command. Note also that $NZA::PROTO_RSYNC_SSH requires
  a fully-qualified name for the tiering destination.


See also: 'help data-replication'

See also: 'show $NZA::AUTO_SYNC_PLUGIN'(*)

See also: 'setup $NZA::AUTO_SYNC_PLUGIN<name> property' (*)

See also: 'help keyword clone'
See also: 'setup snapshot <name> clone'

See also: 'run $NZA::AUTO_SYNC_PLUGIN' (*)
See also: 'destroy $NZA::AUTO_SYNC_PLUGIN'

See also: 'create $NZA::AUTO_SYNC_PLUGIN' (*)
See also: 'setup $NZA::AUTO_SYNC_PLUGIN' (*)

See also: 'setup appliance configuration'
See also: 'record'

See also: 'setup usage'

   Note:
        (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Setup' name='service_set_property_usage'><![CDATA[
$cmdline
Usage: [-y] [-p value]

  -y	      Skip confirmation dialog by automatically responding Yes.
  -p <value>  New property value

]]></template>
<template module='Setup' name='service_set_property_usage1'><![CDATA[

For more information, please see 'create $service -h'

Note:
   To search $service instances based on (any combination of their)
   property values, use NMC 'query' command. See 'query -h' for details.

See also: 'show $service'
See also: 'run $service'
See also: 'create $service'

]]></template>
<template module='Setup' name='setup_aggregation_create_usage'><![CDATA[
$cmdline
Usage: [-l mode] [-n name] [-P policy] <dev1> <dev2> [dev3] ...

    -l <mode>   - LACP mode: off, active or passive. The default is off.
    -n <name>   - Link name for aggregation interface. The defaul is aggr<N>
    -P <policy> - Specifies the port selection policy to use for load spreading of outbound traffic.

Aggregate two or more network interfaces (links) into a single
(802.3ad aggregated) interface (data link). For the aggregated
interface to function, the second part of configuration must be
done at the switch that processes LACP.. (details on 802.3ad
can be found on the web).

Link Aggregation requires configuration on both the appliance and
the switch side. Note that link aggregation will NOT work if the
switch side is not set up, or set up incorrectly.

Supported LACP modes:

 Off mode     - The default mode for aggregations. LACP packets are
                not generated;
 Active mode  - The system generates LACP packets at regular intervals

 Passive mode - The system generates an LACP packets only when it
                receives an LACP packet from the switch. When both the
		aggregation and the switch are configured in passive
		mode, they cannot exchange LACP packets


Valid values for LACP-enabled aggregations are "Active" and "Passive."
The switch must be set with a compatible value. Compatible
appliance/switch LACP mode combinations are:

   appliance    switch
   ===================
   Active       Active
   Active       Passive
   Passive      Active


Note that not using LACP may be necessary for compatibility with
switches that either have LACP turned off or do not support 802.3ad.

For the aggregated link, at least two physical network devices
dev1 and dev2 must be specified.

The network interfaces that participate in the aggregation
won't show up individually if you run 'show network interface'.
The individual physical interfaces effectively "disappear" for all
administration intents and purposes; the only way to make them
"appear" again would be to destroy the aggregation:
${prompt}destroy network $NMC::AGGREGATION
or equivalent:
${prompt}setup network $NMC::AGGREGATION <aggr-name> destroy

Example:

${prompt}create network aggregation -l passive e1000g1 e1000g2
LINK            POLICY   ADDRPOLICY           LACPACTIVITY  LACPTIMER   FLAGS
aggr1           L4       auto                 passive       short       -----
${prompt}destroy network aggregation aggr1
Destroy aggregated interface 'aggr1' ?  Yes

See also: 'setup network interface'
See also: 'setup network routes'
See also: 'show network $NMC::AGGREGATION'

See also: 'setup usage'

See also: 'help'

]]></template>
<template module='Setup' name='setup_aggregation_policy_usage'><![CDATA[
$cmdline
Usage: [policy]

Specifies the port selection policy to use for load spreading of outbound traffic.
The policy specifies which dev object is used to send packets.
A policy consists of a list of one or more layers specifiers separated by commas.
A layer specifier is one of the following:

    L2 - Select outbound device according to source and destination MAC addresses of the packet.

    L3 - Select outbound device according to source and destination IP addresses of the packet.

    L4 - Select outbound device according to the upper layer protocol information contained in the packet.
         For TCP and UDP, this includes source and destination ports. For IPsec, this includes the SPI (Security Parameters Index.)

Any combination of these policies is also valid.
The default policy is L3,L4.

Example:

1) Modifying the policy for the $NMC::AGGREGATION:
${prompt}setup network $NMC::AGGREGATION <aggr-name> policy L2,L3

See also: 'setup network interface'
See also: 'setup network $NMC::AGGREGATION'
See also: 'show network $NMC::AGGREGATION'
See also: 'show network $NMC::AGGREGATION policy'
See also: 'show network $NMC::AGGREGATION <aggr-name> policy'

See also: 'setup usage'

See also: 'help'

]]></template>
<template module='Setup' name='setup_aggregation_destroy_usage'><![CDATA[
$cmdline
Usage: [-y] [aggregated interface]

  -y	Skip confirmation dialog by automatically responding Yes

Destroy the specified interface aggregation.

See also: 'create network aggregation'
See also: 'show network aggregation'

]]></template>
<template module='Setup' name='setup_nameservers_usage'><![CDATA[
$cmdline
Usage: [-1 dns1] [-2 dns2] [-3 dns3]

  -1 <dns1>    first name server's IPv4 address
  -2 <dns2>    second name server's IPv4 address
  -3 <dns3>    third name server's IPv4 address

See also: 'setup network interface'
See also: 'setup network gateway'
See also: 'setup network routes'

See also: 'setup appliance init'
See also: 'setup appliance configuration'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_routes_usage'><![CDATA[
$cmdline
Usage: <destination> [[modifier] <gateway>]

  <destination>    destination host or network IP address or name
  <gateway>        gateway IP address or name

Permanently modify routing table.
For more information on [modifier] see route(1M).

See also: 'setup network interface'
See also: 'setup network gateway'

See also: 'setup appliance init'
See also: 'setup appliance configuration'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_unix_user_usage'><![CDATA[
$cmdline
Usage: [username]

$str

Note that the appliance's management framework provides (only) read-only
access to LDAP and AD managed user database. It is possible to locally
override existing LDAP/AD user properties by defining a local user, via
'create appliance user'.

Use 'show folder <name> acl' and 'setup folder <name> acl' to associate
new, or modify existing ACLs for a given user, as far as access control
is concerned.


See also: 'show appliance user'
See also: 'setup appliance user <name> property'
See also: 'create appliance user'
See also: 'destroy appliance user'

See also: 'setup appliance password'

See also: 'show folder <name> acl'
See also: 'setup folder <name> acl'

See also: 'show network netgroup'

See also: 'setup appliance $NMC::USERGROUP'

]]></template>
<template module='Setup' name='create_unix_user_usage'><![CDATA[
$cmdline
Usage: [username]

Add a new local appliance's user account.

In addition to creating a new local user, the command can be used to
override management and access control policies associated with LDAP
or AD-managed user with the same name, if exists.

Note that in a network of two or more appliances, it is always
preferable to have a centralized, LDAP or AD based, user database.
$NZA::PRODUCT provides a fully compliant LDAP and AD integration.

Still, the interfaces such as 'create appliance user' and
'setup appliance user' provide additional flexibility.
To view all users, local and LDAP/AD based, run 'show appliance user'.


See also: 'setup appliance user'
See also: 'destroy appliance user'

See also: 'show appliance user'
See also: 'show appliance $NMC::USERGROUP'

See also: 'setup appliance password'
See also: 'create appliance $NMC::USERGROUP'

See also: 'show network netgroup'

]]></template>
<template module='Setup' name='delete_unix_user_usage'><![CDATA[
$cmdline
Usage: [-f] [-y] [user]

  -y	Skip confirmation dialog by automatically responding Yes
  -d    Delete user's home folder, if exists

Delete a local user account.

Examples:

1) Delete user 'joe' but do not remove joe's home folder
   ${prompt}destroy user joe


2) Delete user 'sam' skipping confirmation; remove sam's home folder as well
   ${prompt}destroy user sam -y -d

See also: 'show appliance user'
See also: 'create appliance user'
See also: 'setup appliance user'

See also: 'show appliance $NMC::USERGROUP'

]]></template>
<template module='Setup' name='setup_unix_group_usage'><![CDATA[
$cmdline
Usage: [group name]

Modify group ID (gid) of the specified group of local users.
To add or delete locally defined group members, use 'add-members' or
'delete-members', respectively.

Examples:

1) Modify group ID of the group 'engineering':
   ${prompt}setup appliance $NMC::USERGROUP engineering

2) Add more members to the group 'engineering':
   ${prompt}setup appliance $NMC::USERGROUP engineering add-members

2) Delete some of the members from the group 'engineering':
   ${prompt}setup appliance $NMC::USERGROUP engineering delete-members


See also: 'show appliance $NMC::USERGROUP'
See also: 'show appliance user'

See also: 'create appliance $NMC::USERGROUP'
See also: 'destroy appliance $NMC::USERGROUP'

See also: 'setup appliance password'
See also: 'show network netgroup'

]]></template>
<template module='Setup' name='create_unix_group_usage'><![CDATA[
$cmdline
Usage: [group name]

Create a new (local) group of local users.

A given group, whether it is LDAP/AD managed or locally defined,
can be associated with a number of management and access control
policies. Use 'show folder <name> acl' and 'setup folder <name> acl'
to associate new, or modify existing ACLs for any given group
(both LDAP managed and local), as far as access control is concerned.

Note that in a network of two or more appliances, it is always
preferable to have a centralized, LDAP or AD based, user and
group database. $NZA::PRODUCT provides a fully compliant LDAP and
Active Directory (AD) integration.

To view all locally defined groups, run 'show appliance $NMC::USERGROUP'.


See also: 'setup appliance $NMC::USERGROUP'
See also: 'destroy appliance $NMC::USERGROUP'

See also: 'setup appliance $NMC::USERGROUP <name> add-members'
See also: 'setup appliance $NMC::USERGROUP <name> delete-members'

See also: 'show appliance $NMC::USERGROUP'
See also: 'show appliance user'

See also: 'show folder <name> acl'
See also: 'setup folder <name> acl'

See also: 'create appliance user'

See also: 'show network netgroup'

]]></template>
<template module='Setup' name='delete_unix_group_usage'><![CDATA[
$cmdline
Usage: [-y] [group]

  -y	Skip confirmation dialog by automatically responding Yes

Delete a local group. This command deletes only the specified group,
not the users - members of this group.

Examples:

1) Delete a group 'engineering':
   ${prompt}destroy group engineering


2) Delete group 'test' skipping confirmation
   ${prompt}destroy group test -y


See also: 'show appliance $NMC::USERGROUP'
See also: 'create appliance $NMC::USERGROUP'
See also: 'setup appliance $NMC::USERGROUP'

See also: 'setup appliance $NMC::USERGROUP <name> add-members'
See also: 'setup appliance $NMC::USERGROUP <name> delete-members'

See also: 'show appliance user'

]]></template>
<template module='Setup' name='_add_delete_members_usage'><![CDATA[
$cmdline
$str '$group'.

See also: 'show appliance $NMC::USERGROUP'
See also: 'create appliance $NMC::USERGROUP'
See also: 'setup appliance $NMC::USERGROUP'

See also: 'setup appliance $NMC::USERGROUP $group add-members'
See also: 'setup appliance $NMC::USERGROUP $group delete-members'

See also: 'show appliance user'

]]></template>
<template module='Setup' name='setup_gateway_usage'><![CDATA[
$cmdline
Usage: [-g default gateway]

  -g <gateway>	Default gateway address (IPv4 only)

Setup network interface gateway. This effectively creates a default
route for the interface.

See also: 'show network routes'

See also: 'setup network interface'
See also: 'setup network nameservers'
See also: 'setup network routes'
See also: 'setup network $NMC::AGGREGATION

See also: 'setup appliance configuration'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_interface_static_usage'><![CDATA[
$cmdline
Usage:  [-i IP address] [-m mask] [-g default gateway]
	[-1 dns1] [-2 dns2] [-3 dns3]

  -i <IP>	IPv4 address
  -m <mask>	IPv4 subnet mask
  -g <gateway>	Default gateway IPv4 address
  -1 <dns1>	First name server's IPv4 address
  -2 <dns2>	Second name server's IPv4 address
  -3 <dns3>	Third name server's IPv4 address

Configure network interface statically. There are two ways to configure an IP
interface: static and dynamic, via DHCP. For the latter please refer to:
'setup network interface <interface-name> dhcp'. Note that generally, DHCP
is a preferred way of configuring a network interface. However, with multiple
interfaces you may want to avoid DHCP to not have multiple default routes
defined.

Minimally, static configuration requires IP address (-i), subnet mask (-m),
and commonly a gateway (-g). Appliance's primary IP interface can be
alternatively configured via: 'setup appliance init'


See also: 'setup network interface <interface-name> dhcp'
See also: 'setup network gateway'
See also: 'setup network routes'
See also: 'setup network $NMC::AGGREGATION
See also: 'setup network nameservers'

See also: 'setup appliance init'
See also: 'setup appliance configuration'

See also: 'setup network interface <interface-name> create-vlan'
See also: 'setup network interface <interface-name> destroy-vlan'

See also: 'show network interface'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_interface_create_vlan_usage'><![CDATA[
$cmdline
Usage:  [-v vid] [-f]

  -v <vid>	The ID associated with the VLAN (VLAN tag)
  -f 		Force the creation of the VLAN link

   Create VLAN link over the specified Ethernet link.
   The resulting virtual interface name will be automatically
   generated as:

   <name><1000 * vid + physical-link-id>

   For instance, if the physical interface name is bge1 and
   vid is equal 2, the VLAN interface name will be bge2001.


See also: 'setup network interface <interface-name> dhcp'
See also: 'setup network gateway'
See also: 'setup network routes'
See also: 'setup network $NMC::AGGREGATION
See also: 'setup network nameservers'

See also: 'setup appliance init'
See also: 'setup appliance configuration'

See also: 'show network interface'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_interface_create_ipalias_usage'><![CDATA[
$cmdline
Usage:  [-i id]

  -i <id>	The ID associated with the IP alias link name

   Create IP alias link over the specified Ethernet link.
   The resulting virtual interface name will be automatically
   generated as:

   <name>:<id>

   For instance, if the physical interface name is bge1 and
   id is equal 2, the IP alias link name will be bge2:1.


See also: 'setup network interface <interface-name> dhcp'
See also: 'setup network gateway'
See also: 'setup network routes'
See also: 'setup network $NMC::AGGREGATION
See also: 'setup network nameservers'

See also: 'setup appliance init'
See also: 'setup appliance configuration'

See also: 'show network interface'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='share_nfs_usage'><![CDATA[
$cmdline
Usage:  [-r] [-g] [-a] [-W ro_opts] [-w rw_opts] [-s root_opts]
        [-o extra_opts]

  Make local folders available for mounting by remote NFS clients.

  -r			Recursive; share nested folders.
  -g			Show netgroups. Not to be combined with other options.

  -w <rw_opts>	       	Folder is shared read/write only to the listed
     			clients. No other systems can access folder
			except those defined.

  -W <ro_opts>	       	Folder is shared read-only only to the listed
     			clients. No other systems can access folder
			except those defined.

  -s <root_opts> 	Only root users from the hosts specified in
     			access_list have root access. By default, no host
			has root access. Netgroups can be used. This option
			is a modifier and must be combined with rw or ro
			host definitions.

  -o <extra_opts>	Comma-separated list of keywords. If omitted,
			by default sharing is read-write to all clients.
			The following options are available:

  -a			Allow anonymous access to this share. Shared
  			top-level directory will be granted with read-write
			access for anonymous user 'nfs'.

The rw_opts, ro_opts, and root_opts contain zero or more colon-delimited
netgroups and/or hosts, for instance: netgroup-engineering:10.16.16.92 (for
complete list of netgroups use the -g option or run 'show ldap netgroups -h').

Note that <extra_opts> are not validated; the -o options are passed over
to the system as is. For complete list of options see manual pages for
share_nfs(1M). In particular, network ranges can be given in client lists
when prefaced by @, as in \@192.168 or \@192.168.0.0, and further
by \@192.168.0/22.


Note:
-----
  $NZA::PRODUCT uses NFSv4 by default. To force the server to use NFSv3,
  set NFS_SERVER_MAX variable to 3 in the server configuration file.
  Edit the server configuration via
  'setup network service nfs-server edit-settings' command.

  This is particularly useful if _only_ NFSv3 clients are used
  with the $NZA::PRODUCT NFS server.
  If you have a mix of v3 and v4 clients, to force NFSv4 clients
  to use v3, mount using the following syntax:
  server:/share /localmount bg,intr,vers=3,noacl


All shares for NFS as relative to $NZA::VOLROOT, or whatever the
'volroot' property is set to (to see the current settings, run
'show appliance nms config', and/or run 'help options').
Run 'show volume <name> share nfs' to show the actual
mountpoint of a given shared folder.

 ************************************************************************
 *                                                                      *
 *  Unless you are an experienced NFS user or system administrator,     *
 *  here are a few of guidelines that you may find useful.              *
 *                                                                      *
 *  1) Make sure that domain name on the client and the server are the  *
 *     same. Different domain names is one of common reason to getting  *
 *     'Permission denied' when accessing a shared location             *
 *     from NFS client. Run 'show appliance domainname' to find out.    *
 *                                                                      *
 *  2) If the client and server domain names are not matching           *
 *     (and cannot be changed to match),                                *
 *     try using 'anon=0' setting via Extra-Options.                    *
 *                                                                      *
 *  3) Do not leave the 'Root' field empty. The safest option is to     *
 *     explicitly set Root to client's hostname. Note: not the client's *
 *     IP address but the hostname in the form <host.domain> that       *
 *     can be resolved.                                                 *
 *     Use colon (':') delimiter to specify more that one client host   *
 *     iand/or netgroup.                                                *
 *                                                                      *
 ************************************************************************

Examples:

1) The following shares vol1/a/b:

${prompt}setup volume vol1 folder a/b share nfs
Read-Write    : netgroup-engineering:10.16.16.92
Read-Only     : netgroup-marketing
Root          : admin
Extra-Options :

2) Same as above, using 'share' top-level command:
${prompt}share folder vol1/a/b nfs

Notice that nested folder vol1/a/b/c has inherited its share definition
from its parent vol1/a/b. In general, inheritance of folder properties
down the hierachy greatly reduces amount of operations required to manage
appliance when its underlying storage scales up.

3) Use 'show share' command to display ALL shares, including nfs,
   cifs, ftp, and rsync. For instance:

     ${prompt}show share
     FOLDER                   CIFS  NFS   RSYNC FTP WEBDAV
     vol1                     -     -     Yes   -   -
     vol1/a                   -     -     -     Yes -
     vol1/b                   -     Yes   -     -   -
     vol1/c                   -     -     Yes   -   Yes

4) Mounting $NZA::PRODUCT shares from Mac OS X version 10.5 ("Leopard"):

     Assuming nexentastor.mydomain.com is the appliance's hostname:

     nexentastor:/\$ share tier2/test nfs
	Read-Write           : machost.mydomain.com
	Read-Only            :
	Root                 : machost.mydomain.com
	Extra Options        : root
	Anonymous Read-Write : false
	Recursive            : false

	Modifed NFS share for folder 'tier2/test'

     On machost, the 10.5 client:

     machost:~ \$ sudo vi /etc/auto_home
	(adding the following line to /etc/home)
	test	nexentastor.mydomain.com:/volumes/tier2/test

	(note that the above left is makes it /home/test and
	 the above right is the source volume.)

     machost:~ \$ sudo automount -vc
	automount: /net updated
	automount: /home updated
	automount: no unmounts


See also: 'help share_nfs'
See also: 'show <folder> share nfs'
See also: 'setup <folder> unshare nfs'

See also: 'setup network service nfs-server edit-settings'
See also: 'setup network service nfs-server configure'

See also: 'setup folder <name> property inherit'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='setup_iscsi_initiator_session_remove_usage'><![CDATA[
$cmdline
Usage:

Removes specified session. For more information, see iscsiadm(1M).

See also: 'destroy zvol'
See also: 'show zvol'

]]></template>
<template module='Setup' name='setup_iscsi_initiator_session_param_usage'><![CDATA[
$cmdline
Usage: [value]

Set initiator's iSCSI session parameter '$name' to
specified <value>. See iscsiadm(1M) for details.

See also: 'create zvol'
See also: 'setup zvol'
See also: 'show zvol'

]]></template>
<template module='Setup' name='_setup_iscsi_tpgt_usage_create'><![CDATA[
$cmdline
Usage: [tpgt] [ipaddr1 ipaddr2 ...]

  tpgt                  New target portal group,
		       	number in a range 1..65535

  ipaddr1 ipaddr2 ...   IP address(es) to be added to the
  			target portal group

Create a new target portal group.

Target Portal Group is a standard mechanism to control access to the
appliance's iSCSI target. Each Target Portal Group combines one or
more of the appliance's local IP addresses. For instance, if the
appliance is a multihomed host accessible via IP addresses
(IP_1, IP_2, IP_3), you could define one target portal group that
contains (IP_1, IP_2) and another that contains only IP_3.
Obviously, this would be just an example, and there are many other valid
permutations of portal groups based on a 3 given IP addresses.

To prevent iSCSI Target discovery via a certain existing local IP
address(es), simply do not publish those IP addresses via
Target Portal Groups. This will effectively restrict iSCSI traffic
to those and only those local IP interfaces that are explicitly
specified via configured Target Portal Groups.

See also: 'show iscsi target tpgt'

See also: 'setup zvol <name> tpgt'
See also: 'setup zvol'


]]></template>
<template module='Setup' name='_setup_iscsi_tpgt_usage_destroy'><![CDATA[
$cmdline
Usage: [tpgt]

  tpgt    Existing Target Portal Group Tag (TPGT)

Delete existing target portal group.

The operation can be used to prevent iSCSI Target discovery
via a certain existing local IP address(es).
Deleting a certain Target Portal Group will effectively restrict
iSCSI traffic to those and only those local IP interfaces that
are explicitly specified via remaining configured
Target Portal Groups.

See also: 'show iscsi target tpgt'

See also: 'setup zvol <name> tpgt'
See also: 'setup zvol'


]]></template>
<template module='Setup' name='_setup_iscsi_tpgt_usage_modify'><![CDATA[
$cmdline
Usage: [tpgt] [ipaddr1 ipaddr2 ...]

  tpgt                  Existing Target Portal Group Tag (TPGT)

  ipaddr1 ipaddr2 ...   IP address(es) to substitute the ones currently
  			specified in the corresponding target portal
			group

Modify existing target portal group.

See also: 'show iscsi target tpgt'

See also: 'setup zvol <name> tpgt'
See also: 'setup zvol'


]]></template>
<template module='Setup' name='setup_iscsi_initiator_usage'><![CDATA[
$cmdline
Usage: [value]

Set iSCSI Initiator parameter '$name' to
specified <value>. See iscsiadm(1M) for details.

See also: 'create zvol'
See also: 'setup iscsi'

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage'><![CDATA[
$cmdline
Usage: [<IP address>:[port]]

Add an iSNS server to the list of iSNS server addresses to be used
for iSCSI target discovery via iSNS.

See iscsiadm(1M) for details.

If a port is not specified, the default of 3205 is used.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage1'><![CDATA[
$cmdline
Usage: [<IP address>:[port]]

Add a target to a list of SendTargets discovery addresses.

If not specified, the default port 3260 will be used.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage2'><![CDATA[
$cmdline
Usage: [<target-name>,<target address>[:port-number][,tpgt]]

Add a target to the list of statically configured iSCSI target addresses.

The <target-name> can be up to 223 characters.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage3'><![CDATA[
$cmdline
Usage:

Remove iSNS discovery record. Specifically,
remove iSNS server address from the list of iSNS server addresses.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage4'><![CDATA[
$cmdline
Usage:

Remove SendTargets discovery record. Specifically,
remove the specified target from the list of SendTargets discovery
addresses.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage5'><![CDATA[
$cmdline
Usage:

Remove static discovery record. Specifically,
remove the specified target from the list of statically discovered targets.
See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage6'><![CDATA[
$cmdline
Usage:

Enable discovery of iSNS Server discovery. Enabling discovery will
establish new iSCSI session(s), if successful.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage7'><![CDATA[
$cmdline
Usage:

Enable SendTargets discovery. Enabling discovery will establish new
iSCSI session(s), if successful.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage8'><![CDATA[
$cmdline
Usage:

Enable static discovery. Enabling discovery will establish new
iSCSI session(s), if successful.

See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage9'><![CDATA[
$cmdline
Usage:

Disable discovery of iSNS Server discovery.
See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage10'><![CDATA[
$cmdline
Usage:

Disable SendTargets discovery. See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_iscsi_discovery_usage11'><![CDATA[
$cmdline
Usage:

Disable static discovery. See iscsiadm(1M) for details.

]]></template>
<template module='Setup' name='setup_configuration_set_location_usage'><![CDATA[
$cmdline
Usage: [location]

    <location>     - The location to store, and to recover from,
	    	     appliance's configuration

The location is any valid combination of [[volume[/folder/]]directory].
If the <directory> part of the location does not exist, it will be
created.

Examples:
1) ${prompt}setup configuration set-location vol1/config

2) ${prompt}setup configuration set-location /saved_config
   In this example, if the directory /saved_config does not exist,
   it will be created.

The location for saved configurations that is currently in use:
$NZA::SAVED_CONFIGROOT

See also: 'setup appliance configuration save'
See also: 'setup appliance configuration restore'

]]></template>
<template module='Setup' name='setup_configuration_save_usage'><![CDATA[
$cmdline
Usage: [-d description] [-y]

  -d <description>  An arbitrary description. If specified, description
	  	    is stored with the appliance's configuration,
		    and can be listed via 'show appliance saved-configs'

  -y	            Skip confirmation dialogs. In particular, do not
  		    ask for description (see previos) if not specified
		    but proceed to save configuration.

Save selected items of the appliance's configuration. The saved
configuration is essentially a tarball that can be stored at a
pre-defined and configurable location.

The current location in use: $NZA::SAVED_CONFIGROOT

To change the location, run 'setup appliance configuration set-location'.
More importantly, make sure to periodically replicate the saved
configuration - see 'setup $NZA::AUTO_TIER' and 'setup $NZA::AUTO_SYNC_PLUGIN' (*)

At any point in time you may have several configuration tarballs, saved
in different locations.

Overall, the operation supports the following items of configuration:
@bul_list


   PLEASE NOTE
   ===========
   This facility does not allow to take a snapshot of the entire
   appliance's configuration, and/or restore entire appliance's
   configuration from a specified remote location, and/or
   recover appliance's root filesystem in case of a hardware
   failure.

   For related information, please see:

     * 'setup appliance checkpoint'
     * 'help data-replication'

   or search F.A.Q. on the website for "checkpoint"


Examples:

1) Store appliance's configuration locally:
   ${prompt}setup appliance configuration save -d saved-before-upgrade

   Alternatively, run 'setup appliance configuration save' and
   enter description at the prompt.

2) Store appliance's configuration remotely:
   ${prompt}switch appliance host1.xyz-corp.com -f
   nmc${str}host1.xyz-corp.com:/\$ setup appliance configuration save

3) Perform the 'save configuration' operation on a group of appliances:
   ${prompt}switch group my_group
   nmc${s}setup appliance configuration save -y


See also: 'setup appliance configuration restore'
See also: 'setup $NMC::APPL_GROUP'

See also: 'show appliance saved-configs'

See also: 'setup appliance checkpoint restore'

See also: 'help data-replication'

See also: 'setup usage'

   Note:
        (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Setup' name='setup_configuration_restore_usage'><![CDATA[
$cmdline
Usage: [-o] [-y] [-f]

  -o       Overwrite newer configuration.
  -y	   Skip confirmation dialogs by automatically responding Yes.
  -f	   Force appliance reboot even if failed to gain exclusive
           access

Restore certain items of the appliance's configuration.

The saved configuration is essentially a tarball that is
stored at a pre-defined and configurable location.

The current location in use: $NZA::SAVED_CONFIGROOT

Note that the most recently stored configuration will be used.
There is currently no support for showing all stored configurations
and selecting one of them which may be not the most recent.

To display all saved configurations, run 'show appliance saved-configs'.

Overall, the operation supports the following items of configuration:
@bul_list


   PLEASE NOTE
   ===========
   This facility does not allow to take a snapshot of the entire
   appliance's configuration, and/or restore entire appliance's
   configuration from a specified remote location, and/or
   recover appliance's root filesystem in case of a hardware
   failure.

   For related information, please see:

     * 'setup appliance checkpoint'
     * 'help data-replication'

   or search F.A.Q. on the website for "checkpoint"


Examples:

1) Restore $NZA::AUTO_TIER storage services on a designated remote appliance:
   ${prompt}fastswitch myhost
   nmc${str}myhost:/\$ setup appliance configuration restore $NZA::AUTO_TIER

2) Restore $NZA::AUTO_SNAP services locally:
   ${prompt}setup appliance configuration restore $NZA::AUTO_SNAP


See also: 'setup appliance configuration set-location'.
See also: 'setup appliance configuration save'

See also: 'setup appliance checkpoint restore'

See also: 'help data-replication'

See also: 'setup usage'

]]></template>
<template module='Setup' name='setup_recording_destroy_usage'><![CDATA[
$cmdline
Usage: [-y] [recording name]

  -y	Skip confirmation dialog by automatically responding Yes

Destroy the specified recorded NMC session.

See also: 'record'
See also: 'run recording'
See also: 'show recording'

]]></template>
<template module='Setup' name='run_general_diagnostics_usage'><![CDATA[
$cmdline
Usage: [-y] [-d outdir]

   -y		Skip confirmation dialog and do not show generated
                diagnostics.
   -d <outdir>	Generate file in the <outdir> directory

Generate appliance's diagnostics as a text file. Optionally show it.

See also: 'show all'
See also: 'dtrace report-all'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Setup' name='snapshot_destroy_by_mask_usage'><![CDATA[
$cmdline
Usage: destroy_snapshots [-y] [-r] -n <expr>

   -r           Process volumes or folders recusively
   -y           Skip confirmation dialog and do not show generated
                diagnostics. USE WITH CAUTION.
   -n <expr>    Mask for name of snapshots
                This parameter is obligatory.

Delete snapshots of specified volume/folder by name mask.

Example:
	nmc\$ setup folder backup/a destroy_snapshots -r -n sync-minute*

	- will destroy all snapshots of 'backup/a' folder and nested
	folders, which name begins with 'sync-minute'

See also: 'help'

]]></template>
<template module='Show' name='dtrace_redirect'><![CDATA[
*****************************************************************
*								*
*               System DTrace based toolkit                     *
*                                                               *
*****************************************************************

The toolkit is available via 'dtrace' top-level command.

Type 'show performance dtrace' or simply, 'dtrace' and press TAB-TAB
or Enter.

DTrace can be used to generate performance profiles and analyze
performance bottlenecks.

DTrace can help to troubleshoot problems, by providing detailed views
of the system internals.

Use TAB-TAB to navigate, or press Enter and make a selection.

]]></template>
<template module='Show' name='show_nms_property_usage'><![CDATA[
$cmdline
Usage: [prop-name]

Show system-wide $NZA::COMPANY Management Server properties,
their values and descriptions.

Examples:

1) Show all properties

${prompt}show appliance nms property

2) Same as above using 'less' viewer to scroll

${prompt}show appliance nms property | less

3) Display a single property 'saved_configroot'

${prompt}show appliance nms property saved_configroot

As always, use TAB-TAB to list/complete/select available properties.

See also: 'setup appliance nms property'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_nms_locks_usage'><![CDATA[
$cmdline
Usage: [-v]

   -v      Verbose; produce detailed output

Show outstanding locks.

$NZA::PRODUCT Storage Appliance API (SA-API) provides access to the
appliance's management objects and services. The appliance's management
object hierarchy includes: lun, volume, folder, snapshot, network,
network interface, network service, storage service, indexer, zvol,
statistic collector, fault trigger, reporter, and others.

All existing client management applications without exception (and that
list certainly includes $NZA::COMPANY Management Console - NMC) use SA-API to
monitor and administer the appliance. To ensure consistent view of the
appliance from all clients and data integrity at all times, the SA-API
(or more exactly, $NZA::COMPANY Management Server that provides the API)
serializes certain management operations. The corresponding locks can be
viewed and examined via this 'show appliance nms locks' command.

Examples:

1) ${prompt}show appliance nms locks

   An example of output follows below:

LOCK                   TIMESTAMP            OWNER
cpu-utilization-check  Feb 21 13:15:20 2009 cpu-utilization-check (2672, :1.80)

2) ${prompt}show appliance nms locks -v

   An example of verbose output follows below:

PROPERTY          VALUE
LOCK NAME       : cpu-utilization-check
OWNER           : cpu-utilization-check (PID 2672, API caller ID :1.80),
                : started Feb 21 13:15:20 2009
LOCK TYPE       : write
LOCK TIMESTAMP  : Feb 21 13:15:20 2009

LOCK NAME       : memory-check
OWNER           : memory-check (PID 2683, API caller ID :1.81),
                : started Feb 21 13:15:25 2009
LOCK TYPE       : write
LOCK TIMESTAMP  : Feb 21 13:15:25 2009


See also: 'show appliance nms'
See also: 'show appliance runners'
See also: 'show appliance auto-service'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_srvpool_usage'><![CDATA[
$cmdline

Show pool of $NZA::COMPANY Management Servers, their properties
and usage statistics.

Example:

ID   STATE     PORT    PID     cnt     min (ms)  max (ms)  %idle
0    active    2001    618     7574    0.15      1474.24   87
1    idle      2011    886     20578   3.58      12.70     58
2    running   2012    987     23415   2.33      769.79    49

The fields in the table (example above) are:

    ID		- pool ID of the management server. See a note
 		  on ID = 0 below
    STATE	- running | idle | defunct
    PORT	- DBus port the server listens on
    PID		- PID of the server process

    cnt		- number of API calls the server executed
    min		- minimum time (in milliseconds) it took
		  to execute one API call
    max		- maximum time (in milliseconds) it took
		  to execute one API call
    idle	- percentage of time this server was idle,
		  ie, not executing any client requests


A note on POOL ID = 0
=====================
The first row in the table (example above) shows the main management
server that, strictly speaking, does not belong to the pool. This
server is in fact used to manage the pool itself, and some other aspects
of the appliance management that cannot be equally distributed between
the "pooled" servers. The "zero" server is always present, independently
of whether the pool is used or not.

Background
==========
The 3.x generation of the appliance software includes a "server pool"
facility, whereby multiple management servers may be added to the pool
dynamically, on as-needed basis.

The Management Server (NMS) becomes just another resource
that can be added to a pool (of servers) or removed from the pool.

This provides for better response time, concurrent execution of
simultaneous client requests from multiple users, and,
last but not the least - server redundancy and
overall reliability of management adnimistrative access to the appliance.

Server Properties
=================
Additional ("pooled") management servers are run and NMS startup,
based on the following NMS properties:

 * srvpool_cnt_initial
 * srvpool_cnt_max

All NMS properties, including the ones listed above, are managed via:

 ${prompt}setup appliance nms property

To switch server pooling off, set:

  srvpool_cnt_max = 0

Setting the maximum number of servers in a pool to zero effectively
cancels server "pooling".

Another server pool related settings include:

 * srvpool_port_range_min
 * srvpool_affinity_timeout
 * srvpool_interface_serialization_timeout

Pool Management
===============
You can manage the pool via those "srvpool_..." properties and the following
NMC interface:

  ${prompt}setup appliance nms pool


See also: 'setup appliance nms pool'
See also: 'setup appliance nms property'
See also: 'show appliance nms property'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_appliance_netmasks_usage'><![CDATA[

See also: 'setup appliance netmasks'

]]></template>
<template module='Show' name='show_appliance_hosts_usage'><![CDATA[

See also: 'setup appliance hosts'

]]></template>
<template module='Show' name='show_saved_configurations_usage'><![CDATA[
$cmdline
Usage:

List saved configurations. Appliance's configuration is saved via
'setup appliance configuration', and is stored at a pre-defined
location. The current location in use: $NZA::SAVED_CONFIGROOT

See also: 'setup appliance configuration'
See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_zvol_usage'><![CDATA[
$cmdline
Usage: [-v] [-V]

  -v	Verbose output: show all $NMC::ZFS_LUN properties and
        associated iSCSI targets, if any

  -V	Same as above, with detailed output on a per iSCSI target basis.

If neither -v nor -V option used, the command output is a simple table,
with the following columns:

	LUN ID		- zvol name
	Type		- always 'zvol'
	Size		- maximum size
	Reservation	- allocated space that is guaranteed to be available;
			  'none' would mean that the device is thinly
		          provisioned to grow up to the Size (see previous)
	Volume		- the volume zvol is based on
	Share		- 'iscsi' if the block device is shared via iSCSI;
			  'none' otherwise
	Connections	- number of active iSCSI connections
	Swap		- 'yes' if the device is used as swap;
			  'no' otherwise

For instance:

${cb}LUN ID      Type  Size      Reservation  Volume   Share  Connections Swap$cn
zvol1       zvol  100MB     100MB         vol1    none        0      yes
zvol2       zvol  2GB       420MB         vol2    iscsi       7      no

In the example above, zvol1 is used as additional swap area, to effectively
increase the total size of system virtual memory. Zvol2 is used as iSCSI
target, with 7 active iSCSI connections open to it at the moment. Note
also that zvol2 is thinly provisioned: its maximum size is 2GB but only
420MB is actually used at the time of this report.


See also: 'create zvol'
See also: 'destroy zvol'
See also: 'setup zvol'

See also: 'show appliance swap'

See also: 'setup iscsi'
See also: 'setup network service iscsi-target'
See also: 'destroy zvol'

See also: 'show usage'

]]></template>
<template module='Show' name='show_all_snapshot_enter_usage'><![CDATA[
$cmdline
Usage: [-r] [-v]

  -r 	         Recurse into sub-folders, show nested snapshots as well
  -v	         Verbose output, show snapshot properties

  -s <property>  A property to use for sorting the output by column
                 in ascending order based on the value of the property.
		 Use -v (verbose) option to list all available properties,
		 or see zfs(1m) man page ('help zfs').

  -S <property>  Same as the -s option, but sorts by property in descending
                 order.


Examples:

1) Show all snapshots in the system, names and the most basic properties:
   ${prompt}show snapshot
   or, same:
   ${prompt}show snapshot -r
   or, same:
   ${prompt}df snapshot -r

2) Same as above, remotely. That is, list snapshots on a
   remote appliance identified by a given hostname
   (host1.xyz-corp.com in the example)

   ${prompt}switch appliance host1.xyz-corp.com -f
   ${prompt}show snapshot

3) Show snapshots of vol1/a folder, include details:
   ${prompt}show snapshot vol1/a -v
   or, same:
   ${prompt}show vol1/a snapshot -v

4) Recursively show snapshots of vol1/a and nested sub-folders,
   include details:
   ${prompt}show snapshot vol1/a -v -r
   or, same:
   ${prompt}show vol1/a snapshot -v -r

5) Recursively show snapshots of vol1/a and nested sub-folders
   in a tabular (non-verbose) form, in the order of creation:
   ${prompt}show snapshot vol1/a -s creation

6) Same as above, in the descending order:
   ${prompt}show snapshot vol1/a -s creation


See also: 'show folder'
See also: 'query'

See also: 'switch appliance'
See also: 'switch group'

See also: 'show usage'

See also: 'create $NZA::AUTO_SNAP'
See also: 'setup snapshot <name> clone'

See also: 'help keyword clone'
See also: 'help keyword snapshot'

See also: 'help data-replication'

See also: 'setup appliance checkpoint <name> snapshot'
See also: 'show appliance checkpoint <name> snapshot'

]]></template>
<template module='Show' name='show_folder_acl_usage'><![CDATA[
$cmdline
Usage: [-r]

  -r	Show ownership and Access Control Lists (ACLs) for the
        specified folder and its nested sub-folders

]]></template>
<template module='Show' name='show_folder_acl_usage1'><![CDATA[

Examples:

1) Show ownership and Access Control List associated with a
   folder 'vol1/a':

${prompt}show folder vol1/a acl

2) Show ACLs of a given folder and its sub-folders:

${prompt}show vol1/a acl -r

Notice that the 'folder' keyword can be omitted.
An example output follows below:
${prompt}show vol1/a  acl -r
================================== FOLDER: vol1/a ============================
ENTITY      ALLOW                               DENY
owner@      list_directory, read_data,
            add_file, write_data,
            add_subdirectory, append_data,
            write_xattr, execute,
            write_attributes, write_acl,
            write_owner

group@      list_directory, read_data, execute  add_file, write_data,
                                                add_subdirectory, append_data

everyone@   list_directory, read_data,          add_file, write_data,
            read_xattr, execute,                add_subdirectory, append_data,
            read_attributes, read_acl,          write_xattr, write_attributes,
            synchronize                         write_acl, write_owner

user:abc    add_file, write_data, write_xattr

================================= FOLDER: vol1/a/b ===========================
ENTITY      ALLOW                               DENY
owner@      list_directory, read_data,
            add_file, write_data,
            add_subdirectory, append_data,
            write_owner

group@      list_directory, read_data, execute  add_file, write_data,
                                                add_subdirectory, append_data

everyone@   list_directory, read_data,          add_file, write_data,
            read_xattr, execute,                add_subdirectory, append_data,
            read_attributes, read_acl,          write_xattr, write_attributes,


See also: 'setup folder <name> acl'
See also: 'setup folder <name> ownership'
See also: 'setup folder <name> acl reset'

See also: 'show appliance user'
See also: 'setup appliance user'

See also: 'show appliance $NMC::USERGROUP'
See also: 'setup appliance $NMC::USERGROUP'
See also: 'create appliance $NMC::USERGROUP'
See also: 'destroy appliance $NMC::USERGROUP'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_aclinfo'><![CDATA[
List of ACL permissions follows below:

  * read_data:	      read the contents of a file
  * write_data:       modify an existing file
  * list_directory:   list the contents of a directory
  * add_file:         add a new file to a directory
  * append_data:      append to an existing file
  * add_subdirectory: create subdirectories
  * read_xattr:       read extended attributes
  * write_xattr:      write extended attributes
  * execute:          execute a file
  * delete_child:     delete a file within a directory
  * read_attributes:  read basic attributes (non-ACL) of a file
                      (ie: ctime, mtime, atime, etc)
  * write_attributes: write basic attributes to a file or directory
                      (atime, mtime)
  * delete:           delete a file
  * read_acl:         read the ACL
  * write_acl:        modify the ACL
  * write_owner:      change ownership of a file
  * synchronize:      access file locally via synchronous reads and writes

]]></template>
<template module='Show' name='show_volume_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v	Verbose output - show all volumes and their properties

List all volumes in the system.


See also: 'show folder'
See also: 'show volume <name>'
See also: 'show snapshot'

See also: 'switch appliance'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_fs_df_usage'><![CDATA[
$cmdline
Usage: [-v] [-r]

  -v	Verbose output - show volume/folder properties
  -r	Recursively list nested folders
        (the default option unless verbose specified)

Examples:

1) Show vol2/x folder and its sub-folders:

${prompt}show folder vol2/x
or, same:
${prompt}show vol2/x
or, same:
${prompt}df vol2/x

Here's a sample output:
NAME       USED  AVAIL  REFER  MOUNTPOINT
vol2/x    43.5K  1.95G    19K  /volumes/vol2/x
vol2/x/y  24.5K  1.95G  24.5K  /volumes/vol2/x/y
...

2) Show vol2/x only, include folder's properties:

${prompt}show folder vol2/x -v

3) Show vol2/x and its sub-folders in detail:

${prompt}show vol2/x/ -v -r

Notice that the 'folder' keyword can be omitted.

To search folders based on (any combination of their) property
values, use NMC 'query' command. See 'query -h' for details.

See also: 'setup folder'
See also: 'create folder'

See also: 'switch appliance'

See also: 'show snapshot'
See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_volume_version_usage'><![CDATA[
$cmdline
$str

]]></template>
<template module='Show' name='show_volume_version_usage1'><![CDATA[

See also: 'setup volume $s version-upgrade'
See also: 'setup folder <folder-name> version-upgrade'

See also: 'show volume $s version'
See also: 'show appliance syspool'

]]></template>
<template module='Show' name='show_folder_version_usage'><![CDATA[
$cmdline
$str

]]></template>
<template module='Show' name='show_folder_version_usage1'><![CDATA[

See also: 'setup folder $s version-upgrade'

See also: 'show volume <vol-name> version'
See also: 'show volume version'

]]></template>
<template module='Show' name='_show_fs_whospace_usage'><![CDATA[
$cmdline
Usage: [-v] [-U ${who}name] [-T type]

  -v		Verbose output

  -U		$_who name filter pattern

  -T		$_who type $quota_types

  -S		Sort by size

List ZFS $who quotas & space accounting.


See also: 'show folder'
See also: 'show volume'
See also: 'show snapshot'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_volume_properties_usage'><![CDATA[
$cmdline
Show volume properties.

Quoting from zpool(1m) man page:

  Each pool has several properties associated with it.
  Some properties are read-only statistics while others are
  configurable and change the behavior of the pool.
  The following are read-only properties:

  available    Amount of storage available within the pool.

  capacity     Percentage of pool space used.

  health       The current health of the pool.
               Health can be "ONLINE", "DEGRADED", "FAULTED",
	       "OFFLINE", "REMOVED", or "UNAVAIL".

  guid         A unique identifier for the pool.

  size         Total size of the storage pool.

  used         Amount of storage space used within the pool.

The following are modifiable properties:

  autoreplace=on | off

  This property controls automatic device replacement.
  The default value is "on".
  If set to "off", device replacement must be done by using
  'setup volume <name> replace-lun' command.

  To search appliance's objects based on (any combination of)
  property values, use NMC 'query' command.
  See 'query -h' for details.

Examples:

1) To list modifiable properties for a volume '$zname', type:

${prompt}setup volume $zname property  <TAB><TAB>

3) To show all volume's properties and their values:

${prompt}show volume $zname -v


See also: 'setup $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'setup $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'setup $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'show $NMC::FOLDER <name> $NMC::PROPERTY'
See also: 'show $NMC::VOLUME <name> $NMC::PROPERTY'
See also: 'show $NMC::ZFS_LUN <name> $NMC::PROPERTY'

See also: 'create $NMC::VOLUME'

See also: 'help zpool'
See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Show' name='show_volume_iostat_usage'><![CDATA[
$cmdline
Usage: [-c count] [-i interval]

 -i <interval>  Print I/O statistics every <interval> seconds.
                Default: 1 second.

 -c <count>     Number of iostat <interval>-long iterations.
 	        Default: 10 iterations.

Example: report 3 consecutive iterations of IO stats:
${prompt}show volume vol1 iostat -c 5

A sample output:
               capacity     operations    bandwidth
name         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
vol1        1.56T  1.16T     24     33  2.14M   607K
vol1        1.56T  1.16T      2    343   383K  42.8M
vol1        1.56T  1.16T      0    398   127K  49.6M
vol1        1.56T  1.16T     12    323  1.34M  38.0M
vol1        1.56T  1.16T     95    154  11.6M  2.59M

The output includes the following information.
* read operations per secod
* writes operations per second
* kilobytes read per second (read bandwidth)
* kilobytes written per second (write bandwidth)


See also: 'dtrace'

See also: 'show lun iostat'

See also: 'show performance'

See also: 'run benchmark bonnie-benchmark'
See also: 'run benchmark iperf-benchmark'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

See also: 'show usage'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Show' name='show_zfs_arc_stat_real_usage'><![CDATA[

Show realtime and summary ZFS Adaptive Replacement Cache (ARC)
statistics.

Adaptive Replacement Cache was originally designed at IBM:

* http://www.almaden.ibm.com/StorageSystems/projects/arc

Implemented and improved for ZFS, ARC has proved to be efficient
and effective across variety of all possible I/O workloads.

Quoting the IBM paper (the link above):

]]></template>
<template module='Show' name='show_zfs_arc_stat_real_usage1'><![CDATA[

Usage:    [-x extended stats] [-f fields] [-o outfile]
          [-s separator]
          [interval] [count]

]]></template>
<template module='Show' name='show_zfs_arc_stat_real_usage2'><![CDATA[
Examples:

1) Show ARC runtime statistics, use default paramater settings:
   ${prompt}show performance arc

2) Same as above, but run 10 second long iterations and iterate 3 times:
   ${prompt}show performance arc 10 3

3) Execute five 2-second iterations, redirect output into a.log
   ${prompt}show performance arc -o a.log  2 5

   To view resulting log:

   ${prompt}less a.log
   The output may look like:

   Time       read   miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  arcsz     c
   14:16:53   51K    3K    7      2K    5    1K   46    1K   12    124M   198M
   ...

4) Generate report that includes timestamp, cache hits, percentages of demand
and metadata hits; display output on the screen:
   ${prompt}show performance arc -f Time,Hit%,dh%,ph%,mh%


See also: 'dtrace'

See also: 'show performance ?'

See also: 'show volume iostat'
See also: 'show lun iostat'
See also: 'show volume <name> iostat'

See also: 'run benchmark bonnie-benchmark'
See also: 'run benchmark iperf-benchmark'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Show' name='show_zfs_zil_stat_real_usage'><![CDATA[

Show realtime and summary ZFS Intent Log (ZIL)
statistics.

]]></template>
<template module='Show' name='show_zfs_zil_stat_real_usage1'><![CDATA[

Usage:    [-x extended stats] [-f fields] [-o outfile]
          [-s separator]
          [interval] [count]

]]></template>
<template module='Show' name='show_zfs_zil_stat_real_usage2'><![CDATA[
Examples:

1) Show ZIL runtime statistics, use default paramater settings:
   ${prompt}show performance zil

2) Same as above, but run 10 second long iterations and iterate 3 times:
   ${prompt}show performance zil 10 3

3) Execute five 2-second iterations, redirect output into a.log
   ${prompt}show performance zil -o a.log  2 5

   To view resulting log:

   ${prompt}less a.log
   The output may look like:

   N-Bytes  N-Bytes/s N-Max-Rate    B-Bytes  B-Bytes/s B-Max-Rate    ops  <=4kB 4-32kB >=32kB
     24160        805      24160      45056       1501      45056      6      1      5      0
   ...


See also: 'dtrace'

See also: 'show performance ?'

See also: 'show volume iostat'
See also: 'show lun iostat'
See also: 'show volume <name> iostat'

See also: 'run benchmark bonnie-benchmark'
See also: 'run benchmark iperf-benchmark'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Show' name='show_netstat_usage'><![CDATA[
Usage: [-I interface name] [interval] [count]

 -I <interface>  Name of the networking interface, for instance
 		 'bge1', 'e1000g2', 'xge0'

		 To show available interfaces, run:
		 'show network interface'
]]></template>
<template module='Show' name='show_netstat_usage1'><![CDATA[

 <interval>      Report network statistics every <interval> seconds.
                 Default: 1 second.

 <count>         Number of <interval>-long iterations.
 	         Default: 10 iterations.

Show network interface statistics: iteratively report
network activity for a given appliance's networking interface.

Examples:

1) Show network interface statistics:
   ${prompt}show performance network

2) Same as above, but run 10 second long iterations and iterate 3 times:
   ${prompt}show performance network 10 3

3) Same as above, for the network interface 'bge0'
   ${prompt}show performance network -I bge0 10 3


See also: 'show network interface'

See also: 'show performance ?'

See also: 'show volume <name> iostat'
See also: 'show lun iostat'
See also: 'show volume iostat'

See also: 'run benchmark bonnie-benchmark'
See also: 'run benchmark iperf-benchmark'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Show' name='show_import_volumes_usage'><![CDATA[
$cmdline
Usage: [volume-name] [-v] [-D]

Show volumes available for import. Use 'setup volume ... import'
to perform the import.

  -v	        Verbose. Show details on all exported or
  		destroyed (if -D specified) volumes that are available
		for import.
  -D    	Show destroyed volumes only. The -D option allows to
	        reimport volumes destroyed via 'setup volume <vol> destroy'
		command.
  <volume-name> Filter on 'volume-name'. Restrict the output only to
                import-able volumes with a specified name.

See also: 'setup volume import'
See also: 'help zfs' (and look for "import")

See also: 'query'
See also: 'show usage'

See also: 'help'

]]></template>
<template module='Show' name='show_all_runners_usage'><![CDATA[
$cmdline
Usage:
${s} The report includes:

  * runtime states: running, starting, pending, ready (same as idle)
  * status: enabled, disabled, maintenance (the latter - if reached
    maximum fault count)
  * schedule
  * tunables

${s2}Examples:

1) Show all fault triggers
   ${prompt}show trigger

   The result is a summary table that will look as follows:

   RUNNER                  STATUS         STATE        SCHEDULE
   hosts-check             enabled        pending      hourly
   runners-check           enabled        ready        every 12 hours
   nms-zfscheck            enabled        ready        every 12 minutes
   memory-check            enabled        pending      every 12 minutes
   disk-check              enabled        running      hourly
   cpu-utilization-check   enabled        ready        every 15 minutes
   nms-check               enabled        ready        not schedulable
   volume-check            enabled        running      hourly
   ...

2) List all runners, including triggers, collectors and reporters:
   ${prompt}show appliance runners


See also: 'show faults'

See also: 'show appliance auto-services'
See also: 'show appliance nms locks'

See also: 'setup trigger'
See also: 'setup collector'
See also: 'setup reporter'

See also: 'query runner'

See also: 'help runners'
See also: 'help fault-management'
See also: 'help terms'

]]></template>
<template module='Show' name='show_runner_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v	Verbose - show full descriptions of runner's properties.

Show $type '$runner'. The report includes:

  * runtime state: running, starting, pending, ready (same as idle)
  * status: enabled, disabled, maintenance (the latter - if reached
    maximum fault count)
  * ${runner}'s schedule

Use option -v (verbose) to list all ${type}'s properties and their
descriptions.

See also: 'setup $type $runner'

See also: 'help runners'
See also: 'help index'
See also: 'help fault-management'

See also: 'show appliance runners'
See also: 'help terms'

]]></template>
<template module='Show' name='show_faults_usage'><![CDATA[
$cmdline
Usage:
  -v	Verbose - show extended fault descriptions and details.

Show outstanding (that is, active, not acknowledged) faults in the system.
The facility retrieves and displays fault IDs, descriptions, and severities
- either all or on a per fault trigger basis.

The simplest form of this command is:

${prompt}show faults

The 'show faults' command produces a comprehensive fault management report,
"covering" all system services and sub-systems
(including storage and network sub-systems).

Please see more usage examples below.

After the fault is investigated and after the required troubleshooting
action is performed, run:

'setup trigger <trigger-name> $NMC::CLEAR_FAULTS'

This will clear the corresponding fault(s).

See 'setup trigger <trigger-name> $NMC::CLEAR_FAULTS -h' for more
information.

For a network summary on all known $NZA::PRODUCT appliances on the network, see
'show faults all-appliances'

Examples:

${prompt}show faults
or, same:
${prompt}show trigger all-faults
or, same:
${prompt}show trigger all faults

As always, the operation can be executed remotely:
${prompt}fastswitch host1.xyz-corp.com
${prompt}show faults

(host1.xyz-corp.com is a remote appliance's hostname in this example.)

If the system has no outstanding faults/alarms, the output will be:

    The appliance appears to be healthy: no faults

For more information on fault monitoring, reporting, and clearing
please consult $NZA::PRODUCT appliance documentation.

See also: 'help fault-management'

See also: 'show faults all-appliances'
See also: 'show reporter'
See also: 'setup appliance mailer'
See also: 'switch appliance'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_faults_network_usage'><![CDATA[
$cmdline
Usage:

Network-wide Fault Management Summary that may look like follows:

APPLIANCE                    FAULTS  FAILED-SERVICES  SUSPENDED-RUNNERS
localhost                    0       0                0
nexenta-vm1.xyz-corp.com     2       0                0
nexenta-vm2.xyz-corp.com     1       0                3
nexenta-vm3.xyz-corp.com     0       3                0
...

Unlike 'show faults' operation that shows only current appliance's
faults (albeit in detail), this summary report includes
total fault counts for each known $NZA::PRODUCT appliance
on the network.

Upon generating the summary, use a combination of NMC 'switch'
and 'show faults' operations to "zoom-in" into a particular ("faulted")
appliance for details and troubleshooting.

See also: 'show faults'
See also: 'setup appliance mailer'
See also: 'switch appliance'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_appliance_version_usage'><![CDATA[
$cmdline
Usage: [-c] [-C]

  -c	Show changelog, i.e. "what is changed" in this and previous
        releases

  -C	Show remote changelog, i.e. "what is changed" in upcoming upgrades

See also: 'query'
See also: 'show usage'

See also: 'help'

]]></template>
<template module='Show' name='show_appliance_memory_usage'><![CDATA[
$cmdline
Usage: [-V]

  -V	Verbose, detailed output on a per slab basis - expert mode
  	only.

Show appliance's physical memory and memory usage.

Please note! The verbose mode operation will take a considerable time
to complete. Depending on your system, it may be anywhere from 30 seconds
to 10 minutes, and more.

See also: 'switch appliance'  - on how to lookup memory usage
on a remote appliance.

See also: 'query'
See also: 'show usage'

See also: 'help'

]]></template>
<template module='Show' name='show_appliance_syspool_usage'><![CDATA[
$cmdline
Usage:
  -v	Verbose - show all properties of existing checkpoints

Show history of upgrades and system updates, in terms of system
checkpoints and their properties.

  TERMS
  =====
  Throughout the product, the terms "system folder",
  "root filesystem", and "checkpoint" are used interchangeably.

  The appliance's root filesystem (rootfs) contains OpenSolaris
  kernel and system configuration.

Example:

${prompt}show appliance checkpoint

ROOTFS           CREATION      CHECKPOINT-TYPE CURRENT DEFAULT VERSION
rootfs-nmu-005   Jul 30 14:37  rollback        No      No      1.1.9b104
rootfs-nmu-004   Jul 20 10:37  rollback        No      No      1.1.9b104
rootfs-nmu-003   Jun 04 18:15  upgrade         Yes     Yes     2.0.0b104
rootfs-nmu-002   Apr 22 9:31   upgrade         No      No      1.1.7b104
rootfs-nmu-001   Mar 25 10:31  rollback        No      No      1.1.4b104
rootfs-nmu-000   Feb 17 13:27  initial         No      No      1.1.4b104

In the example above, the current (active) checkpoint corresponds to
version 2.0.0 of the $NZA::PRODUCT. The printout also shows that this
appliance was initially installed on Feb 17 (the year 2009 is removed
here for shortness sake), and underwent several software upgrades, some
of them in-place or "live upgrades" denoted in the table above as
"rollback" checkpoints.

In particular, the last two upgrades in the example table shown above
were of the type "rollback", which means: live upgrade not requiring
reboot.
For details on the types of software upgrades and discussion, please
refer to $NZA::PRODUCT User Guide.

Note that you can always reboot into any of the existing checkpoints -
each checkpoint shows up in the GRUB boot menu, with the default one
(DEFAULT = Yes) used by GRUB to boot the appliance.

In fact, you can go back and forth between the checkpoints without
compromising any of the remaining checkpoints in any way. Each
checkpoint is a complete Operating System which was snapshot-ed at
some point. In the example above, this point in time is shown as
CREATION column in the table of checkpoints.

Notice the CURRENT and DEFAULT columns. CURRENT = Yes indicates
the current active checkpoint - the checkpoint you are currently in.
DEFAULT = Yes indicates the default checkpoint from the boot manager
perspective - the one that the appliance will boot by default,
unless you manually select another one in a GRUB menu.

Typically, DEFAULT = CURRENT and both point to the same checkpoint
(like in the example above).

One common question is why the date on the current active checkpoint
is older than the date on some other listed checkpoints?

This is, again, because of the live upgrade mechanism. In the example
above the root filesystem created on June 04 was in-place upgraded
two times, to eventually contain version 2.0. The creation time
however, did not change.


Use option -v (verbose) for extended detailed printout of all existing
checkpoints. Otherwise, all checkpointed system folders are shown
in a single table, with their selected properties including creation
time, state and status.


See also: 'setup appliance checkpoint'

See also: 'help data-replication'

See also: 'switch appliance'

See also: 'show usage'

]]></template>
<template module='Show' name='show_installed_plugin_usage'><![CDATA[
$cmdline
Usage: [-v]
  -v	Verbose - show all properties of selected plugin

Show installed pluggable modules (plugins). Use -v (verbose) to show
plugin details.

Use -v (verbose) to list all known plugin properties.

The most recent information on appliance's plugins is available
on the website, at http://www.nexenta.com

Use 'setup plugin' to install, uninstall and configure
pluggable modules.


See also: 'setup plugin'
See also: 'show plugin remotely-available'
See also: 'show usage'

]]></template>
<template module='Show' name='show_all_appliance_plugin_usage'><![CDATA[
$cmdline
Usage: [-v]
  -v	Verbose - show all properties of selected plugin

  -a    Show all plugins, including those available
        in the remote software.

By default, only already installed plugins are listed. Use the -a option
to list all available plugins, or run 'show plugin remotely-available'.

Show installed and remotely available pluggable modules (plugins).

The most recent information on appliance's plugins is available
on the website, at http://www.nexenta.com

Use 'setup plugin' to install, uninstall and configure
pluggable modules.

Use 'show plugin remotely-available' to list plugins available in the
remote $NZA::PRODUCT software repository.


See also: 'setup plugin'
See also: 'show plugin remotely-available'
See also: 'show plugin installed'

]]></template>
<template module='Show' name='show_remote_available_plugin_usage'><![CDATA[
Usage: [-t nms|nmv|nmc]

  -t nms|nmv|nmc     Plugin type = nms | nmv | nmc
  -v 		     Verbose, show all details

Search and list plugins in a remote repository.
The resulting list specifies plugins available for installation.

The most recent information on appliance's plugins is available
on the website, at http://www.nexenta.com

Use 'setup plugin' to install, uninstall and configure
pluggable modules.

Use 'show plugin installed' to list already installed plugins.


See also: 'setup appliance repository'
See also: 'setup plugin'
See also: 'show plugin installed'

]]></template>
<template module='Show' name='show_appliance_authentication_usage'><![CDATA[
The appliance provides secure access to other $NZA::PRODUCT
appliances, as well as administrative management client applications
on the network. The inter-appliance access is executed either via SSH,
or via SA-API (User Guide, Section "Storage Appliance API"), or both.

All management client applications, whether developed internally
by Nexenta Systems, Inc and/or by 3rd parties, access appliance
via SA-API.

In all cases, access to an appliance requires client authentication.
$NZA::PRODUCT supports two authentication mechanisms:

  1) via IP address of the client machine
  2) via ssh-keygen generated authentication keys

The 2nd, ssh-keygen based, mechanism is the preferred one.
This is the mechanism used by $NZA::PRODUCT appliances to communicate
between themselves. The latter is required to run storage replication
services, to execute in a group mode, to switch between appliances for
the purposes of centralized management. To enable inter-appliance
communication, simply use 'ssh-bind' command
(see 'setup network ssh-bind').

Once the appliances are ssh-bound, all the capabilities mentioned
above are enabled automatically, and executed in a secure way.
Please see User Guide Section "Centralized Management" for more
information.

To use IPv4 address based authentication, use
'setup appliance authentication' and select 'dbus-iptable' option.

To use ssh-keygen generated authentication keys with your management
application running on Windows, Linux or any other platform,
use the same command 'setup appliance authentication' command,
and select option 'keys'.


See also: 'setup appliance $NMC::client_auth keys'
See also: 'setup appliance $NMC::client_auth dbus-iptable'

]]></template>
<template module='Show' name='show_appliance_auth_keys_usage'><![CDATA[
$cmdline
Usage: [-v]
  -v	Verbose - show complete authorized keys

For a background on $NZA::PRODUCT authentication, please run:

   ${prompt}show appliance $NMC::client_auth -h

See also: 'setup appliance $NMC::client_auth'
See also: 'show appliance $NMC::client_auth dbus-iptable'
See also: 'show usage'

]]></template>
<template module='Show' name='show_appliance_license_agreement_usage'><![CDATA[
$cmdline

Display $NZA::PRODUCT software license agreement. For license key,
please use 'show appliance license'.


See also: 'setup appliance register'
See also: 'show appliance license'

See also: 'show usage'

]]></template>
<template module='Show' name='show_appliance_license_usage'><![CDATA[
$cmdline
Usage: [-l]

Show $NZA::PRODUCT software license key. To show license agreement,
use the -l option or run 'show appliance license agreement'.

  -l	Show actual license agreement.

For details and more information on $NZA::PRODUCT licensing,
please search F.A.Q. pages on the website.

Example:

${prompt}show appliance license
Model             : Unified Storage Appliance (Enterprise Edition)
Machine Signature : ABCDEFKX
License           : T016-DE65F32570-ABCDEFKX-DHMXAK
Key Type          : T016
Days Left         : 333


See also: 'setup appliance register'
See also: 'show appliance license agreement'

See also: 'show usage'

]]></template>
<template module='Show' name='show_appliance_license'><![CDATA[
                          * * *
                      LICENSE ERROR

    $err
    The following options are available at this point:

       (a) purchase $NZA::PRODUCT license on-line or via third party
           and re-register the appliance
       (b) stop using $NZA::PRODUCT software

    Note that Your data stored on the appliance remains intact and
    accessible via network shares, including NFS and CIFS shares.

]]></template>
<template module='Show' name='show_appliance_driver_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v	Verbose. Show device driver details.

Display status of third party drivers.

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_uptime_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v	Verbose.

Shows appliance's uptime - the time passed since last boot.

In a verbose mode (the -v option), the output includes the
current time, how long the system has been running, number of active
terminals (users), and the appliance's load averages for the past
1, 5, and 15 minutes.

See also: 'help uptime'
See also: 'show usage'

]]></template>
<template module='Show' name='show_appliance_upgrade_usage'><![CDATA[
$cmdline
Usage: [-v] [-a]

  -v	Verbose. Show detailed information for each release

  -a	Include releases that can't be used for upgrade
    	without reinstalling the system.

The command will download list of currently available releases
and show ones that can be used for upgrade.

See also: 'show appliance repository'
See also: 'setup appliance upgrade'

]]></template>
<template module='Show' name='show_appliance_sysinfo_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v	Verbose. Show SCSI/IDE/SATA device details, if present.

See also: 'help sysinfo'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_jbod_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v		Verbose. Show properties for all JBODs.

Show list of available JBODs in the appliance.
The table includes:

 * alias
 * model
 * available and busy slots
 * serial number
 * brief sensors status. (list of failed sensors or 'ok' if there are no one)

You can append JBOD name to view properties only for one JBOD

See also: 'show jbod all'
See also: 'show jbod alerts'

See also 'show jbod <jbod-name> sensors'
See also 'show jbod <jbod-name> slotmap'
See also 'show jbod <jbod-name> model'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_jbod_alerts_usage'><![CDATA[
$cmdline

Show failed sensors for all JBODs. You can append JBOD name
to view properties only for one JBOD.

The output is the same as for 'show jbod <jbod-name> sensors'

See also: 'show jbod all'
See also: 'show jbod'

See also 'show jbod <jbod-name> sensors'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_jbod_all_usage'><![CDATA[
$cmdline

Show all information that available for all JBODs. You can
append JBOD name to view information only for one JBOD.

The output is the combination of outputs for :

 * 'show jbod <jbod-name> -v'
 * 'show jbod <jbod-name> slotmap'
 * 'show jbod <jbod-name> sensors'

See also: 'show jbod -v'
See also: 'show jbod <jbod-name> slotmap'
See also: 'show jbod <jbod-name> sensors'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_jbod_sensors_usage'><![CDATA[
$cmdline

Show all sensors for specified JBOD.
The table includes:

 * element name
 * sensor name
 * value
 * state

Element name is in form of <element type>:<id> or just 'jbod' for
JBOD self.

Sensor name also can include ID if there are more than one sensor with
same type in the element.

The value includes units name if it can be usefull.

To view only failed sensors use 'show jbod [<jbod-name>] alerts.

See also: 'show jbod all'
See also: 'show jbod'

See also 'show jbod alerts'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_jbod_luns_usage'><![CDATA[
$cmdline
Usage: <jbod-name> lun

List inserted disks. The table includes:

 * slot number (started from 1)
 * LUN name
 * Device Id

The table also shows empty slots. Empty values are replaced
with '-'.

Rows are ordered by slot number

You can use TAB to autocomplete disks related to specified JBOD
and use any command from 'show lun <name> ...' subtree.

See also: 'show jbod all'
See also: 'show jbod'

See also: 'show jbod <jbod-name> slotmap'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_jbod_model_usage'><![CDATA[
$cmdline

Show current JBOD model and a JBOD name returned via SES protocol.

In case the current model is set to a vendor/product of known expander
chipset, nmc suggests to rename the model name.

See also' 'setup jbod <jbod-name> model rename'

See also: 'show jbod'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_jbod_slotmap_usage'><![CDATA[
$cmdline

Show ASCII picture for JBOD and list inserted disks.

Not all JBOD's model has own ASCII pictures. In this case
the picture will be silently omitted.

The list with inserted disks is the same as for
'show jbod <jbod-name> lun' command.

See also: 'show jbod all'
See also: 'show jbod'

See also: 'show jbod <jbod-name> lun'
See also: 'show lun'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='blink_jbod_element_usage'><![CDATA[
$cmdline

Turn on identify LED on the given element. Use this
command to quickly find broken fan or psu before
replacing.

To turn off the LED press Ctrl-C.

To blink slot use command 'show $NMC::LUN <lun> blink'
or 'show jbod <jbod-name> $NMC::LUN <lun> blink'

See also: 'show jbod all'
See also: 'show jbod'

See also: 'show jbod <jbod-name> lun <lun> blink'
See also: 'show $NMC::LUN <lun> blink'
See also: 'show jbod <jbod-name> blink'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='blink_jbod_usage'><![CDATA[
$cmdline

Turn on identify LED on JBOD self. Use this command
to quickly find JBOD in the server rack.

To turn off the LED press Ctrl-C.

See also: 'show jbod <jbod-name> lun <lun> blink'
See also: 'show lun <lun> blink'
See also: 'show jbod <jbod-name> <element> <id> blink'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_lun_slotmap_usage'><![CDATA[
$cmdline

Show LUN to physical slot mapping for all SCSI LUNs in the appliance.
The latter includes:

 * Disk drives
 * Tape drives
 * Tape library changer devices
 * iSCSI devices
 * FireWire devices

If the appliance has attached JBODs than ASCII image for the JBODs will be
shown and new column 'JBOD' will be added to the mapping list.

Slot map has the following outline:

  { disk GUID => slot number }

This map, as well as JPEG image of the box (with drive slots shown
and enumerated) is used then by the appliance's UI to perform
related monitoring and management (including fault management)
operations.

Use 'setup lun slotmap' to setup physical slot to disk mapping.


See also: 'setup lun slotmap'

See also: 'query lun'
See also: 'show lun'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Show' name='show_disk_usage'><![CDATA[
$cmdline
Usage: [-v] [-V]

  -v		Verbose. Show SCSI detailes, if present.
  -V		Verbose plus. Same as above, plus read and write cache settings,
	 	if present.

Show all LUNs in the appliance, LUNs of a specified type,
and/or a specified device. The level of detail is set by -v/-V switches.

In general, the 'show lun' command has several variations, and the
resulting output depends on whether the device type and/or device
name is specified. Some of the examples are given below:

 1) Show a specific device, in detail

    ${prompt}show lun c2t0d0 -v

A sample output is a list of device properties and values:
PROPERTY                      VALUE
name                          c2t0d0
device                        sd0
type                          scsi
...			      ...
...			      ...

 2) Show all DISK devices in the system:

    ${prompt}show lun disk

A sample output follows below. This shows the device IDs and logical
names, the bus type (scsi), raw size, storage volume (empty, if
not allocated).

Note that a new volume may be created out of any subset of not yet
allocated and not mounted drives. For more on volume creation see
'create volume'.

LUN ID    Device    Bus     Size        Volume      Mounted  Remov   Attach
c2t0d0    sd13      scsi    8.7GB                   yes      no      DAS
c2t1d0    sd15      scsi    8.7GB                   no       no      DAS
c2t2d0    sd17      scsi    8.7GB                   no       no      DAS
c4t1d0    disk1     scsi    1668.9GB    tierN       no       no      iSCSI
c4t2d0    disk2     scsi    1668.9GB    tierN       no       no      iSCSI
c4t3d0    disk3     scsi    1335.1GB    volsync     no       no      iSCSI
c4t4d0    disk4     scsi    1335.1GB    volsync     no       no      iSCSI
c4t5d0    disk5     scsi    1668.9GB    tierN       no       no      iSCSI
c4t6d0    disk6     scsi    1668.9GB    tierN       no       no      iSCSI
c4t7d0    disk7     scsi    1668.9GB    tierN       no       no      iSCSI

See also: 'create volume'
See also: 'show volume <vol-name> status'

See also: 'lunsync'

See also: 'query lun'

See also: 'switch appliance'
See also: 'show usage'

See also: 'help'

]]></template>
<template module='Show' name='show_disk_iostat_usage'><![CDATA[
$cmdline
Usage: [-i interval] [-c count]   [-D] [-E] [-M] [-z]

 -i <interval>  Print I/O statistics every <interval> seconds.
                Default: 1 second.

 -c <count>     Number of iostat <interval>-long iterations.
 	        Default: 10 iterations.

 -D             Report the reads per second, writes per second,
	        and percentage disk utilization

 -E             Display all device error statistics
 -M             Display data throughput in MB/sec instead of KB/sec.
 -z             Do not print lines consisting of all zeros.


Example: report 3 consecutive iterations of IO stats
${prompt}show lun iostat -c 3

A sample output follows:

                 extended device statistics                      cpu
 device    r/s    w/s   kr/s   kw/s wait actv  svc_t \%w \%b  us sy wt id
 sd0     205.0  100.0 1016.0  576.0  0.0  5.0   10.0   0  92
 ...

The output includes the following information.
 device    name of the disk
 r/s       read operations per second
 w/s       writes operations per second
 kr/s      kilobytes read per second (read bandwidth)
 kw/s      kilobytes written per second (write bandwidth)
 wait      average  number  of  transactions  waiting  for  service
 actv      average number of transactions actively being serviced
 \%w       percent of time there  are  transactions  waiting  for  service
 \%b       percent of time the disk is busy (transactions in progress)


See also: 'dtrace'

See also: 'show volume iostat'
See also: 'show volume <name> iostat'

See also: 'show performance'

See also: 'show auto-sync <name> stats'(*)
See also: 'show auto-tier <name> stats'

See also: 'run benchmark bonnie-benchmark'
See also: 'run benchmark iperf-benchmark'

See also: 'help iostat'
See also: 'help dtrace'
See also: 'show usage'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Show' name='show_disk_blink_usage'><![CDATA[
$cmdline
Usage: [-y] [-a attempts] [-p seconds] [-t seconds]

  -a <attempts>	   Number of disk read attempts, default is 60

  -p <seconds>	   Pause (in seconds) between the attempts, default
                   is 2 seconds

  -t <seconds>     Disk read attempt duration (in seconds). If
                   parameter is omitted then attempt duration is
		   undetermined and depends on hardware read speed.

  -y               Skip confirmation dialog by automatically answering Yes

Enable blinking LED activity for disk '$disk'.

This command may help to identify physical disk location within
the box or attached JBOD.

Examples:
1) Blink $disk LED for a 10 seconds:
   ${prompt}show $NMC::LUN $disk blink -t 10

2) Blink $disk LED for a two time intervals (10 seconds each) with 5
   second pause between them:
   ${prompt}show $NMC::LUN $disk blink -a 2 -p 5 -t 10

3) Blink $disk LED for a three time interval with 2 second pause
   between them:
   ${prompt}show $NMC::LUN $disk blink -a 3


See also: 'show lun slotmap'
See also: 'show usage'

]]></template>
<template module='Show' name='show_network_appliances_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v 	Output effectively comprising all 'show all' reports
        run on each appliance
  -l    Include the local appliance (localhost) in the report

List all $NZA::PRODUCT appliances in the network
with or without details, depending on the requested verbosity.

$NZA::PRODUCT appliance may or may not be ssh-bound. Non-bound
appliances are discovered automatically.

To ssh-bind a new appliance, run 'setup network ssh-bind',
and enter a hostname and password.

Examples:

1) List all ssh-bound and/or dynamically discovered network
   $NZA::PRODUCT appliances; include local appliance:

   ${prompt}show network appliance -l

An output may look like:

APPLIANCE                  OS       NMS   BOUND/ACCESS  REL. DATE
localhost                  0.97     -     -/-           Mar 14 17:05:48 2007
nexenta-vm1.xyz-corp.com   0.94b67  0.95  Yes/Yes       Jul 23 21:39:55 2007
nexenta-vm2.xyz-corp.com   0.96b69  0.96  Yes/No        Jul 23 21:39:55 2007
standalone-vm.xyz-corp.com 0.96b69  0.96  No/No         Jul 23 21:39:55 2007

The BOUND/ACCESS column refers to ssh binding of a given
appliance; not that an appliance may not be ssh-bound, and if it is
ssh-bound, it may not accessible via ssh at the time of operation.

2) Show a specific appliance in great detail
   ${prompt}show network appliance host1.xyz-corp.com -v

3) Show all remote appliances in great detail
   ${prompt}show network appliance -v

See also: 'show network ssh-bindings'
See also: 'switch appliance'
See also: 'fastswitch'
See also: 'setup network ssh-bind'

See also: 'query'
See also: 'show usage'

]]></template>
<template module='Show' name='show_ssh_bindings_usage'><![CDATA[
$cmdline
Usage: [-v]

 -v   Verbose mode - show details, including all ssh bound hostnames.

Show $NZA::PRODUCT appliances and non-appliances that are SSH bound to the
local appliance.

Clustering, tiering over SSH, syncing (that is, running $NZA::PRODUCT
auto-sync service - see User Guide, Section "Terminology") over SSH,
and managing remote appliance over SSH - these are some of the
capabilities of the $NZA::PRODUCT appliance that rely on pre-existing trust.

SSH binding allows for remote management as well as strong cryptographic
verification of multi-host trusts, forming a network of ssh-bound hosts.

SSH binding is an operation that establishes a secure connection
to a remote host using the SSH protocol. Creating ssh binding between
two appliances is required when using appliance groups, clusters,
replication services, VMDC and other features of the appliance.

Generally, to establish SSH binding, all you need is a hostname
and a root password of the remote host
(which may be another $NZA::PRODUCT appliance).

Examples:

1) List all ssh bindings

   ${prompt}show network ssh-bindings

A sample output may look as follows:

HOST            PINGABLE  SSH-ACCESSIBLE  IS-APPLIANCE
root\@myhost    Yes       Yes             Yes

2) List full information of ssh bindings

   ${prompt}show network ssh-bindings -v

A sample output may look as follows:

HOST: root\@myhost
  Pingable       : Yes
  Ssh-Accessible : Yes
  Is-Appliance   : Yes
  Hostname(s)    : myhost.mydomain 192.168.1.1


See also: 'setup network ssh-bind'
See also: 'setup network ssh-unbind'

]]></template>
<template module='Show' name='show_ssl_bindings_usage'><![CDATA[
$cmdline
Usage: [-v]

  -v 	Show certificate contents insteed of short host binding description

Show either information about host binding or full certificates info contained in database

Examples:

1) List sertificate aliaces, subject hosts & expiration dates

   ${prompt}show network ssl-bindings

An output look like this:

ALIAS			HOSTNAME		EXPIRES
My Certificate1		myhost.xyz-corp.com   	Jul 23 21:39:55 2007
ABC 			abc.xyz-corp.com   	Jul 23 21:39:55 2007

2) List database sertificates main data

   ${prompt}show network ssl-bindings -v

An output look like this:
Certificate:
    Data:
        Version: 1 (0x0)
        Serial Number:
            cf:3e:b6:a6:05:c1:c9:6c
        Signature Algorithm: sha1WithRSAEncryption
        Issuer: C=US, ST=California, L=Cupertino, O=Nexenta Systems Inc, OU=Storage Technology, CN=nmv.nexenta.com/emailAddress=support\@nexenta.com
        Validity
            Not Before: Feb 26 07:05:45 2008 GMT
            Not After : Feb 24 07:05:45 2013 GMT
        Subject: C=US, ST=California, L=Cupertino, O=Nexenta Systems Inc, OU=Storage Technology, CN=nmv.nexenta.com/emailAddress=support\@nexenta.com
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
Trusted Uses:
  TLS Web Client Authentication
No Rejected Uses.
Alias: nexstor


See also: 'setup network ssl-bind'
See also: 'setup network ssl-unbind'

]]></template>
<template module='Show' name='show_idmap_usage'><![CDATA[
$cmdline
Usage: [identity]

  <identity>    - user name, user ID, group name, or group ID.

Identity is specified as [type:]value, where the type is one
of the following:

        usid         Windows user SID in text format
        gsid         Windows group SID in text format
        sid          Windows group SID in text format that can belong
	             either to a user or to a group
        uid          Numeric POSIX UID
        gid          Numeric POSIX GID

        unixuser     UNIX user name
        unixgroup    UNIX group name
        winuser      Windows user name
        wingroup     Windows group name

If <indentity> is omitted, the command will list all existing mapping
rules.

Quoting idmap(1m):

  The Native Identity Mapping service (idmap) supports two types of
  mappings between Windows security identities (SIDs) and POSIX
  user IDs and group IDs (UIDs and GIDs):

  o      Name-based mapping. An administrator maps Windows and UNIX
	 users and groups by name.

  o      Ephemeral ID mapping. A UID or GID is dynamically allocated
         for every SID that is not already mapped by name.

Examples:

1) List all existing mapping rules and dump all the mappings
   cached since the last system boot:

   ${prompt}show network service cifs-server idmap


2) Show mapping of a given identity:

   ${prompt}show network service cifs-server idmap joe


See also: 'idmap'
See also: 'setup network service $service'

]]></template>
<template module='Show' name='show_cifs_service_usage'><![CDATA[
$cmdline
Usage: [-v] [-V] [-l]

  -v	Verbose. Shows CIFS local groups.

  -V	Verbose-verbose. Shows extended information, including PAM and
        log records.

  -l    Show full LUN ID.

Show CIFS server state, start time and the mode of operation:
domain or workgroup.

In the domain mode, the command will also list domains controllers:
primary, local, selected.

Use -v (verbose) option to show CIFS local groups, group privileges,
and group members.

Use -V (verbose-verbose), to include extended details.


See also: 'idmap'
See also: 'setup network service $service'

]]></template>
<template module='Show' name='show_cacerts_usage'><![CDATA[
$cmdline

Lists CA certificates, installed on appliance to work with LDAP client.

]]></template>
<template module='Show' name='show_ndmp_performance_usage'><![CDATA[
Shows performance characteristics for NDMP sessions. Optional parameters are
	-p Time to show performance statistics, in minutes. Default value is unlimited
	-i Interval between information update, in seconds. Default value is 1 second

Example:
	show network service ndmp-server performance -p 1 -i 5

]]></template>
<template module='Show' name='show_share_usage'><![CDATA[
$cmdline
Usage: [-v] [-N] [nfs | cifs | webdav | ftp | rsync]

  -v	Verbose output.
  -N	Non-recursive; do not show nested shares

Show all shares, or all shares of a given type.

Examples:

1) Show absolutely all network shares in the system:

${prompt}show share

This will list in a tabular form all NFS-shared folders, all
CIFS-shared folders, and all WebDAV, FTP, and RSYNC shared folders.

2) Same as above, with detailed (verbose) output:

${prompt}show share -v

3) Show all NFS shares:

${prompt}show share nfs

4) Show all NFS shares for a given folder 'vol1/a/b':

${prompt}show folder vol1/a/b share nfs

5) Same as above, detailed output:

${prompt}show folder vol1/a/b share nfs -v

6) Same as above, but list just a given folder 'vol1/a/b'
and produce detailed output:

${prompt}show folder vol1/a/b share nfs -v -N


See also: 'show share nfs'
See also: 'show share cifs'
See also: 'show volume <vol-name> share'
See also: 'setup zvol <name> unshare'
See also: 'setup zvol <name> share'
See also: 'setup folder <name> unshare'
See also: 'setup folder <name> share'
See also: 'setup volume <vol-name> unshare'
See also: 'setup volume <vol-name> zvol <name> unshare'
See also: 'setup volume <vol-name> zvol <name> share'
See also: 'setup volume <vol-name> folder <name> unshare'
See also: 'setup volume <vol-name> folder <name> share'
See also: 'setup volume <vol-name> share'

]]></template>
<template module='Show' name='show_all_appliance_usage'><![CDATA[
$cmdline
Usage: [-v]

 -v   Verbose mode. Include the current system definition details,
      List all hardware devices, as well as pseudo devices, system devices,
      loadable modules, and the values of selected kernel tunable parameters.
      Also, show saved appliance configurations, if any.

Show _all_ appliance: all its services, interfaces, shares, volumes, and
hardware at a glance.

Example:
${prompt}show all

The output (which may be quite lengthy) will include the following items:
	=========== Service: auto-snap ============
	...
	=========== Service: auto-scrub ============
	...
	=========== Service: auto-sync(*) ============
	...
	=========== Service: auto-tier ============
	...
	==== Interfaces ====
	...
	==== Routes ====
	...
	========== Volumes and Folders ===========
	...
	========== Volume Status/IOSTAT ===========
	...
	=============== IOSTAT ================
	...
	==== Swap Partition ====
	...
	========== NFS Shares ===========
	...
	=========== Memory ============
	...
	=========== Runners ============
	...
	=========== Faults ============
	...
	==== System Information ====
	...
	==== Processor Information ====
	...


See also: 'run diagnostics'

See also: 'query'
See also: 'show usage'
See also: 'show network appliance'

See also: 'help'

Note:
   (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Show' name='show_appl_groups_usage'><![CDATA[
$cmdline
Usage: [group-name]

Type once, run anywhere! Appliance grouping facilitates single-point
management of multiple appliances.

This NMC interface can be used to show all currently defined groups of
appliances, or a single group specified in the command line.

Run 'show network $NMC::appliance' to list all appliances on the
network.

Once a group is defined, use 'switch' command to change to this group
(or any other group defined at this point). See 'switch $NMC::APPL_GROUP -h'
for details.

See also 'setup $NMC::APPL_GROUP -h' for information on how to setup
a new group, usage details and examples.

See also: 'show network appliance'
See also: 'show usage'

]]></template>
<template module='Show' name='show_iscsi_tpgt_usage'><![CDATA[
$cmdline

Show existing iSCSI Target Portal Groups.

Target Portal Group is a standard mechanism to control access to the
appliance's iSCSI target. Each Target Portal Group combines one or
more of the appliance's local IP addresses. For instance, if the
appliance is a multihomed host accessible via IP addresses
(IP_1, IP_2, IP_3), you could define one target portal group that
contains (IP_1, IP_2) and another that contains only IP_3.
Obviously, this would be just an example, and there are many other valid
permutations of portal groups based on a 3 given IP addresses.

To prevent iSCSI Target discovery via a certain existing local IP
address(es), simply do not publish those IP addresses via
Target Portal Groups. This will effectively restrict iSCSI traffic
to those and only those local IP interfaces that are explicitly
specified via configured Target Portal Groups.

Each portal group is identified by its TPGT number, where TPGT stands
for Target Portal Group Tag.


See also: 'setup iscsi target tpgt'
See also: 'create iscsi target tpgt'
See also: 'setup zvol'

See also: 'show iscsi target tpgt'

See also: 'setup zvol <name> tpgt'
See also: 'setup iscsi target tpgt create'

]]></template>
<template module='Show' name='show_iscsi_discovery_address_usage'><![CDATA[
$cmdline
Usage: [-v]

   -v      List all known targets; produce detailed output

When used with the -v option, lists all known targets at a specified
discovery-address. The -v option returns one or more  target names along
with zero or more target addresses and associated target portal group tags
(TPGT), if applicable.

See also: 'show iscsi discovery isns-server'
See also: 'show iscsi discovery static-config'

]]></template>
<template module='Show' name='show_iscsi_discovery_isns_usage'><![CDATA[
$cmdline
Usage: [-v]

   -v      List all known targets; produce detailed output

When used with the -v option, this subcommand lists all known targets at a
specified isns-server address. The -v option returns one of more target
names along with zero or more target addresses and associated target portal
group tags, if applicable.


See also: 'show iscsi discovery discovery-address'
See also: 'show iscsi discovery static-config'

]]></template>
<template module='Show' name='show_appliance_user_usage'><![CDATA[
$cmdline	
Usage: [username] [-v] [-f filter]

  -v		Verbose output

  -f <filter>	Filter usernames. Note that filtering of user names
  		is especially useful when interacting with large
		LDAP/AD databases, especially since the LDAP or AD
		server may impose administrative limit on the size
		of the associated result set
		The following notation finds all users with the
		username starting with 'jo': -f jo
	   	See more examples below.

Show a given user, if a username is specified in the command line.
Otherwise, show all users, both locally defined and LDAP/AD managed.

Filtering of user names (the -f option) may speed up the query
and also may help to circumvent the administrative limitation
on the number of records returned by the LDAP or AD server.


Examples:

1) Show all users, in a tabular form (note: the output includes
   only the most basic properties):

   ${prompt}show appliance user

   Note that this query may fail with especially large LDAP database.
   Use filter - see more examples below.

2) Show a list of all users, with all their properties:

   ${prompt}show appliance user -v

   Note that this query may fail with especially large LDAP database.
   Use filter - see more examples below.

3) Show a given user 'joe'

   ${prompt}show appliance user joe

4) Show all users with names starting with 'a', in a tabular form;
   the output will include only the most basic properties:

   ${prompt}show appliance user -f a

   The result may contain users such as 'alice', 'alex', 'albert', etc.

5) Show all users with usernames starting with 'jo', in detail;

   ${prompt}show appliance user -v -f jo

   The result may contain users such as 'john', 'joe', 'joseph', etc.

See also: 'setup appliance user'
See also: 'create appliance user'
See also: 'destroy appliance user'

See also: 'show appliance $NMC::USERGROUP'
See also: 'show network netgroup'
	
]]></template>
<template module='Support' name='support_usage'><![CDATA[

Usage: [-s directory]

  -s   Save collected information to a given directory instead of
       sending it by e-mail. In this case NMC asks for level of
       information detail only. Then NMC gathers information,
       saves it and displays name of files that should be sent to
       support technicians.

Request Technical Support.

$NZA::PRODUCT web GUI and management console both provide a simple and
effective mechanism to generate support requests.

In NMC, run 'support' and follow the prompts. The operation results in
a simple email to support technicians via configured SMTP mail server
(see 'setup appliance mailer').

For this facility to work, it is essential that the appliance's mailer
is propertly configured. To verify, please run:

${prompt}setup appliance mailer verify

Support request will automatically include a snapshot of your
system settings and configuration. Collected information is then
analyzed by support technicians, which ultimately reduces the amount of
time spent on support and trouleshooting.

]]></template>
<template module='Support' name='support_usage1'><![CDATA[
                   *** NOTE ***

     Technical support is provided based on the $NZA::PRODUCT
     product licensing terms. See your software license for details.
]]></template>
<template module='Support' name='support_usage2'><![CDATA[

For more information, please see $NZA::PRODUCT Quick Start Guide
document, Section "Documentation and Technical Support".


See also: 'setup appliance mailer'
See also: 'show all'

]]></template>
<template module='SysRestore' name='_welcome'><![CDATA[
                          * * *
                      SYSTEM RESTORE
 								 
     Welcome to System Restore Wizard. The appliance's root filesystem
     (rootfs) contains OpenSolaris kernel and system configuration. 
     
     The Wizard will guide you through the steps to restore system 
     configuration, or replicate the entire appliance's root filesystem
     from a saved location.

     The operation is protected by $NZA::PRODUCT system checkpointing
     mechanism. For additional information, please see User Guide at:
     http://www.nexenta.com/docs

]]></template>
<template module='SysRestore' name='_two_principal_ways'><![CDATA[

     There are two principal ways to restore a root filesystem:

        (1) clone an existing root filesystem

        (2) copy differences (delta) from an existing root filesystem,
	    which again may be local or remote, residing on another
	    host, which in turn may not necessarily be a $NZA::PRODUCT
	    appliance.

     Cloning of an existing root filesystem results (as the name
     implies) in exact clone of the original rootfs. 
     
     If the source rootfs does not already reside on the appliance's
     system volume, it will be first transported over via $NZA::PRODUCT
     auto-sync mechanism. It will then be converted in-place into a
     new bootable root filesystem.

     Another alternative is to copy over only the differences (the
     delta) from the designated source root filesystem. 
     This typically implies the source having the same version number.
     You may want to use this approach to synchronize system
     configuration between two appliances, or replicate system
     configuration from a single source.

     Please make your selection:
	     
     ${NMC::RL_REVERSE}1${NMC::RL_NORMAL}  - clone root filesystem from local or remote location
   
     ${NMC::RL_REVERSE}2${NMC::RL_NORMAL}  - copy differences only: source => local rootfs

]]></template>
<template module='SysRestore' name='_welcome_local_clone'><![CDATA[
	
     The source root filesystem that needs to be replicated (cloned)
     may reside either locally or remotely.

     Having a local snapshot or a local copy of a root filesystem
     typically implies that you had already used $NZA::PRODUCT 
     $NZA::AUTO_SYNC_PLUGIN(*) service to replicate remote appliance's rootfs,
     with the $NZA::AUTO_SYNC_PLUGIN(*) destination being one of the local
     appliance's local volumes.

     For information on how to backup or snapshot root filesystem,
     and for general information on appliance's Data Replication
     services, please see:

     * 'create $NZA::AUTO_SYNC_PLUGIN'(*)
     * 'help data-replication'

     * 'setup appliance checkpoint <name> snapshot'
     * 'show appliance checkpoint <name> snapshot'

Note:
   (*) - Requires the corresponding auto-service plugin.
   
]]></template>
<template module='SysRestore' name='_create_checkpoint_tier_dest'><![CDATA[

     Note that the current active root filesystem is left intact.
     The System Restore Wizard will first, clone the current rootfs,
     and second, copy the differences into this new cloned location...
]]></template>
<template module='SysRestore' name='system_restore_usage'><![CDATA[

Welcome to System Restore Wizard. The appliance's root filesystem 
(rootfs) contains OpenSolaris kernel and system configuration. 

The Wizard will guide you through the steps to restore system 
configuration, or replicate the entire appliance's rootfs from a saved 
location.

  TERMS
  =====
  Throughout the product, the terms "system folder",
  "root filesystem", and "checkpoint" are used interchangeably.

There are two principal ways to restore a root filesystem:

  1) clone an existing root filesystem

  2) copy differences (delta) from an existing source 
  
In each case the source to restore from may be local or remote,
residing on another host, which may not necessarily be $NZA::PRODUCT 
appliance.

Cloning of an existing  root filesystem results, as the name implies,
in exact clone of the original rootfs. If the original (the source) 
rootfs does not already reside on the appliance's system volume, 
it will be first transported over from its source location using 
$NZA::PRODUCT auto-sync mechanism. It will then be converted in-place
into a new bootable root filesystem.

The easiest and fastest way to clone rootfs is to clone an existing
local snapshot or a local copy. Having a local snapshot or a local copy
typically implies that you had already used $NZA::AUTO_SYNC_PLUGIN(*) service 
to replicate remote appliance's rootfs, with the auto-sync destination 
being one of the local appliance's volumes.

The second principal way - copying differences - requires having two 
root filesystems of the same $NZA::PRODUCT release that you'd like to 
synchronize, configuration wise.

Please note! System Restore is protected by the $NZA::PRODUCT system 
checkpointing mechanism. The current appliance's root filesystem will 
be checkpointed prior to any modification.

Please note! You can always reboot appliance into any existing
checkpoint, by selecting the latter in the GRUB menu. You can always
promote any existing checkpoint to become a default checkpoint, via: 

${prompt}setup appliance checkpoint <name> activate
 
The default checkpoint (unlike any other checkpoint) is automatically 
selected by the appliance's boot manager (GRUB) at boot time. 


  SELECTED CONFIGURATION ITEMS
  ============================
  To save and restore certain distinguished items of the appliance's
  configuration, please use:

   * 'setup appliance configuration save'
   * 'setup appliance configuration restore'

  See related manual pages for more information.
   

See also: 'help data-replication'

See also: 'setup appliance checkpoint'
See also: 'setup appliance <checkpoint-name> destroy'
See also: 'setup appliance <checkpoint-name> activate'
See also: 'setup appliance <checkpoint-name> property'

See also: 'setup appliance upgrade'
See also: 'setup appliance reboot'

See also: 'show appliance checkpoint'

See also: 'create $NZA::AUTO_SYNC'(*)

See also: 'help keyword clone'
See also: 'setup snapshot <name> clone'

See also: 'setup appliance configuration set-location'.
See also: 'setup appliance configuration save'
See also: 'setup appliance configuration restore'

See also: 'setup usage'

Note:
   (*) - Requires the corresponding auto-service plugin.
]]></template>
<template module='SysRestore' name='_summary_report'><![CDATA[

***************************************************************************
*                  SYSTEM RESTORE SUMMARY REPORT                          *
*                                                                         *
* $info
* $info_new
*                                                                         *
* The $cur_def appliance's root filesystem (checkpoint) is:                *
* $info_default
*                                                                         *
* Use:                                                                    *                                              
*   nmc\$ show appliance checkpoint  - to list existing checkpoints        *
*   nmc\$ setup appliance checkpoint - to administer existing checkpoints  *
*                                                                         *
* You can now reboot appliance into the new root filesystem:              * 
* $info_new
*                                                                         *
* Please note! You can reboot appliance into any existing checkpoint      * 
* at any time, by selecting the latter in the GRUB menu.                  *
*                                                                         *
* You can always promote any existing checkpoint to become a default      *
* checkpoint, via 'setup appliance checkpoint <name> activate'.           *
*                                                                         *
* The default checkpoint (unlike any other checkpoint) is automatically   *
* selected by the appliance's boot manager GRUB at boot time.             *
*                                                                         *
***************************************************************************

]]></template>
<template module='Unalias' name='unalias_usage'><![CDATA[
$cmdline
Usage: [name] [-s] [-a]

  -s       Remove alias from NMC configuration file.
  -a       Remove all aliases.
   

Aliases and the commands they alias can be used interchangeably.
Run 'alias' without parameters to list the current aliases,
or see 'alias -h' for details.

Use -s (store) option, to remove user-defined aliases from
NMC configuration; alternatively you could use 
'setup appliance nmc edit-settings' to edit the entire
configuration file.

Make sure to use 'edit-settings' sub-command (above) to
store the user-defined aliases persistently and use them
for the current and future NMC sessions.

See also: 'alias' 
See also: 'help options'

See also: 'options' 
See also: 'help commands' 
See also: 'option -h' 

See also: 'setup appliance nmc' 
See also: 'setup appliance nms'

]]></template>
<template module='Util' name='_check_OOM'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Warning! The appliance operates under severe memory pressure.
     $NZA::COMPANY Management Console will now terminate, to free up
     memory for critical system services.

]]></template>
<template module='Util' name='_check_OOM1'><![CDATA[

                          * * *
                      SYSTEM NOTICE

     Warning! The appliance operates under severe memory pressure.
     Remaining free memory is less then ${free_ram_pct}% of the total
     available memory.

]]></template>
<template module='Util' name='show_auto_service_stats_usage'><![CDATA[
$cmdline
Usage: [-i interval]

  -i <interval>	Keep show TCP socket statistics at a specified
                interval in seconds

  -v 		Verbose; in addition to data transfers, include
  		management traffic generated by the service

Show live TCP socket statistics for the specified service$svc.

Use -i option to to show TCP traffic generated by the service at a
specified interval. If the -i option is omitted, the output includes
two 1-second long iterations. Otherwise the command runs until the
traffic stops or Ctrl-C pressed, whatever comes first.

For background, general information, features and differences between
appliance's 2nd tier data services, please see:

${prompt}help data-replication

Example:

${prompt}show auto-sync :vol1-a-000 stats -i 1
TCP CONNECTION                               SNEXT      RNEXT      TRANSFERED
192.168.37.128.39305=>192.168.37.134.22      2176247586 4217818800 -
192.168.37.128.39305=>192.168.37.134.22      2176255378 4217819152 8.14 KB
192.168.37.128.39305=>192.168.37.134.22      2176266306 4217819632 11.41 KB
192.168.37.128.39305=>192.168.37.134.22      2176273922 4217819952 7.94 KB
192.168.37.128.39305=>192.168.37.134.22      2176285106 4217820432 11.66 KB
...


See also: 'dtrace'

See also: 'show performance'

See also: 'show volume iostat'
See also: 'show lun iostat'
See also: 'show volume <name> iostat'

See also: 'help data-replication'

See also: 'run benchmark bonnie-benchmark'
See also: 'run benchmark iperf-benchmark'

See also: 'show usage'
See also: 'help'

]]></template>
<template module='Util' name='print_help_inbox'><![CDATA[

NMC Inbox collects all email notifications sent by appliances
on the network. As such, it can be used as an addition, or replacement,
to the appliance's mailing facility (see 'setup $NMC::MAILER').

To administer local Inbox, run 'setup $NMC::INBOX' and select one
of the available actions. Use '?' for a brief summary information.

If Inbox facility is enabled, messages arrive asynchronously
"behind the scenes": there is no need to perform any NMC
command to actually receive new messages. Each appliance on the
network registered with (or is discovered by) the local appliance
effectively becomes a producer of messages that get then deposited
in the local Inbox.

Inbox can be disabled, enabled, and reset (emptied of all messages).
New messages in the Inbox can be marked as read by running
'setup $NMC::INBOX mark-as-read' command.

Newly arrived messages immediately reflect themselves in the NMC prompt,
in the square brackets [] embedded in the prompt. For instance,
the following shows 3 outstanding messages:

nmc\@thost[3]:/\$

To zero out this counter, simply run 'setup $NMC::INBOX mark-as-read'

To disable unread message counter, run 'option prompt_inbox' and set
the value of this option to zero. Alternatively, run
'setup $NMC::INBOX disable' and follow the prompts.

To view Inbox contents, run 'show $NMC::INBOX'


See also: 'show $NMC::INBOX'
See also: 'show $NMC::MAILER'
See also: 'setup $NMC::MAILER'

See also: 'option'

See also: 'setup usage'
See also: 'help'

]]></template>
<template module='Util' name='print_help_header'><![CDATA[
*****************************************************************
$line1
$line2
*								*
*  press    TAB-TAB to list and complete available options      *
*                                                               *
*  type     help   for help                                     *
*           exit   to exit local NMC, remote NMC, or group mode *
*           q[uit] or Ctrl-C     exit NMC dialogs               *
*           q[uit] or Ctrl-C     exit NMC text viewer           *
*								*
*           option -h            help on NMC options            *
*           <any command> -h     help on any command            *
*           <any command> ?      brief summary                  *
*           help keyword [-f]    locate NMC commands            *
*           help -k [-q]         same as above                  *
*           setup usage		 combined 'setup' man pages	*
*           show usage		 combined 'show' man pages	*
*                                                               *
*  type     help   and press TAB-TAB                            *
*                                                               *
$line3
*                                                               *
*****************************************************************
]]></template>
<template module='Util' name='print_help_getting_started'><![CDATA[

Welcome to $NZA::COMPANY Management Console (NMC). $s

NMC is:
  * a power shell that delivers appliance's multi-tier storage
    services, data replication, fault management, and reporting.

  * a single-point management client - all $NZA::PRODUCT appliances in
    the network can be administered without re-login.
    In fact, "switching" between appliances is instantaneous...

With NMC you can:
  * create, grow, and destroy, export and import storage volumes

  * share (via NFS/CIFS/Rsync/FTP) volumes and folders

  * take snapshots, custom and automated, periodic and on-demand,
    and implement user-defined snapshot retention policies

  * automatically and periodically generate storage and service
    reports

  * run storage services, periodically and on-demand

  * record and non-interactively replay NMC (batch) sessions

  * discover, configure, and create iSCSI sessions, deploy
    iSCSI-attached LUNs,

  * and more.

NMC provides:
  * group execution mode to manage a group of appliances
    as a single entity.

  * enhanced batch execution mode via record/replay mechanism.

  * high-level context-defined command completion mechanism

  * numerous manual pages (a.k.a. usage guides), and summary helps

  * context-defined usage "cues" and hints

  * interactive dialogs with step-wise guided command execution,

  * easy iSCSI Target integration (yes, the appliance can be used
    as a block storage device as well)

  * and more.

NMC makes it all easy. Powerful storage appliance at your fingertips!

---------------

This text is short version of NMC User Guide. For complete NMC
documentation please refer to "$NZA::PRODUCT User Guide" included
with the product.

This document includes a brief introduction, with navigation guidelines
and the most basic conventions is followed by $NZA::COMPANY Storage Appliance
terminology, top-level commands and their subcommands,
fault management and reporting facilities, numerous usage examples,
data replication capabilities, and finally, a collection of tips.

The document is organized as follows:

  ${cb}Table of Contents$cn
  -----------------
  1.  Getting Started _________________  ('help getting-started')
  2.  Navigation ______________________  ('help navigation')
  3.  Terms ___________________________  ('help terms')
  4.  Commands ________________________  ('help commands')
      4.1. Top-Level Commands
         4.1.1. Super-Commands
      4.2. Built-in Unix Commands
      4.3. Aliases
      4.3. Unix Shell
  5.  Options _________________________  ('help options')
  6.  First Steps _____________________  ('help first-steps')
  7.  Runners _________________________  ('help runners')
  8.  Fault Management ________________  ('help fault-management')
  9.  DTrace __________________________  ('help dtrace')
  10. Indexing and Searching Archives _  ('help index')
  11. Advanced Usage __________________  ('help advanced-usage')
  12. Tips ____________________________  ('help tips')
  13. Data Replication ________________  ('help data-replication')
  14. More Help _______________________  ('help more-help')


The right column in the Table of Contents lists NMC shortcuts.
To go directly to a certain section, use the shortcut 'help <section>',
for instance:

${prompt}help getting-started
${prompt}help commands
${prompt}help first-steps
${prompt}help fault-management
etc.

Alternatively, type 'help', press TAB-TAB, and select one of the listed
sections.

${cb}1. Getting Started$cn
=================================================================

  The fastest way to start with NMC is to press TAB-TAB, type a first
  a few letters of a selection (a.k.a. command completion - note that
  the available choices are displayed just below the cursor),
  and then repeat this routine until the command is complete. At which
  point you press Enter.

  This is similar in a way to completing the directory and executable
  names in a regular bash.

  By definition, a command is complete if either one of the following
  is true:

   - TAB-TAB does not produce a new output
   - Command is executable. A visual cue given in this latter case will be:
     $c1<Enter>$cn

  There's an alternative way of entering commands: type a word or two
  and press Enter. (For instance, type 'show' or 'setup' and press Enter).
  The selection is then done in a menu-style: Left and Right arrows
  to browse and Enter to select.

  NOTE:
  -----
        This menu-based way of "completing" NMC commands can be
	disabled by via 'ask_incomplete' NMC option. See Section
	"Options" ('help options') for details.

  Whichever way you use, in the majority of cases NMC will present
  a number of (completion) choices. To quickly find out the meaning of
  all those multiple options, type '?' and press Enter. For instance,
  type 'show appliance', and press TAB-TAB or Enter:

  ${prompt}show appliance

  In response NMC will show a number of options - in this particular case
  appliance's services and facilities that can be "shown". Note that
  $c1<?>$cn is part of the 'show appliance' completion set - its presence
  indicates availability of brief per-option summary descriptions.

  Next:
   - type '?'
   - observe brief descriptions
   - decide which (completion) option to use
   - repeat the sequence, if needed

  See also:
   * Section "Navigation" ('help navigation') for further
     general-usage discussion on how to navigate "through" NMC.

   * Section "First Steps" ('help first-steps') for a sample
     interactive configuration session, with commentary.

   * Section 'More Help" ('help more-help')

]]></template>
<template module='Util' name='print_help_navigation'><![CDATA[
${cb}2. Navigation$cn
=================================================================
  NMC is a management shell, and as such, it provides all the features
  typically expected from command shells:

  * Command history accessible via:
     - 'history'
     - Up and Down arrows at the prompt
     - Ctrl-R to perform reverse search
  * IO redirection, piping and background execution. See Section "Tips"
    ('help tips') for usage tips and examples.

  LESS:
  -----
       NMC employs a popular 'less' utility as its internal Viewer.
       To allow convenient paging, scrolling and searching, NMC runs
       the viewer automatically - if and when when the operation produces
       more than a single screen of an output.

       When in 'less', you will see the following short instruction at
       the bottom of the screen:

          ${crev}Up/Down/PgUp/PgDn - scroll, /- search, q - quit${cn}

       In addition to 'q', you could always use Ctrl-C to terminate
       the viewing mode (as well as any other NMC operation and dialog).

       The viewer supports a multitude of options, allows to
       search NMC operations' result set, save it in a text file,
       and more. Run 'help less' for more information.


  All NMC operations are accompanied with usage information and examples.
  While entering a command, look for $c1<-h>$cn - its presence would mean
  that the usage text for this command is available. For instance, type
  'create volume' and press TAB-TAB.

  TAB-TAB:
  --------
        With NMC you'll never need to do much typing. For instance,
        in the example with 'create volume', you could first type 'cr'
	and press TAB-TAB. NMC will complete it to 'create'. Then
	type 'v' or 'vol', and press TAB-TAB. And so on.

  ${prompt}create volume
  ${c0}show  $c1<Enter> <help|-h>  <?>$cn

  This presents the following completion choices:

  1) ${c0}show$cn	- to show existing volumes
  2) ${c1}Enter$cn	- to enter a dialog, fill out the required parameters
  		  and subsequently execute the command (by pressing Enter)
  3) ${c1}-h$cn		- to get the command's manual page, with synopsis,
		  full description of all commandline options that would
		  allow to execute the command in a batch
		  (non-interactive) mode, and usage examples.
  4) ${c1}?$cn		- few-lines summary information



  NMC keywords
  -------------

  Need to quickly find NMC command but do not remember the exact
  syntax? Run 'help keyword <some-keyword>' to locate and print all
  NMC commands that use the specified keyword, or in fact any substring.
  For more information, see Section "NMC keywords" or run 'help keyword'.

]]></template>
<template module='Util' name='print_help_terms'><![CDATA[
${cb}3. Terms$cn
=================================================================

  ${cb}NMC${cn}		$NZA::COMPANY Management Console. NMC communicates with
  		the local NMS (see next) and remote NMC and NMS
		to execute user requests. Multiple NMC instances
		can be running on a given appliance. NMC is single-
		login management client capable of managing multiple
		appliances and groups of appliances.

  ${cb}SA-API${cn}	Storage Appliance API.

  ${cb}NMS${cn}		$NZA::COMPANY Management Server. There is only one server
  		instance per appliance. The server provides public
		and documented Storage Appliance API (SA-API) available
		to all appliance management and monitoring clients,
		remote and local, including (but not limited to) NMC.

  ${cb}NMV${cn}		$NZA::COMPANY Management View. GUI-based client that uses
  		the same SA-API (above) to communicate with the NMS.
		NMV shows status of all appliances on the network,
  		displays graphical statistics collected by "statistic
		collectors" (see below), and more.

  ${cb}Volume${cn}	$NZA::PRODUCT storage appliance volume utilizes ZFS pool. There
 		is a one-to-one relationship between a volume and an
		underlying ZFS pool. See 'create volume -h' for details.

  ${cb}Folder${cn}	$NZA::PRODUCT storage appliance folder utilizes ZFS filesystem.
 		There is a one-to-one relationship between a folder and
	        an underlying ZFS filesystem. See 'create folder -h'
		for details.

  ${cb}$asn${cn}	A type of the appliance's storage services. The $asn
	  	service enables easy management of snapshots, providing
		regular multiple period scheduling on a per-folder or
		per-volume basis (with or without recursion into nested
		folders/filesystems).
		In addition, $asn allows to define a certain snapshot-
		retention policy. Snapshots can be kept for years, and/or
		generated frequently throughout the day.
		See 'create $asn -h' for details.

  ${cb}$ati${cn}	A type of the appliance's storage services. One of the
  		appliance's primary uses is: digital archiving solution.
		The provided $ati service can regularly and
		incrementally copy data from one host (local or remote,
		appliance or non-appliance) to a destination, local or
		remote, again of any type. $NZA::PRODUCT $ati (or simply,
		tiering) service runs on a variety of transports, and
		can use snapshots as its replication sources. This
		solution fits the more common backup scenarios found in
		disk-to-disk backup solutions. However, unlike regular
		backup solutions with only the latest copy available on
		the backup destination, this solution provides the
		advantage of both "the latest copy" and a configurable
		number of previous copies.
		See 'create $ati -h' for details.

  ${cb}$asy${cn}	A type of the appliance's storage services. The $asy
  		service will maintain a fully synchronized copy of a
		given volume or folder on another $NZA::COMPANY Storage
		Appliance. Where tiering via $ati provides a copy,
  		$asy provides a true mirror, inclusive of all
		snapshots. The major difference between $NZA::PRODUCT
		$ati and $asy services is that the
		latter transfers both data ${cb}and${cn} filesystem metadata
		from its source to its (syncing) destination. This
		allows for standby hosts, as well as image-perfect
		recovery sources for reverse mirroring in case of a
		failure in the primary storage.
		See 'create $asy -h' for details.

   		(*) - Requires the corresponding auto-service plugin.

  ${cb}Trigger${cn}	Fault Triggers, or simply "triggers", are the
  		appliance's primary means of fault management and
		reporting. Each trigger typically runs periodically
		at a scheduled interval and performs a single
		function, or a few related functions.
		Triggers actively monitor appliance's health.
		Run 'show trigger' to list available fault
		triggers and/or outstanding faults (if any).
		For more details see Section "Fault Management", or
		run 'help fault-management'.

  ${cb}Collector${cn} 	Statistic Collectors, or simply "collectors" are,
  		as the name implies, the appliance's means to collect
		network and storage statistics. A large number of
		network and storage IO counters is collected on a
		regular basis and recorded
		into SQL database. The data is then used to generate
		daily reports, and (via NMV - see above)
		various performance/utilization graphs and charts.
		Run 'show collector' to list available statistic
		collectors.

  ${cb}Reporter${cn}	Yet another type of pluggable module tasked
  		to generate periodic reports. Run 'show reporter'
		to list available "reporters", and details.

  ${cb}Indexer${cn}	Indexer is a special runner that exists for a
  		single purpose: to index a specified folder, or
		folders. Once a folder is indexed, it can be
		searched for keywords, and the search itself takes
		almost no time.
		In a way, Indexers provide functionality similar
		to Internet search engines (think "Google").
		However, in addition to searching the most recent
		raw and structured data, Indexer will allow you to
		search back in history - as long as there are snapshots
	       	retained to keep this history.
		For details on $NZA::PRODUCT Indexing facility,
  		including descriptions of features, searchable data
		formats, syntax of search expressions, technical
		characteristics, usage guidelines and examples,
		please see Section "Indexing and Searching $NZA::PRODUCT
		Archives", or run 'help index'.

  ${cb}Runner${cn}	Triggers, Collectors, Reporters, and Indexers - also
   		commonly called "Runners" - are pluggable modules that
		perform specific Fault Management, Performance Monitoring,
   		Reporting, and archive Indexing  tasks.
		All appliance's runners use the same SA-API (see above)
		provided by NMS (see above). The runners can be easily
		added - they are the source of future customizations.

  ${cb}LUN${cn}		Physical and logical drives, attached to the appliance
   		directly or via iSCSI or FC SAN, are commonly called LUNs.
		The terms "LUN", "hard drive" and "disk" are used
		interchangeably.
		See also http://en.wikipedia.org/wiki/Logical_Unit_Number

  ${cb}Zvol${cn}		Emulated (virtual) block device based on a given
   		appliance's volume. Can be used as additional swap
		partition but the primary usage: easy iSCSI integration.
		Zvol is a powerful and flexible tool also because of its
		tight integration with the appliance's storage services.

  ${cb}Plugin${cn} 	$NZA::PRODUCT extension, a pluggable module can be easily
   		added (installed) and removed. Plugin uses the same
		SA-API (see above) as all the rest software components,
   		and implements a certain well-defined (extended)
		functionality. At installation time, plugin integrates
		itself with the appliance's core software.
		Use NMC commands 'show plugin' and 'setup plugin'
		to show installable plugins and administer those already
		installed.


  More Help:
  ----------
        For more information, please refer to $NZA::PRODUCT User Guide
	or run 'help more-help'.
]]></template>
<template module='Util' name='print_help_commands'><![CDATA[
${cb}4. Commands$cn
=================================================================

  4.1. Top-Level Commands
  -----------------------

  NMC supports $number top-level commands; the exact list can
  be obtained at any time by pressing TAB-TAB at a prompt.
  These are:

  $c0@NMC::TOP_LEVEL_CMDS[0..4]$cn
  $c0@NMC::TOP_LEVEL_CMDS[5..$#NMC::TOP_LEVEL_CMDS]$cn

  For each top-level command, to get a complete set of its use cases,
  simply run 'help <command>'. For instance:

  ${prompt}help setup


     4.1.1. Super-Commands
     ---------------------

     Out of total $number, the first two top-level commands are
     somewhat notable: the majority of administrative management
     actions can be performed using ${c0}setup$cn and ${c0}show$cn.

     NOTE:
     -----
           The appliance is sub-divided into components -
           managed services and objects - which in turn consist of
           sub-components, and so on. The taxonomy, that is, the
	   entire management hierarchy reveals itself via 'setup'
	   and 'show' super-commands.

     As an example, type 'setup' and press TAB-TAB or Enter.
     The menu of choices will include 'appliance', 'network', 'group',
     etc. From this point on you can traverse the tree-like
     appliance's management hierarchy. For a context-sensitive
     summary, type or select '?' at any step of the way.
     See Section "Navigation" (or run 'help navigation') for
     more details.

     ------------------------------

  A brief summary:

  * ${c0}show$cn	- display any any given object, setting
  		  and status.
    For a quick list of Appliance's objects, run 'show' and press
    TAB-TAB.

    For complete assembly of 'show' man pages, run:

    ${prompt}show usage

    The latter will list all 'show' subcommands, with their
    corresponding manual pages.

    NOTE:
    -----
          The NMC 'show' operation is used to display a given object(s),
          service(s), system facilities. To quickly locate and display
	  appliance's objects using arbitrary complex search
	  criteria, NMC provides a powerful 'query' command.
	  See the 'query' manual page ('query -h') for more information.

  * ${c0}setup$cn	- create or destroy any given object,
  		  and modify any given setting.

    Run 'setup' to set a property, enable, disable, share, unshare,
    import, export, create, destroy, run a service, grow a volume -
    generally, to perform any configuration management operation.

    For the complete collection of 'setup' man pages, run:

    ${prompt}setup usage
    This will list all 'setup' subcommands, with their corresponding
    usage pages and examples.

  * ${c0}query$cn	- advanced query and selection operations

    Run 'query -h' or 'query usage' for an overview, usage, and examples.

  * ${c0}switch$cn	- manage another $NZA::COMPANY Appliance or a group
  		  of appliances.

    The 'switch' command has two variations:
    - 'switch group'	- switch to a group, or in other words, enter
    			  a "group mode" to perform operations on a
			  a given group of appliances

    - 'switch appliance'- switch to a remote appliance. All NMC operations
    			  including interactive ones, will be communicated
			  to a remote appliance, and the results will be
			  transferred back.

    See also: 'fastswitch' below, in subsection "Aliases".

  * ${c0}destroy$cn	- destroy any given object.

    For the complete collection of 'destroy' man pages, run:

    ${prompt}destroy usage
    This will list all 'destroy' subcommands, with their corresponding
    manual pages.

  * ${c0}create$cn	- create any given object: volume, folder,
  		  snapshot, storage service.

    For a quick list of objects that can be created, type 'create'
    and press TAB-TAB

    For the full list of 'create' man pages, run:

    ${prompt}create usage
    This will list all 'create' subcommands, with their corresponding
    manual pages.

  * ${c0}run$cn		- execute any given runnable object, including
  		  storage services: @NZA::AUTO_SERVICES

    The command facilitates on-demand execution of scheduled
    periodic services and sets of services.

    For an overview and usage of any one of the listed services, run
    'run <service-name> -h'

    Complete list of 'run' manual pages is available via:

    ${prompt}run usage
    This will list all 'run' subcommands, with their corresponding
    manual pages.

  * ${c0}share$cn	- share (via NFS, CIFS, FTP) a volume or a folder

    For complete assembly of 'share' man pages, share:

    ${prompt}share usage
    This will list all 'share' subcommands, with their corresponding
    manual pages.

    Use 'show share' command to display ALL shares, including nfs,
    cifs, ftp, and rsync. For instance:

     ${prompt}show share
     FOLDER                   CIFS  NFS   RSYNC FTP
     vol1                     -     -     Yes   -
     vol1/a                   -     -     -     Yes
     vol1/b                   -     Yes   -     -
     vol1/c                   -     -     Yes   -


  * ${c0}unshare$cn	- unshare a volume or a folder

    Complete list of 'unshare' manual pages is available via:

    ${prompt}unshare usage
    This will list all 'unshare' subcommands, with their corresponding
    manual pages.

  * ${c0}record$cn	- Start and stop NMC recording sessions.

    With NMC, management operations often require specification of
    multiple options. A combination of NMC recording facility and
    NMC multiple-selection and other dialogs gives the best of both:

      1) interactive (guided) filling-out of the required parameters
   and
      2) ability to execute the "recorded" command many times,
         on many different $NZA::PRODUCT appliances.

    For more information, see "Batch mode" in Section "Tips",
    or run 'help tips'.

  4.2. Built-in Unix Commands
  ---------------------------

  In addition, NMC also provides a few more built-in commands and
  the absolutely essential Unix commands.

  The extra built-ins are: @rest_builtins

  For instance, run 'df [folder-name]' to display volumes and folder,
  or 'df -h' for usage.

  The list of available available Unix depdends on the option
  'expert_mode' value (see Section "Options" ('help options')).
  and contains:

  *
  * $text0
  * $text1
  * $text2
  * $text3
  * $text4

  For instance: run 'find' or 'grep' to search files and directories,
		    'edit' or 'less' to edit/view files, and so on.

  ===================

  Tip:  Unix commands can executed without leaving the NMC -
        simply use '!' ("bang") in front of the executable.
        Examples:

        ${prompt}!zfs list    - list ZFS filesystems on the appliance
                                by directly executing ZFS command
        ${prompt}!ls /tmp     - list contents of /tmp
        ${prompt}!tar --help  - display tar options

  NOTE:
  -----
        NMC supports a so called "expert mode" operation, one of the
	differences between "experts" and "non-experts" being that
	the former are permitted to execute more Unix commands.
	To enable expert mode, run:

        ${prompt}option expert_mode=1

        For more information on NMC options, see Section "Options"
	('help options').
        For the complete and most updated list of NMC commands,
	including Unix commands, simply press TAB-TAB at NMC prompt.


  4.3. Aliases
  ------------

   Usage: <alias>[=<new_value>] [-s]

  -s       Store the new value, update NMC configuration file.

  Aliases and the commands they alias can be used interchangeably.
  Run 'alias' without parameters to list the current aliases.

  Use -s (store) option, to store user-defined aliases persistently;
  alternatively you could use 'setup appliance nmc edit-settings'
  command edit the entire set of NMC options, aliases including.

  With no arguments, prints out a list of the current aliases.
  With only the <alias-name> argument, prints out a definition of the
  alias with that name.

  * Use 'alias' and 'unalias' commands to view, set, and unset
    aliases at runtime

  * To make the settings persistent, use either -s option
    or edit the NMC configuration file via:
    ${prompt}setup appliance nmc edit-settings

  * Built-in aliases are:

	alias quit=exit
	alias options=option
	alias fastswitch='switch appliance -f'

  * The following are examples of multi-word aliases:

${prompt}alias 'setup checkpoint'='setup appliance checkpoint'
${prompt}alias "show remote appliance" = "show network appliance"

  Aliases and the commands they alias can be used interchangeably.
  Run 'alias' without parameters to list the current aliases.
  Make sure to use 'edit-settings' sub-command (above) to
  store the user-defined aliases persistently and use them
  for the current and future NMC sessions.


  4.4. Unix Shell
  ---------------

  Finally, in addition to all the listed Top-Level NMC commands,
  built-in Unix commands and NMC aiases, there is a full-fledged
  ("raw") ${cs}shell$cn access.

  CAUTION: $NZA::PRODUCT appliance is not a general purpose operating
  system: managing the appliance via ${cs}Unix shell$cn is NOT recommended.
  This management console (NMC) is the command-line interface (CLI)
  of the appliance, specifically designed for all command-line
  interactions.

  Using ${cs}Unix shell$cn without authorization of your support provider
  may not be supported and MAY VOID your license agreement.
  To display the agreement, please use:

${prompt}show appliance license agreement

]]></template>
<template module='Util' name='print_help_options'><![CDATA[
${cb}5. Options$cn
=================================================================

  NMC can be used universally to view and configure every single aspect
  of the appliance: volumes and folders, storage and network services,
  fault triggers and statistic collectors.

  For the current NMC options, run:
  ${prompt}option

  To display the 'option' man page and the list of available options
  and their values, run:
  ${prompt}option -h

  To change any value temporarily, run 'option <name>=<value>, for
  instance:
  ${prompt}option bg=dark
  or
  ${prompt}option alt_editor=joe

  To make persistent change, run 'option <name>=<value> -s, for
  instance:
  ${prompt}option bg=dark -s

  Alternatively, run:
  ${prompt}setup appliance nmc

  This will open a configuration file, with the NMC options and embedded
  (per-option) documentation.

  See also: Section "Tips" below (or run 'help tips')

  ===================

  To configure $NZA::COMPANY Management Server (NMS):
  ${prompt}setup appliance nms

  Note that changes to the server are somewhat global because
  the server is shared by all clients, which, in addition to possibly
  multiple console based clients (a.k.a. NMCs), includes:
     * storage services
     * fault triggers
     * reporters
     * collectors

  and certainly,
     * GUI-based management.

  All those clients listed above access the same universal NMS-provided
  public API described elsewhere (see Section "More Help"
  (or run 'help more-help') for additional sources of information)

]]></template>
<template module='Util' name='print_help_first_steps'><![CDATA[
${cb}6. First Steps$cn
=================================================================

  Following is a sample NMC session that will:

  1) create a volume
  2) create folders
  3) share a folder
  4) setup a local tiering service

  The operations listed below are accompanied by many examples, and
  generally, heavily commented. Needless to say, the comments are
  generally applicable to all other NMC operations..

  1) Creating a volume
     -----------------

     To create a new $NZA::COMPANY Appliance volume, you need spare drives
     a.k.a. LUNs.

     Run:
     ${prompt}show lun

     to see all LUNs, allocated and unallocated.

     Note: Detailed manual page for this command (as well as all the
           rest NMC commands) is available via -h option:
           ${prompt}show lun -h

     If there are no spare LUNs, you could still go through the steps
     below, but a volume obviously won't be created..

     Run:
     ${prompt}create volume
     or, the same:
     ${prompt}setup volume create

     and press TAB-TAB. This former will display a list of
     completion options explained in detail in Section "Navigation"
     ('help navigation'). Notice the ${c1}-h$cn and ${c1}help$cn
     options - their presence indicates the availability of extended
     usage information. Thus, for the usage run:

     ${prompt}create volume -h

     One of the supported options allows to try (or "dry-run")
     volume creation without actually creating the volume and
     making any changes. You could try this option to play with various
     redundancy configurations: mirror, raid, etc.

     Run:
     ${prompt}create volume

     and press Enter. NMC will guide you through the interactive process
     of creating a new volume. At any step of the way you can break
     without making any changes by pressing Ctrl-C. Note helpful hints
     at the bottom of all dialogs.

     To create a volume non-interactively, supply the volume name and
     configuration in the command line, e.g.:
     ${prompt}setup volume create vol1 mirror c2t0d0 c2t0d0 -y

     This will create a two-way mirror 'vol1' based on two SCSI drives.

     If unsatisfied, you can destroy the newly created volume and repeat
     the steps above. To destroy the volume, run:

     ${prompt}destroy volume

     or even, skipping the confirmation dialogs:

     ${prompt}destroy volume vol1 -y   (that is, assuming the new volume
					name is 'vol1')

  2) Creating folders
     ----------------

     This and the following subsections assumes that we have a volume 'vol1'
     in the system.

     ${prompt}create folder vol1/a/b

     This will create a folder vol1/a and its subfolder vol1/a/b - in one
     shot. See 'create folder -h' for detailed discussion and examples.

     Obviously, the volume and folder names are given just as an example,
     here and throughout the rest of this section.

     Use:
     ${prompt}setup volume vol1 folder a
     ${prompt}show volume vol1 folder a/b
     etc.  - to view and change folder properties.

     NOTE:
     -----
           Most NMC operations that deal with folders and volumes
	   can be typed in a shorter form, by omitting 'volume'
	   and 'folder' keywords.

	   For instance:

           ${prompt}setup volume vol1 folder a
	   is equivalent to:
           ${prompt}setup vol1/a

	   and:
           ${prompt}show volume vol1 folder a snapshot
	   is the same as:
           ${prompt}show vol1/a snapshot
	   and the latter in turn is the same as:
	   ${prompt}show snapshot vol1/a

	   For more information, please refer to Section "Tips",
    	   subsection "NMC shortcuts and aliases"


  3) Sharing a folder via NFS
     ------------------------

     First, make sure nfs is up and running:

     ${prompt}show network service nfs-server

     Notice that 'show network service' and 'setup network service'
     can be used to display and configure network services, as the
     names imply..

     Type:
     ${prompt}share
     and press TAB-TAB or Enter

     The completion choices will include 'folder', 'usage', 'volume',
     and 'vol1/' - that is, assuming 'vol1' is the only volume
     in the system.

     Let's now share the entire volume 'vol1':

     ${prompt}setup volume vol1 share nfs -o rw=primary,root=primary

     This will share the entire 'vol1', with all its subfolders.
     More exactly, by default the subfolders inherit the "sharing"
     property from their parent. It is possibly to override this
     default behavior - see 'setup volume vol1 share nfs -h'

     Alternatively, since 'share' is one of the top-level commands
     (see Section 'Commands' or run 'help commands'), you could use
     a shorter variant:

     ${prompt}share vol1/ nfs -o rw=primary,root=primary

     Here we are also using the fact that 'volume' and 'folder'
     keywords can be omitted, as explained in the previous subsection.

     Finally, you could create a share interactively, by running:

     ${prompt}share vol1/ nfs

     NMC will ask you to specify read-write access and other share
     qualifiers, and will guide you through the process.

     NOTE:
     -----
	   It is important to note that all shares for NFS as relative
	   to $NZA::VOLROOT, or whatever the 'volroot' property is set to
	   (to see the current settings, run 'show appliance nms config',
	    and/or refer to Section "Options", and/or run 'help options').

	So, the above would be mounted as server:/$NZA::VOLROOT/vol1
	if our hostname was server.

     NOTE:
     -----
   	Unless you are an experienced NFS user or system administrator,
   	here are a few of guidelines that you may find useful.

   	1) Make sure that domain name on the client and the server are the
      	   same. Different domain names is one of common reason to getting
      	   'Permission denied' when accessing a shared location
      	   from NFS client. Run 'show appliance domainname' to find out.

        2) If the client and server domain names are not matching
           (and cannot be changed to match),
           try using 'anon=0' setting via Extra-Options.


        3) Do not leave the 'Root' field empty. The safest option is to
           explicitly set Root to client's hostname. Note: not the client's
           IP address but the hostname in the form <host.domain> that
           can be resolved.
           Use colon (':') delimiter to specify more that one client host
	   and/or netgroup.


     NOTE:
     -----
        $NZA::PRODUCT uses NFSv4 by default. To force the server to use
	NFSv3, set NFS_SERVER_MAX variable to 3 in the server
	configuration file. Edit the server configuration via
        'setup network service nfs-server edit-settings' command.

        This is particularly useful if _only_ NFSv3 clients are used
        with the $NZA::PRODUCT NFS server.
        If you have a mix of v3 and v4 clients, to force NFSv4 clients
        to use v3, mount using the following syntax:
        server:/share /localmount bg,intr,vers=3,noacl


     See 'show <name> share nfs -h' for more details.

     You could display existing shares via 'show', for instance:

     ${prompt}show vol1/ share nfs -v

     This will display all 'vol1' nfs shares recursively, including
     subfolders. See the corresponding man page for details:

     ${prompt}show vol1/ share nfs -h

  4) Setting up a tiering service
     ----------------------------

     The appliance supports a number of storage services, including
     tiering service a.k.a. $NZA::AUTO_TIER - for the basics and
     terminology please refer to Section "Terms", or run 'help terms'.

     In large arrays where the appliance encompasses both first tier
     and second tier storage, you'll often see local-to-local tiering.
     Tiering is accomplished by taking a given filesystem or share,
     breaking into smaller manageable chunks, and replicating that data
     at that point in time to another volume. Using snapshots at the
     target end, one can maintain a full efficient backup of the primary
     storage at unique intervals typical of backups.

     A simple example of tiering data from an NFS file server to
     our example volume would be to first create a filesystem to tier to,
     such as:

     ${prompt}create folder vol1/cad

     and then to setup an $NZA::AUTO_TIER from our source NFS server
     (for our CAD tools in this case):

     ${prompt}create $NZA::AUTO_TIER -s rsync+nfs://hd/cad/* -d /vol1/cad -r 1 -T 3am -i daily -p 1.

     NOTE:
     -----
           As always, the options are documented and available via -h:

           ${prompt}create $NZA::AUTO_TIER -h

	   The corresponding man page provides extensive explanations
	   and more tiering examples.

           Irrespectively of the man pages, it is fair to assume that the
	   first-time and maybe even second-time users will
           never want to type a complete command in its entirety...
	   As always, for the an interactive, dialog-driven way, run:

           ${prompt}create $NZA::AUTO_TIER

	   And follow the prompts.
     End of note.

     The lengthy $NZA::AUTO_TIER creation command above will pull data
     via NFS, breaking up the synchronization into chunks determined
     by the number of subdirectories found in cad. The -r flag determines
     the level of recursiveness, the -T details the time of day, etc.

     The $NZA::AUTO_TIER service is not limited just to the first two
     tiers, as tertiary tiering for more critical data is also common.
     As legal and business drivers dictate, tiering will also include
     access policy enforcement, limiting data access to restricted
     personnel to over longer periods of time. The service addresses
     Disaster Recovery, Continuous Data Protection, and other emerging
     technology solutions as every aspect of business goes digital,
     and must be protected, preserved, or potentially expired as required.

]]></template>
<template module='Util' name='print_help_runners'><![CDATA[
${cb}7. Runners$cn
=================================================================

  The appliance comes with a number of standard "runners" - pluggable
  modules that perform essential Fault Management, Performance
  Monitoring and Reporting, and Indexing functions.

  The runners include Fault Triggers (or simply Triggers),
  Statistics Collectors, Reporters and Indexers.
  All runners rely on the $NZA::COMPANY Management Server to provide Storage
  Appliance API (SA-API), the API that is also used by management
  console and GUI clients.

  Typically, each runner is a simple module that performs a single
  identifiable task. For instance:

  * fault trigger 'memory-check' monitors free memory in the appliance
  * collector 'nfs-collector' collects and records in the database
    NFS statistics
  * reporters 'volume-reporter' and 'network-reporter' generate
    periodic network and storage utilization reports, respectively,
    and so on.

  These are just a few examples. To show the existing runners, run:

  ${prompt}show appliance runners
  or, for details:
  ${prompt}show appliance runners -h

  The appliance's framework allows to quickly deploy additional
  "runners" that have the ability to exercise the entire NMS-provided
  SA-API (Section "Terms"), execute periodically, and/or on event,
  and/or run constantly in the background.

  In addition, you can quickly add your custom "runners" to execute
  peridocially and on condition (with a powerful and flexible condition
  evaluation mechanism). The corresponding NMC operation:

  ${prompt}create $NZA::RUNNER_SCRIPT

  Specification, details and examples available via:

  ${prompt}create $NZA::RUNNER_SCRIPT -h

  Runners rely on the appliance's mailing facility to communicate
  with the appliance's administrator(s) - do not forget to configure
  the mailing:

  ${prompt}setup appliance mailer
  or, for details, see:
  ${prompt}setup appliance mailer -h

  All appliance's runners are runtime-configurable. Runners' times-to-run
  and other properties can be changed via:

  ${prompt}setup trigger
  ${prompt}setup collector
  ${prompt}setup reporter
  ${prompt}setup indexer
  ${prompt}setup $NZA::RUNNER_SCRIPT

  As always, each of the 'setup' commands listed above has its 'show'
  counterpart, to show the existing configuration and runtime status:

  ${prompt}show trigger
  ${prompt}show collector
  ${prompt}show reporter
  ${prompt}show indexer
  ${prompt}show $NZA::RUNNER_SCRIPT

  Example #1:

  ${prompt}setup trigger cpu-utilization

  This can be used to disable, enable, run, and configure the standard
  fault trigger that monitors CPU utilization. For instance, press TAB-TAB
  or Enter, type or select 'property', and view all 'cpu-utilization'
  properties available for tuning. You could change the alarm-generating
  thresholds (in this case - low and critically low idle CPU), make it
  run more or less frequent, etc.

  Example #2:

  ${prompt}show trigger cpu-utilization -v

  This will show the trigger's current runime state, status
  and existing configuration in detail (notice the verbose -v option).


  For more details on Fault Triggers, please see Section
  "Fault Management", or run 'help fault-management'.

  For more details on the appliance's indexing facility, please see
  Section "Indexing and Searching $NZA::PRODUCT Archives",
  or run 'help index'.

]]></template>
<template module='Util' name='print_help_fm'><![CDATA[
${cb}8. Fault Management$cn
=================================================================

  Storage appliance is absolutely required to provide a reliable
  24/7/365 service. To that end, $NZA::PRODUCT appliance provides a rich
  set of fault management and HA features. Part of the fault management
  facility is realized through Fault Triggers. A fault trigger, or
  simply, a trigger, is a special kind of a pluggable runner module
  ('help runners') that performs a certain fault management and
  monitoring operation(s). Each trigger monitors one, or a few related
  conditions:

  * memory
  * CPU utilization
  * state and status of appliance's hardware, including
    physical disks
  * state and status of appliance's services and facilities,
    including all storage services
  * state and status of other runners.
  * state and operational status of the $NZA::COMPANY Management Server.

  If any of the monitored conditions violates, a fault trigger raises
  an alarm, which manifests itself in several ways:

  1) email notification to the administrator, with detailed description
     of the fault, including: severity, time, scope, suggested
     troubleshooting action, and often an excerpt of a related log
     with details.

  2) one of the following NMC 'show' operations:

     a) 'show faults'
     b) 'show trigger <name>'
     c) 'show trigger all-faults'
     d) 'show appliance runners'
     e) 'show faults all-appliances'

  NOTE:
  -----
        The 'show faults' command produces a comprehensive fault
	management report, "covering" all system services and sub-systems
	(including storage and network sub-systems).

  NOTE:
  -----
        Notifications of hardware faults are *immediate*. Unlike
	many other potentially faulty conditions that are getting
	periodically "polled", any hardware fault itself triggers
	the appliance's fault management logic, that in turn
	includes email notification.

  In addition to local fault management operation each
  appliance monitors ("keeps-alive") other explicitly
  ssh-bound and/or dynamically discovered appliances on the network.

	  ASIDE:
	  ------
	           To show all appliances on the network, run:
		   ${prompt}show network appliance
		   The list is updated in real time.
		   For more details, see:
		   ${prompt}show network appliance -h

  This particular keep-alive function is carried out by a special
  fault trigger called 'hosts-check'. Along with all the rest fault
  triggers and other pluggable modules (see Section 'Runners'),
  hosts-check is runtime-configurable and can be tuned for a particular
  environment (unless the defaults are good - which is expected):

  ${prompt}setup trigger hosts-check

  To see all available fault triggers, run:
  ${prompt}show trigger all
  or, simply run 'show trigger' and select one of the available
  options.

  NOTE:
  -----
        ${prompt}show trigger all-faults
	or, same:
	${prompt}show faults

	This will display all currently outstanding alarms (if any)
	along with their descriptions, criticality, and timestamps.
	In addition, NMC will check whether any important system
	facilities are not operational, and include this information
	in the report.

  ------

  In all cases a trigger that "carries" the alarm will be shown
  in ${cr}red${cn}, assuming NMC colors are enabled.

  In addition, the faulted trigger will try to notify system
  administrator via appliance's mailing facility. Therefore, as
  already noted elsewhere, it is important to setup the appliance's
  mailer:

  ${prompt}setup appliance mailer

  Trigger counts the fault conditions every time it runs. Typically,
  the trigger will send email once the faulty condition is observed
  a certain configurable number of times. Typically, after that the
  trigger itself goes into 'maintenance' state - it will still run
  and count the faulty conditions but it will not send email
  notification anymore - that is, until system administrator clears
  it from its maintenance state:

  ${prompt}setup trigger <name> clear-errors

  Similar to the rest appliance's runners, triggers are flexible,
  in terms of their runtime behavior and trigger-specific
  conditions they monitor. For details on any specific fault
  trigger, run:

  ${prompt}show trigger <name> -v

  where <name> stands for the trigger's name.

  NOTE:
  -----
       The appliance includes one special fault trigger - 'nms-check'.
       This trigger performs fault management/monitoring function
       for the Fault Management facility itself.
       Nms-check tracks NMS connectivity failures and internal errors.

       ${prompt}show trigger nms-check -v

       In presence of network failures, this will show all alarms
       (in detail) that the appliance failed to report.

  NOTE:
  -----
	${prompt}show faults all-appliances

	This generates Fault Management summary report that includes all
	known (explicitly ssh-bound and dynamically discovered)
	$NZA::PRODUCT appliances.

	Upon generating the summary, use a combination of NMC 'switch'
	operation and 'show faults' -
	to "zoom-in" into a particular ("faulted") appliance for details.

]]></template>
<template module='Util' name='print_help_dtrace'><![CDATA[
${cb}9. DTrace$cn
=================================================================
*								*
*               System DTrace based toolkit                     *
*                                                               *
=================================================================
DTrace is a comprehensive dynamic tracing framework created by
Sun Microsystems for performance profiling and troubleshooting
problems on production systems in real time.

DTrace can be used to generate performance profiles and analyze
performance bottlenecks.

DTrace can helps to troubleshoot problems, by providing detailed views
of the system internals.

DTrace is integrated in $NZA::COMPANY Management Console as one of its
top level commands ('help commands').

To start using DTrace, type 'dtrace' at NMC prompt, and use TAB-TAB
to navigate, or simply press Enter and make a selection.

See help (-h) for usage. In most cases examples are provided; to see
an example, select 'example' option. For instance:

   ${prompt}dtrace cpu cpuwalk example

To override the default behavior of any given dtrace utility,
specify extra options in the command line, for instance:

   ${prompt}dtrace cpu cpuwalk 5

   This will run for 5 seconds (as opposed to default:
   to run until Ctrl-C is pressed).

For details on particular command line options use help (-h),
for instance:

   ${prompt}dtrace cpu cpuwalk -h

]]></template>
<template module='Util' name='print_help_index'><![CDATA[
${cb}10. Indexing and Searching Archives$cn
=================================================================

  Indexer is a special kind of a "runner" (see Section "Runners", or
  run 'help runners') that can be be dynamically created and destroyed.
  To create an indexer means to associate it with a storage folder
  (for the subsequent indexing of this folder snapshots and the most
  recent content). Symmetrically, to destroy an indexer means to
  disassociate it from the corresponding folder, and in addition,
  to remove from storage all the corresponding indexing data.


  Once a folder and possibly some/all of its snapshots get indexed,
  the corresponding data can be searched. Indexing a large amount
  of storage takes time, which is why indexer works offline, according
  to the specified schedule. However, the search on an indexed folder
  is almost immediate. Search results integrated both into NMV GUI
  and management console (although $NZA::COMPANY Management View is definitely
  a preferable way to view the results).

  $NZA::PRODUCT Indexing facility features:

	* Supports Unicode (including code-points beyond the BMP),
	  and stores indexed data in UTF-8

	* Ranked probabilistic search - important words get
	  more weight than unimportant words, so the most relevant
	  documents are more likely to come near the top of the
	  results list.

	* Relevance feedback - given one or more documents,
	  Indexer can suggest the most relevant index terms to expand
	  a query, suggest related documents, categorize documents, etc.

	* Phrase and proximity searching - users can search for
	  words occurring in an exact phrase or within a specified number
	  of words, either in a specified order, or in any order.

	* Full range of structured boolean search operators
	  (for instance: "stock NOT market", "title AND virtualization",
	   "NexentaOS OR $NZA::PRODUCT", etc.).
	  The results of the boolean search are ranked by the
	  probabilistic weights. Boolean filters can also be applied
	  to restrict a probabilistic search.

	* Stemming of search terms (e.g., a search for "football" would
	  match documents which mention "footballs" or "footballer").
	  This helps to find relevant documents which might otherwise
	  be missed.
	  Stemmers are currently included for Danish, Dutch, English,
	  Finnish, French, German, Hungarian, Italian, Norwegian,
	  Portuguese, Romanian, Russian, Spanish, Swedish, and Turkish.

	* Wildcard search is supported (e.g. "nexen*").

	* Synonyms are supported, both explicitly (e.g. "~cash")
	  and as an automatic form of query expansion.

	* Suggest spelling corrections for user supplied queries.
	  This is based on words which occur in the data being indexed,
	  and will work even if a word cannot be be found in a dictionary
	  (e.g., "nexenta" would be suggested as a correct for "nexnenta").

	* Supports database files > 2GB - essential for scaling to large
	  document collections

	* Allows simultaneous update and searching. New documents become
	  searchable right away.


  $NZA::PRODUCT Indexing facility can index the following "raw" data:

	* HTML
	* PHP
	* PDF
	* OpenOffice/StarOffice
	* Microsoft Word/Excel/Powerpoint/Works
	* Word Perfect
	* AbiWord
	* RTF
	* Perl POD documentation
	* and certainly, plain text.


  $NZA::PRODUCT Indexing facility can index the following "structured"
  data:
	* MySQL
	* PostgreSQL
	* SQLite
	* Oracle
	* DB2
	* MS SQL
	* ODBC
	* LDAP


  Here is how indexer works. Each time it runs, the indexer
  "notices" new snapshots, that is snapshots created since its
  last run on the given folder. The indexer then proceeds to
  indexed those new snapshots into space-optimized index database.
  Snapshots are indexed only once. Every scheduled pass may
  index up to a configurable number snapshots. If an indexed
  folder gets destroyed, the corresponding index get destroyed
  as well. If a snapshot get destroyed, the corresponding part of
  the index database is cleaned up too.

  Note that a folder without any snapshots can be indexed as well.
  The feature is called "continuous indexing" and is enabled by
  default.

  The rest of the description is done in a form of questions
  and answers.


  Question: How much space will index take?

  Answer:
  The estimate is: it usually takes from 0.001% to 0.1% of
  the size of the original source. Note that the indexing
  database is stored in a $NZA::PRODUCT folder with enabled
  compression, which should provide for even better space
  utilization.

  Question: Where the indexing database is located?

  Answer: Indexing database root is global and configurable
  through NMS option 'indexroot'. The default value is:
  syspool/.index


  Question: How can I manage (enable, disable, tuneup)
  an individual index in NMC?

  Answer:
  ${prompt}setup indexer <name> [enable|disable|property] ...


  Question: Any guidelines to make index run faster?

  Answer: Use option "recursion_limit". It will limit the
  depth of source scanning. Default is 0 (unlimited).
  For instance, set it to 3 and see if it helps.

  In addition, limit a number of snapshots to index during
  a single indexer run - the corresponding option is called
  "snaps_per_schedule". The default is value 3, that is,
  by default the indexer will work on up to 3 new snapshots,
  when scheduled.


]]></template>
<template module='Util' name='print_help_advanced'><![CDATA[
${cb}11. Advanced Usage$cn
=================================================================

  The topics in this section include:
    1) managing remote appliance
    2) managing a group of appliances
    3) recording and replaying NMC sessions
    4) adding fault triggers and statistic collectors
    5) saving/restoring appliance's configuration
    6) upgrading appliance software
    7) using DTrace

  More Help:
  ----------
        For more information, please refer to $NZA::PRODUCT User Guide
	or run 'help more-help'.
]]></template>
<template module='Util' name='print_help_tips'><![CDATA[
${cb}12. Tips$cn
=================================================================

* ${c1}Colors$cn
  ------------
  NMC currently supports the following screen backgrounds:
  none 		- none (no colors)
  1 or white	- white background (default)
  2 or grey	- grey background
  3 or dark	- dark or black background

  If your screen background is dark, run:
  ${prompt}option bg=dark

  To make the pallete change permanent:

  ${prompt}setup appliance nmc

  This will open NMC configuration file in the internal NMC editor
  (see next).

* ${c1}NMC man pages and help$cn
  ------------------
  The NMC manual pages (a.k.a. command usage guides) are universally
  available via -h command line option.

  Typing '?' in the command line typically produces a short summary
  on the available option.

  The system manual pages are also available - simply type
  'help <option>'
  For instance, run 'help zfs' to display ZFS manual page.

  For more information and examples, see Section "Navigation"
  ('help navigation').

* ${c1}VIM$cn
  ---------
  VIM is a default NMC text editor -
  	see http://en.wikipedia.org/wiki/Vim_(text_editor)

  VIM supports 3 modes of operations:
     - browsing/scrolling  (Up, Down, PgUp, PgDn, Home, End)
     - editing ('i', and ESC to get back to browsing)
     - command mode (':', and ESC to exit, 'w' to save changes,
		     'q' to quit)

  In addition to VIM, the appliance provides an alternative text editor
  called 'joe' a.k.a. joe-editor. You can enable its usage instead of the
  default via NMC' option 'alt_editor'. See Section "Options"
  ('help options') for details.

  As far as editing configuration files, both VIM and 'joe' are temporary
  (fallback) solutions.

* ${c1}Quitting, exiting, aborting$cn
  press Ctrl-C to abort any command, terminate any dialog.

  type 'exit' or 'quit' to exit remote NMC (e.g., back to local NMC),
  local NMC, or a group execution mode.

  type 'q' to quit this help.

* ${c1}Group mode$cn
  --------------
  for group mode, see 'setup group'

* ${c1}Remote appliance management$cn
  -------------------------------
  for remote appliance management, see 'switch appliance'

* ${c1}show all$cn
  ------------
  to show "all" appliance in one shot, run:

  ${prompt}show all

  In general, if a command produces a lengthy output, it makes sense
  to redirect the output into a file or pipe it into a viewer, such
  as 'more', 'less', etc.. For NMC the latter is the default behavior.
  See a note on 'less' in Section "Navigation" ('help navigation').

* ${c1}less$cn
  ----------
  Once in less, use cursor keys to browse, '/' to search forward,
  '?' to search backward, 'q' to quit.  Press 'h' for help and more
  options, or quit 'less' (with 'q') and run 'help less'.

* ${c1}usage$cn
  -----------
  There are two ways to get manual pages, usage examples, and general
  information on any one of the following commands:

  $c0@NMC::TOP_LEVEL_CMDS[0..4]$cn
  $c0@NMC::TOP_LEVEL_CMDS[5..$#NMC::TOP_LEVEL_CMDS]$cn

  These are:

  1) 'help <command>'
  2) <command> usage

  For instance, to list the entire collection of 'show' sub-commands
  with their respective usage & manual pages, run:

  ${prompt}show usage
  or, same:
  ${prompt}help show

* ${c1}NMC shortcuts and aliases$cn
  -----------------------------
  NMC is a power shell providing many ways to speed up manual
  operations - and make it easy. This includes various shortcuts
  and aliases, namely:

  1) All NMC operations that deal with folders and volumes
     can be typed in a shorter form, by omitting 'volume'
     and 'folder' keywords.  For instance, the following two
     'setup' operations are identical:

     ${prompt}setup volume vol1 folder a
     ${prompt}setup vol1/a

     For more examples please refer to Section "First Steps".

  2) The following are two identical ways to run a given service
     instance _now_:

     ${prompt}run auto-tier :vol1-a-000
     and
     ${prompt}setup auto-tier :vol1-a-000 run

     By design, setup is a super-command (see Section "Commands")
     that can be used to modify, create, restore, run, and preform
     other "active" management operations on the appliance. However,
     a number of the most important "actions" are defined explicitly,
     and provided largely for convenience.
     The set includes:

     ${c0}create destroy run share unshare$cn

  3) Aliasing

     See section "Commands" ('help commands'), subsection "Aliases".

* ${c1}Clearing screen$cn
  -------------------
  to clear the screen, press Ctrl-l

* ${c1}NMC options$cn
  ----------------
  to view NMC tunables and settings:

  ${prompt}option

  to enter NMC's expert mode:

  ${prompt}option expert_mode=1

  Alternatively, run
  ${prompt}setup appliance nmc

  and simply uncomment the corresponding line in the
  NMC configuration file and save the file. NMC will re-read the
  modified options automatically.

* ${c1}Management Server configuration$cn
  ------------------
  to list and modify $NZA::COMPANY Management Server (NMS) settings:

  ${prompt}setup appliance nms property

* ${c1}Repeating$cn
  -------------
  to quickly repeat a command that was already executed,
  press Ctrl-R and type a few first letters (of the command).

  Or, simply press Up and Down arrows at the prompt.


* Need to quickly find NMC command but do not remember the exact
  syntax? Run 'help keyword <some-keyword>' to locate and print all
  NMC commands that use the specified keyword.
  For more information, see Section "NMC keywords" or run 'help keyword'

  Alternatively, run 'setup usage' or 'show usage' and search for
  a given keyword. Use '/' to search forward, '?' to search backward.


* ${c1}Creating a volume$cn
  ---------------------
  to create a new volume:

  ${prompt}create volume
  and:

  ${prompt}create volume -h
  for help.

* ${c1}Batch mode$cn
  ---------------
  With NMC, management operations can be assembled in batches and run
  non-interactively. Here's a quick example:

  Run the following sequence:

     ${prompt}record xyz
     ${prompt}create volume
     ... fill out all the required options ...

     ${prompt}record view-current
     ... optionally, view the currently recorded content ...

     ${prompt}record stop
     ... and finally, stop the recording.

     Now, at this point the entire volume-creation operation is stored
     in session 'xyz' in a fully-expanded form. You could destroy the
     volume and later re-run (that is, replay) the session:

     ${prompt}run recording xyz

     The result will be that the volume is created again, but this
     time without user interaction.

  NMC recorded sessions can contain any number of valid operations,
  and can be used to execute in a group mode, and/or on a remote appliance.

  For more information, please see 'record -h'.


* ${c1}Force operation$cn
  ----------------------------

  The -f (force) flag is used consistently throughout NMC to force
  operations to proceed in spite of certain exceptional conditions.
  Few examples:

  Example #1:

	The auto-sync service expects that its destination is not tampered
	with between auto-sync runs. If prior to the next scheduled the
	auto-sync determines that the destination is not in-sync anymore,
	it will fail to run - unless the -f (force) option is specified.
	See 'create auto-sync -h' for details.

  Example #2:

	$NZA::COMPANY Management Server will not restart if any of the scheduled
	services or fault triggers or statistic collectors are currently
	executing. More exactly, if you try to restart NMS (via
	'setup appliance nms restart') while it executes anything on
	behalf of its clients, the restart operation will fail -
	unless the -f (force) option is specified.
	See 'setup appliance nms restart -h' for details.


* ${c1}Configuration management$cn
  ----------------------------
  Run 'setup' to set property, enable, disable, share, unshare,
  import, export, create, destroy, run a service, grow a volume -
  generally, to perform any configuration operation.

* ${c1}Locating objects and services$cn
  ---------------------------------
  Use ${c0}query$cn to find objects, locate services, and optionally,
  display their properties.

  For instance, the quickest way to list all existing objects:
  ${prompt}query -e name -v

  This maybe not very meaningful (although quick). To list appliance's
  objects by category, run:

  ${prompt}show   <TAB><TAB>
  ${prompt}setup  <TAB><TAB>

  See 'query -h' or 'query usage' for detailed description and
  many more examples.

* ${c1}iSCSI$cn
  --------------
  On iSCSI initiator side, to discover iSCSI targets and attach
  (via iSCSI) remote disks and disk arrays, use:
  ${c0}show iscsi$cn and ${c0}setup iscsi${cn} commands.

  In addition to iSCSI initiator, NMC provides extremely easy
  iSCSI target integration. You can convert the $NZA::COMPANY NAS into
  a Target in a matter of seconds, thus combining the flexibility
  and power of both file and block level access to the same
  storage.
  Simply,  use ${c0}setup zvol$cn and ${c0}create zvol${cn} commands.
  See ${c0}create zvol -h$cn for details.


* ${c1}lunsync$cn
  --------------
  Re-enumerate devices in the appliance. See 'lunsync -h' for more
  information.

* ${c1}$NZA::SYSPOOL$cn
  --------------
  '$NZA::SYSPOOL' is the name of the system volume. To show its properties,
  history of changes, IO statistics, shares, snapshots, status, version,
  and existing ${NMC::ZFS_LUN}s:

  ${prompt}show $NMC::VOLUME $NZA::SYSPOOL

  To create a new system checkpoint:
  ${prompt}setup appliance $NZA::SYSPOOL checkpoint

  To add hot spares:
  ${prompt}setup $NMC::VOLUME $NZA::SYSPOOL grow

* ${c1}How to see what is currently running?$cn
  --------------

  ${prompt}show appliance runners
  ${prompt}show appliance auto-services
  ${prompt}show appliance nms locks


  The first command will show all registered runners
  (including fault triggers, collectors, reporters, and indexers),
  and their state: idle or running.

  The second command will do the same for all replication
  services.

  The last command reports per NMS client locking information.
  To ensure consistent view of the appliance from all clients and
  data integrity at all times, $NZA::COMPANY Management Server
  serializes certain management operations. The corresponding
  locks can be viewed via 'show appliance nms locks'.

  To see Unix running processes and related details, run:

  ${prompt}prstat

  In all cases uses -h (help) option for usage.

]]></template>
<template module='Util' name='print_help_data_replication'><![CDATA[
${cb}13. Data Replication$cn
=================================================================

$NZA::PRODUCT provides a complete range of data replication capabilities:

  1. Auto-Tier - "Auto-tier" (or simply, tiering), makes use of
     snapshots and user definable source and destination points to
     regularly replicate a single copy of a file system to another
     storage pool, whether local or remote. By using snapshots on the
     target, this tiered copy may have arbitrarily different retention
     and expiration policies and can be administered separately from
     the source file system (folder). Auto-tier runs on a variety of
     transports, and can use snapshots as its source.

  2. (*) Auto-Sync - "Auto-sync" (or simply, syncing) maintains a fully
     synchronized copy of a given volume, file system (folder), or
     emulated block device (zvol) on another NAS.

  3. (*) Auto-CDP - Continuous Data Protection, or remote mirroring.
     Replicates $NZA::PRODUCT volumes remotely, in real time, at a block
     level. Conceptually, the service performs a function similar to
     local disk mirroring scheme of RAID 1 except that in case of
     Auto-CDP the "mirroring" is done over IP network.

  Auto-CDP is provided as a separate $NZA::PRODUCT extension (plugin).
  Rest of this section explains the difference between auto-tier
  and auto-sync.

  Both auto-sync and auto-tier are schedulable, fault-managed,
  fully configurable (tunable) $NZA::PRODUCT Data Replication services
  that can be used in a variety of backup, archiving, and DR scenarios.

  Both auto-sync and auto-tier are designed from ground up to use a
  variety of transports (a.k.a. protocols), which provides required
  flexibility to execute over Internet and Intranet, from behind a
  firewall and in the environment that requires extra security.

  Both auto-sync and auto-tier support any schedule. You can schedule
  the services to run every minute, every hour at a given minute of
  the hour, every few hours, every day at a certain time, etc. You can
  schedule services to run once a year, or at certain day of every
  second month, and so on.

  Both auto-sync and auto-tier provide a combined
  replication + snapshots capability. You can tier from a given source
  (for instance, from a given snapshot or a directory), and generate
  snapshot at the remote or local destination every time the replication
  has run.

  Both auto-sync and auto-tier support all 3 possible directions of
  the replication:

  * local to local (L2L)
  * local to remote (L2R)
  * remote to local (R2L)

  When replicating to or from remote host, the latter does not
  necessarily need to be a $NZA::PRODUCT appliance, although in the
  auto-sync case it must be another ZFS based system.

  Both auto-sync and auto-tier are recursive. The auto-sync service will
  replicate its designated source folder and nested sub-folders.

  Similarly, the auto-tier service replicates its source directory and
  nested sub-directories, which may include nested folders/filesystems.

  As of the version 1.1.6 of the appliance software:

    * the services can be set up to run only once - at a given scheduled
      time.

    * auto-sync can execute in a daemon mode and run incremental
      replications every second or every few seconds.

    * auto-sync can be used to replicate locally or remotely the
      appliance's system folder (a.k.a. root filesystem) that contains
      appliance's Operating System and configuration. The replication
      destination may or may be another $NZA::PRODUCT appliance, and -
      in the case when it is an appliance - may or may not reside on
      appliance's system volume. The equivalent tiering capability is
      not being planned.

  The primary difference, however, between auto-sync and auto-tier is
  threefold:

    1) Data and Meta-data

       Auto-sync transfers not only data (files, directories) but
       filesystem meta-data as well, including snapshots.

    2) Folder and Directory

       Auto-tier can have a directory within a filesystem as its top
       level source, while auto-sync cannot. To be able to transfer
       meta-data, auto-sync must have a folder (filesystem) as its top
       level source.

    3) Copying Over

       When executing the very first time, auto-tier can write over
       the existing files and directories at the destination.

       When executing the very first time, auto-sync cannot copy over
       the existing destination - it will create new folder(s) at the
       destination, and keep those folders fully in-sync with the
       source folders after each of the subsequent scheduled runs
       of the service. Those new folders will be complete clones of
       the folders at the source.

  By default, auto-sync is based on 'zfs send/receive' transport.
  Independently of its underlying transport, auto-sync always
  re-creates source snapshots at the destination.

  For instance, auto-sync that uses 'rsync' transport will perform
  rsync from all newly created snapshots at the source, and each
  time a given snapshot gets transferred, it will also snapshot
  the destination, thereby re-creating source snapshots at the
  destination. Note that the "newly created snapshots" in the previous
  statement simply mean those snapshots that were created since
  the previous scheduled or manual (on demand) auto-sync run(**).

  There is one common mistake, in terms of using auto-sync, and this
  is related to changes at its destination. Note that for auto-sync
  over 'zfs send/receive', its destination cannot and should not be
  mounted, except in one case - via using a specially designed
  'auto-mount ' property.

  Generally, mounting auto-sync destination is:

    1. An attribute change performed on the destination folder
       (filesystem). Note that any change at the destination means
       a loss of synchronization with the auto-sync source.

    2. Secondly, mounted auto-sync destination exposes the latter
       to changes, including changes in the ZFS meta information.
       Any change in the data or meta-data at the destination is
       "noticed" by auto-sync as a loss of synchronization between
       the source and the destination, and is getting processed
       as a (recoverable) error condition.

Note:
  (*) - Requires the corresponding auto-service plugin.

]]></template>
<template module='Util' name='print_help_more_help'><![CDATA[
${cb}14. More Help$cn
=================================================================

* Run 'help <command>' to get a manual page on any command, including
  (but not limited to) NMC built-in commands.

* Run 'help keyword <some-keyword>' to locate and print all NMC commands
  that use the specified keyword. For more information, see next Section
  "NMC keywords" or run 'help keyword'.

* Run 'setup usage' or 'show usage'. These commands produce a combined
  comprehensive usage guide for the 'setup' and 'show' super-commands,
  respectively. Search the result using '/' (forward search) and '?'
  (backword search)

* Read in-depth $NZA::PRODUCT user guide documentation. This comprehensive
  document includes Getting Started and Tutorial chapters,
  numerous examples, screenshots, and tips.

* In NMV ($NZA::COMPANY Management View) - click on Help menu, and explore
  the available options.

* Check out http://www.nexenta.com for the latest in documentation

* All known issues will be either documented on the install media
  or made available online

* As always, email support${at}nexenta.com for any further questions

]]></template>
<template module='Util' name='print_help_keyword'><![CDATA[
${cb}13. NMC keywords$cn
=================================================================

  $NZA::COMPANY Management Console supports an ever increasing number of
  commands. The 'help keyword' command (or its shorthand version 'help -k')
  can be used to easily locate NMC commands.

  With NMC, you never need to remember keywords and actual commands.

Usage: <some-keyword> [-f]

  [-f]      Perform a full search.

Example: display all supported commands that contain "appl":

  ${prompt}help keyword appl

  or, same:

  ${prompt}help -k appl


  Type 'help keyword <some-keyword>' - and NMC will list all available
  operations that mention the specified keyword.

  By default, NMC performs an exhaustive search. Note that many NMC
  operations are formed dynamically, at runtime.

  Use -f (full) option to perform a complete search.

  For instance:

  ${prompt}help keyword check

  will result in the following printout:

* create appliance checkpoint
* show appliance checkpoint
* setup appliance checkpoint
* destroy appliance checkpoint

  while:

  ${prompt}help keyword check -f

  will find many more matching NMC command lines:

* create appliance checkpoint
* show zvol <name> property checksum
* show folder <name> property checksum
* show appliance checkpoint
* show volume <vol-name> zvol <name> property checksum
* show volume <vol-name> folder <name> property checksum
* setup network service nmv-server confcheck
* setup network service snmp-agent confcheck
* setup network service hal confcheck
* setup network service rsync-server confcheck
* setup network service cifs-server confcheck
* setup network service ftp-server confcheck
* setup network service ndmp-server confcheck
* setup network service ldap-client confcheck
* setup network service ntp-client confcheck
* setup network service nfs-server confcheck
* setup network service iscsi-initiator confcheck
* setup network service iscsi-target confcheck
* setup network service webdav-server confcheck
* setup network service ssh-server confcheck
* setup zvol <name> property checksum
* setup folder <name> property checksum
* setup appliance checkpoint
* setup volume <vol-name> zvol <name> property checksum
* setup volume <vol-name> folder <name> property checksum
* setup volume <vol-name> property checksum
* destroy appliance checkpoint

  Note that a default search performs a lookup among
  static command lines. Please use -f (full) option for the
  complete but longer search.

  More examples follow below.

  ${prompt}help keyword scsi

  results in:

* create iscsi
* show network service iscsi-initiator
* show network service iscsi-target
* show lun scsi
* show lun iscsi
* show iscsi
* setup network service iscsi-initiator
* setup network service iscsi-target
* setup iscsi
* destroy iscsi

  First time users, try the following keyword searches:

  ${prompt}help keyword lun
  ${prompt}help keyword auto
  ${prompt}help keyword share
  ${prompt}help keyword group


You do not need to remember the exact command line option - a few
letters, a substring, will suffice. For instance:

Once the command is located, use -h (help) option to display its
usage in detail. See Section "Navigation" (or run 'help navigation')
for more information on how to quickly and conveniently navigate
through NMC.

See also: 'show usage'
See also: 'setup usage'
See also: 'run usage'
See also: 'query usage'
See also: 'switch usage'

]]></template>
<template module='Util' name='appliance_netmasks_usage'><![CDATA[
$cmdline

Contains network masks used to implement Internet Protocol (IP)
standard subnetting.

The /etc/netmasks file contains network masks used to implement IP standard
subnetting.This file contains a line for each network that is subnetted. Each
line consists of the network number, any number of spaces or tabs, and the
network mask to use on that network. Network numbers and masks may be specified
in the conventional IP . (dot) notation (similar to IP host addresses, but with
zeroes for the host part). The following number is a line from a netmask file:

128.32.0.0 255.255.255.0

This number specifies that the Class B network 128.32.0.0 has 8 bits of subnet
field and 8 bits of host field, in addition to the standard 16 bits in the
network field. When running network information service, this file on the
master is used for the netmasks.byaddr map.

]]></template>
<template module='Util' name='appliance_hosts_usage'><![CDATA[
$cmdline

In modern systems host tables have been superseded by DNS.
However, the host table is still widely used for small sites
isolated from the network.
If the local information rarely changes, and the network is not
connected to the Internet, DNS offers little advantage.

The host table (file '/etc/hosts') - is a simple text file that
associates IP addresses with hostnames, one line per IP address.
For each host a single line should be present with the following
information:

   IP_address   hostname   [aliases...]

Note:
=====
  Some appliance data services (e. g., auto-cdp) may depend on
  availability of either DNS hostname, or its replacement via
  the local host table.

]]></template>
<template module='Util' name='_show_props_log_stats_fmri'><![CDATA[

  ${NMC::RL_REVERSE}1${NMC::RL_NORMAL}  show $service '$name_suffix' state and properties
     'show $service $name_suffix -v'

  ${NMC::RL_REVERSE}2${NMC::RL_NORMAL}  show $service '$name_suffix' log
     'show $service $name_suffix log'

]]></template>
<template module='Util' name='_show_props_log_stats_fmri1'><![CDATA[
  ${NMC::RL_REVERSE}3${NMC::RL_NORMAL}  show $service '$name_suffix' run-time statistics
     'show $service $name_suffix stats'

]]></template>
<template module='Util' name='_show_props_log_stats_fmri2'><![CDATA[
  ${NMC::RL_REVERSE}4${NMC::RL_NORMAL}  show volume '$vol' status
     'show volume $vol status'

]]></template>
<template module='Util' name='_show_props_log_stats_fmri3'><![CDATA[
  ${NMC::RL_REVERSE}4${NMC::RL_NORMAL}  show volume '$vol' I/O statistics
     'show volume $vol iostat'

]]></template>
<template module='Util' name='show_auto_service_usage'><![CDATA[
$cmdline
Usage: [-v] [-i interval]

  -v		Verbose - show $service service details
  -i <interval>	Show only services for the given time interval, i.e
		$interval.

For background, general information, features and differences between
appliance's 2nd tier data services, please see:

${prompt}help data-replication

Examples:

1) List all $NZA::AUTO_SYNC_PLUGIN services: their states (idle, running)
and administrative statuses (disabled, online, maintenance):

${prompt}show $NZA::AUTO_SYNC_PLUGIN

This will produce something like:

NAME             SERVICE    FREQUENCY STARTED   STATUS       ENABL  STATE
vol1-a-000       auto-sync  daily     Apr_29    online       true   idle
vol1-users-001   auto-sync  weekly    Apr_29    online       true   idle
vol1-users-000   auto-sync  weekly    Apr_29    online       true   idle
...
...

2) Same as above, remotely. That is, list the services on a
   remote appliance identified by a given hostname
   (host1.xyz-corp.com in the example)

${prompt}switch appliance host1.xyz-corp.com -f
${prompt}show $NZA::AUTO_SYNC_PLUGIN

3) List all $NZA::AUTO_SYNC_PLUGIN service instances in detail:

${prompt}show $NZA::AUTO_SYNC_PLUGIN -v

or alternatively (because the output may be quite lengthy):

${prompt}show $NZA::AUTO_SYNC_PLUGIN -v | less

4) List hourly $NZA::AUTO_TIER services:

${prompt}show $NZA::AUTO_TIER -i hourly

The valid interval (-i) values are: $interval

Note:
   To search $service instances based on (any combination of their)
   property values, use NMC 'query' command. See 'query -h' for details.

   (*) - Requires the corresponding auto-service plugin.

See also: 'help data-replication'

See also: 'show appliance auto-services'

See also: 'create $NZA::AUTO_TIER'
See also: 'create $NZA::AUTO_SYNC_PLUGIN'
See also: 'create $NZA::AUTO_SNAP'
See also: 'create $NZA::AUTO_SCRUB'

See also: 'switch appliance'
See also: 'switch group'

See also: 'query'
See also: 'show usage'

See also: 'help'

]]></template>
<template module='Util' name='show_all_auto_services_usage'><![CDATA[
$cmdline
Usage: [-v] [-i interval]

  -v		Verbose - show auto-service details
  -i <interval>	Show only services for the given time interval, i.e
		$interval.


For background, general information, features and differences between
appliance's 2nd tier data services, please see:

${prompt}help data-replication


1) List all storage services: their states (idle, running)
and administrative statuses (disabled, online, maintenance):

${prompt}show appliance auto-services

This will produce something like:

NAME             SERVICE    FREQUENCY STARTED   STATUS       ENABL  STATE
vol1-a-000       auto-tier  hourly    Apr_26    online       true   idle
vol1-a-000       auto-sync  daily     Apr_29    online       true   idle
vol1-users-001   auto-sync  weekly    Apr_29    online       true   idle
vol1-a-b-000     auto-tier  daily     Apr_25    disabled     false  idle
vol1-users-000   auto-sync  weekly    Apr_29    online       true   idle
...
...

2) Same as above, remotely. That is, list the services on a
   remote appliance identified by a given hostname
   (host1.xyz-corp.com in the example)

${prompt}switch appliance host1.xyz-corp.com -f
${prompt}show appliance auto-services

3) List all service instances in detail:

${prompt}show appliance auto-services -v

or alternatively (because the output may be quite lengthy):

${prompt}show appliance auto-services -v | less

4) List all hourly services:

${prompt}show appliance auto-services -i hourly

The valid interval (-i) values are: $interval

Note:
   (*) - Requires the corresponding auto-service plugin.

See also: 'help data-replication'

See also: 'show appliance nms locks'

See also: 'show $NZA::AUTO_TIER'
See also: 'show $NZA::AUTO_SYNC_PLUGIN'
See also: 'show $NZA::AUTO_SNAP'
See also: 'show $NZA::AUTO_SCRUB'

See also: 'switch appliance'
See also: 'switch group'

See also: 'query'
See also: 'show usage'

See also: 'help'

]]></template>
<template module='Zfs' name='zfs_usage'><![CDATA[
$cmdline
Usage: 

Execute selected (non-modifying) zfs commands. For convenience, NMC 
includes the ability to execute 'zfs list' and 'zfs get' commands as 
entered by user, literally and without interpretation on the management
client side. 

The corresponding command line is simply sent to the $NZA::COMPANY Management
Server (NMS) for execution on a given appliance (to which NMC is
"switched" at the moment - see 'switch appliance -h' for details).

Note that this functionality is complementary and redundant, as far
as the provided 'show folder', 'show snapshot', and 'show zvol' interfaces 
are concerned. Provided for convenience only!

Using 'show' and the rest top-level NMC commands is preferrable and
recommended. For information on NMC top-level commands, please run: 

${prompt}help commands

See also: 'setup folder'

See also: 'zfs list'
See also: 'zfs get'
See also: 'zpool'

See also: 'show usage'
See also: 'setup usage'

See also: 'help keyword folder'
See also: 'help keyword zvol'
See also: 'help keyword snapshot'

]]></template>
<template module='Zfs' name='zfs_list_usage'><![CDATA[

Note that this interface is complementary and redundant, as far
as the provided 'show folder', 'show snapshot', and 'show zvol' interfaces 
are concerned. Provided for convenience only!

Using 'show' and the rest top-level NMC commands is preferrable and
recommended. For information on NMC top-level commands, please run: 

${prompt}help commands

Examples:

1) ${prompt}zfs list -t snapshot
   List all snapshots
   An alternative way to execute the same would be: 'show snapshot'
 
2) ${prompt}zfs list -t filesystem -o name,used,avail
   List (name, used, available) for all existing filesystems (folders).
   A sample output follows below:

   NAME                     USED  AVAIL
   syspool                 1.53G  1.89G
   syspool/rootfs-nmu-000  41.3M  1.89G
   syspool/rootfs-nmu-003  2.18M  1.89G
   syspool/rootfs-nmu-005  86.5K  1.89G
   syspool/rootfs-nmu-006  1.48G  1.89G
   vol1                     195K   976M
   vol1/a                    56K   976M
   vol1/a/b                  37K   976M
   vol1/a/b/c                18K   976M
   ...
 
   
For more information, please refer to zfs(1m) or run 'help zfs'.


See also: 'show folder'
See also: 'setup folder'

See also: 'show volume'
See also: 'setup volume'

See also: 'show zvol'
See also: 'setup zvol'

See also: 'show snapshot'
See also: 'setup snapshot'

See also: 'zfs get'
See also: 'zpool list'

]]></template>
<template module='Zfs' name='zfs_get_usage'><![CDATA[

As per ZFS man page, 'zfs get':

   """Displays properties for the given datasets.
      If no datasets are specified, then the command displays properties
      for all datasets on the system.
   """

Note that this interface is complementary and redundant, as far
as the provided 'show folder', 'show snapshot', and 'show zvol' interfaces 
are concerned. Provided for convenience only!

Using 'show' and the rest top-level NMC commands is preferrable and
recommended. For information on NMC top-level commands, please run: 

${prompt}help commands

Examples:

1) ${prompt}zfs get all vol1/a
   List all properties of 'vol1/a'
   An alternative way to execute the same would be: 'show folder vol1/a -v'
 
2) ${prompt}zfs get used,avail vol1/a
   Show used and available storage space values.
   An example of possible output follows below:
   
   NAME    PROPERTY   VALUE   SOURCE
   vol1/a  used       56K     -
   vol1/a  available  976M    -
 
   
For more information, please refer to zfs(1m) or run 'help zfs'.


See also: 'show folder'
See also: 'setup folder'

See also: 'show volume'
See also: 'setup volume'

See also: 'show zvol'
See also: 'setup zvol'

See also: 'show snapshot'
See also: 'setup snapshot'

See also: 'zfs list'
See also: 'zpool get'

]]></template>
<template module='Zpool' name='zpool_usage'><![CDATA[
$cmdline
Usage: 

Execute selected (non-modifying) zpool commands. For convenience, NMC 
includes the ability to execute 'zpool list' and 'zpool get' commands as 
entered by user, literally and without interpretation on the management
client side. 

The corresponding command line is simply sent to the $NZA::COMPANY Management
Server (NMS) for execution on a given appliance (to which NMC is
"switched" at the moment - see 'switch appliance -h' for details).

Note that this functionality is complementary and redundant, as far
as the provided 'show volume' and 'setup volume' interfaces 
are concerned. Provided for convenience only!

Using 'show', 'setup' and the rest top-level NMC commands is preferrable
and recommended. For information on NMC top-level commands, please run: 

${prompt}help commands


See also: 'zpool list'
See also: 'zpool get'

See also: 'zfs list'
See also: 'zfs get'

See also: 'show volume'
See also: 'setup volume'

See also: 'show usage'
See also: 'setup usage'

See also: 'help keyword volume'

]]></template>
<template module='Zpool' name='zpool_list_usage'><![CDATA[
As per ZPOOL man page, 'zpool list':

   """Lists  the  given  pools along with a health status and space
      usage. When given no arguments, all pools in the system are 
      listed.

      -o props    Comma-separated  list  of  properties to display.
   """

Note that this interface is complementary and redundant, as far
as the provided 'show volume' and 'setup volume' interfaces 
are concerned. Provided for convenience only!

Using 'show', 'setup' and the rest top-level NMC commands is preferrable
and recommended. For information on NMC top-level commands, please run: 

${prompt}help commands


Examples:

1) ${prompt}zpool list
   List all volumes
   An alternative way to execute the same would be: 'show volume'
 
2) ${prompt}zpool list -o name,size,used,avail
   List (name, size, used, available) for all existing volumes.
   A sample output follows below:

   NAME      SIZE   USED  AVAIL
   syspool  3.47G  1.53G  1.94G
   vol1     1008M   200K  1008M
 
3) ${prompt}zpool list -o name,size,used,avail vol1
   Same as above, for a specific volume.

   
For more information, please refer to zpool(1m) or run 'help zpool'.


See also: 'show volume'
See also: 'setup volume'

See also: 'zpool get'
See also: 'zfs list'

]]></template>
<template module='Zpool' name='zpool_get_usage'><![CDATA[

As per ZPOOL man page, 'zpool get':

   """Displays properties for the given datasets.
      If no datasets are specified, then the command displays properties
      for all datasets on the system.
   """

Note that this interface is complementary and redundant, as far
as the provided 'show volume' and 'setup volume' interfaces 
are concerned. Provided for convenience only!

Using 'show', 'setup' and the rest top-level NMC commands is preferrable
and recommended. For information on NMC top-level commands, please run: 

${prompt}help commands


Examples:

1) ${prompt}zpool get all vol1
   List all 'vol1' properties.
   An alternative way to execute the same would be: 'show volume vol1 -v'
 
2) ${prompt}zpool get size,used,avail vol1
   Show used and available storage space values.
   An example of possible output follows below:

   NAME  PROPERTY   VALUE  SOURCE
   vol1  size       1008M  -
   vol1  used       200K   -
   vol1  available  1008M  -
 
   
For more information, please refer to zpool(1m) or run 'help zpool'.


See also: 'show volume'
See also: 'setup volume'

See also: 'zpool list'
See also: 'zfs get'


]]></template>
<template module='Zpool' name='zpool_status_usage'><![CDATA[

As per ZPOOL man page, 'zpool status':

   """Displays the detailed health status for the given pools.
      If no pool is specified, then the status of each pool
      in the system is displayed.

      If a scrub or resilver is in progress, this command reports 
      the percentage done and the estimated time to completion.

      -x    Only display status for pools that are exhibiting errors 
            or are otherwise unavailable.

      -v    Displays verbose data error information, printing out 
            a complete list of all data errors since the last
            complete pool scrub.
   """

Note that this interface is complementary and redundant, as far
as the provided 'show volume' and 'setup volume' interfaces 
are concerned. Provided for convenience only!

Using 'show', 'setup' and the rest top-level NMC commands is preferrable
and recommended. For information on NMC top-level commands, please run: 

${prompt}help commands


Examples:

1) ${prompt}zpool status vol1
   Show status for volume 'vol1'
   An alternative way to execute the same would be: 
   'show volume vol1 status'
 
2) ${prompt}zpool status -v 
   Show verbose status of all appliance's volumes
   An alternative way to execute the same would be: 
   'show volume status'
   
For more information, please refer to zpool(1m) or run 'help zpool'.


See also: 'show volume <name> status'
See also: 'show volume'

See also: 'setup volume'

See also: 'zpool list'
See also: 'zpool get'
See also: 'zfs get'

]]></template>
<template module='Zpool' name='zpool_history_usage'><![CDATA[

As per ZPOOL man page, 'zpool history':

   """Displays the command history of the specified pools 
      or all pools if no pool is specified.

      -i    Displays internally logged ZFS events in addition 
            to user initiated events.

      -l    Displays log records in long format, which in addition
            to standard format includes, the user name, the
            hostname, and the zone in which the operation was 
	    performed.
   """

Note that this interface is complementary and redundant, as far
as the provided 'show volume' and 'setup volume' interfaces 
are concerned. Provided for convenience only!

Using 'show', 'setup' and the rest top-level NMC commands is preferrable
and recommended. For information on NMC top-level commands, please run: 

${prompt}help commands


Examples:

1) ${prompt}zpool history vol1
   Show ZFS command history for volume 'vol1'
   An alternative way to execute the same would be: 
   'show volume vol1 history'
 
2) ${prompt}zpool status 
   Show ZFS command history for all volumes
   An alternative way to execute the same would be: 
   'show volume history'
   
For more information, please refer to zpool(1m) or run 'help zpool'.


See also: 'show volume <name> history'
See also: 'show volume'

See also: 'setup volume'

See also: 'zpool list'
See also: 'zpool get'
See also: 'zfs get'

]]></template>
<template module='Zpool' name='zpool_usage'><![CDATA[
$cmdline
Usage: 

Execute selected (non-modifying) zpool sub-commands. For convenience, NMC 
includes the ability to execute 'zpool list', 'zfs status' and a few
other non-modifying commands, as entered by user, literally and without 
interpretation on the management client side. 

The corresponding command line is simply sent to the $NZA::COMPANY Management
Server (NMS) for execution on a given appliance (to which NMC is
"switched" at the moment - see 'switch appliance -h' for details).

Note that this functionality is complementary and redundant, as far
as the provided 'show volume' interfaces are concerned. Provided for 
convenience only!

Using 'show' and the rest top-level NMC commands is preferrable and
recommended. For information on NMC top-level commands, please run: 

${prompt}help commands

See also: 'setup volume'

See also: 'zfs list'
See also: 'zfs get'
See also: 'zpool'

See also: 'show usage'
See also: 'setup usage'

See also: 'help keyword volume'

]]></template>
<template module='Zpool' name='zpool_iostat_usage'><![CDATA[
Note that this interface is complementary and redundant, as far
as the provided 'show volume' and 'setup volume' interfaces 
are concerned. Provided for convenience only!

Using 'show', 'setup' and the rest top-level NMC commands is preferrable
and recommended. For information on NMC top-level commands, please run: 

${prompt}help commands

See also: 'show volume <name> iostat'
See also: 'show volume'

See also: 'help keyword iostat'

See also: 'zpool list'
See also: 'zpool get'
See also: 'zfs get'

]]></template>
<template module='nmc' name='kickstart'><![CDATA[

                                * * *
                            SYSTEM NOTICE

     Volume(s) '@faulted_pools' encountered an uncorrectable I/O error.
     Manual intervention is required.  In case of FC/iSCSI/USB attached
     drives, please verify connectivity to the corresponding target(s)
     and then try to recover the volume(s) using this recovery console.

]]></template>
</templatelist>
